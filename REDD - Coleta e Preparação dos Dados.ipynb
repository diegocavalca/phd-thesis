{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDD - Coleta e Preparação dos Dados (Low Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste documento será realizada a tabulação dos dados para cliassificação de cargas da base REDD.\n",
    "\n",
    "A janela considerada será de 5 minutos (3000 segundos).\n",
    "\n",
    "Procedimentos experimentais baseados em http://www.sbrt.org.br/sbrt2017/anais/1570359866.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.029988Z",
     "start_time": "2019-09-10T20:15:29.341011Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-08f648a838a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[0m_current_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_v1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m   _current_module.__path__ = (\n\u001b[0;32m    653\u001b[0m       [_module_util.get_parent_dir(estimator)] + _current_module.__path__)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\tensorflow_estimator\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0m_names_with_underscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_s\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_s\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\tensorflow_estimator\\_api\\v1\\estimator\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\tensorflow_estimator\\_api\\v1\\estimator\\experimental\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdnn_logit_fn_builder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkmeans\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeansClustering\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearSDCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msession_run_hook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_to_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode_keys\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\inputs.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# pylint: disable=unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy_input_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpandas_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas_input_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\numpy_io.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueues\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeeding_functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_functions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m   \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m   \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m   \u001b[0mHAS_PANDAS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdependency\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhard_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdependency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdependency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\pytz\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[0mall_timezones_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLazySet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_timezones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1101\u001b[1;33m \u001b[0m_all_timezones_lower_to_standard\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtz\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_timezones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1102\u001b[0m \u001b[0mcommon_timezones\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m ['Africa/Abidjan',\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\pytz\\lazy.py\u001b[0m in \u001b[0;36m_lazy\u001b[1;34m(self, *args, **kw)\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_iter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                         \u001b[0mlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_iter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mmethod_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_props\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                             \u001b[0mdelattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLazyList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\pytz\\__init__.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1096\u001b[0m  'Zulu']\n\u001b[0;32m   1097\u001b[0m all_timezones = LazyList(\n\u001b[1;32m-> 1098\u001b[1;33m         tz for tz in all_timezones if resource_exists(tz))\n\u001b[0m\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[0mall_timezones_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLazySet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_timezones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\pytz\\__init__.py\u001b[0m in \u001b[0;36mresource_exists\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;34m\"\"\"Return true if the given resource exists\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mopen_resource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado\\lib\\site-packages\\pytz\\__init__.py\u001b[0m in \u001b[0;36mopen_resource\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresource_stream\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresource_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'zoneinfo/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "#warnings.filterwarnings(\"warning\")\n",
    "import traceback\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (13, 10)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "#print(\"GPU Available: \", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando os dados da base REDD via NILMTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.033979Z",
     "start_time": "2019-09-10T20:15:29.346Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nilmtk import DataSet\n",
    "from nilmtk.utils import print_dict\n",
    "\n",
    "redd = DataSet('datasets/REDD/low_freq.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.035971Z",
     "start_time": "2019-09-10T20:15:29.351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configurações da amostragem\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Informações do CS446 Project : Electric Load Identification using Machine Learning (REDD)\n",
    "building_idx = 3\n",
    "set_sampling_rate = 3 \n",
    "start = datetime(2011, 4, 16, 5, 11, 27)\n",
    "end = datetime(2011, 5, 31, 0, 19, 54)\n",
    "time_interval_minutes = 5 # Split de amostra\n",
    "\n",
    "\n",
    "# ... 6 seconds - Imaging Time Series  (UK-DALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento dos dados\n",
    "---\n",
    "Delimitar (intervalo de tempo global e janelas de medições) e exportar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.037967Z",
     "start_time": "2019-09-10T20:15:29.356Z"
    }
   },
   "outputs": [],
   "source": [
    "building_idx = 1\n",
    "\n",
    "building = redd.buildings[building_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.039962Z",
     "start_time": "2019-09-10T20:15:29.360Z"
    }
   },
   "outputs": [],
   "source": [
    "# Available devices in building\n",
    "building.elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.040959Z",
     "start_time": "2019-09-10T20:15:29.364Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the fixed time block measurement\n",
    "redd.set_window(start='2011-04-18', end='2011-04-20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.042953Z",
     "start_time": "2019-09-10T20:15:29.366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Showing device consumption (inside time block)\n",
    "num_apps = 20\n",
    "fig, axes = plt.subplots((num_apps+1)//2,2, figsize=(24, num_apps*2) )\n",
    "for i in range(num_apps):\n",
    "    e = redd.buildings[1].elec[i+1]\n",
    "    axes.flat[i].plot(e.power_series_all_data(sample_period=3), alpha = 0.6)\n",
    "    axes.flat[i].set_title(e.label(), fontsize = '15')\n",
    "plt.suptitle('', fontsize = '30')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Energy Consumption in time-box (1 box = 5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.043950Z",
     "start_time": "2019-09-10T20:15:29.370Z"
    }
   },
   "outputs": [],
   "source": [
    "# Intervalos de geracao dos dados\n",
    "def datetime_range(start, end, delta):\n",
    "    '''\n",
    "    Generating a list of datetime intervals (chunks of energy consumption)\n",
    "    from `start` to `end` at each `delta` units.\n",
    "    '''\n",
    "    current = start\n",
    "    while current < end:\n",
    "        yield current\n",
    "        current += delta\n",
    "\n",
    "# List of datatimes \n",
    "dts = [dt.strftime('%Y-%m-%d %H:%M:%S') \n",
    "           for dt in \n",
    "               datetime_range(start, \n",
    "                              end, \n",
    "                              timedelta(minutes=time_interval_minutes))\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.045945Z",
     "start_time": "2019-09-10T20:15:29.416Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking chunks list...\n",
    "for idx in range(1, len(dts)):\n",
    "    print('de', dts[idx-1], ' a ', dts[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.047939Z",
     "start_time": "2019-09-10T20:15:29.421Z"
    }
   },
   "outputs": [],
   "source": [
    "power = building.elec[1].power_series_all_data(sample_period=set_sampling_rate)\n",
    "mains1 = pd.DataFrame(data = {\"Power\": power.values }, index=power.index)\n",
    "print('Mains 1 orginal shape: ', mains1.shape)\n",
    "power = building.elec[2].power_series_all_data(sample_period=set_sampling_rate)\n",
    "mains2 = pd.DataFrame(data = {\"Power\": power.values }, index=power.index)\n",
    "print('Mains 2 orginal shape: ', mains2.shape)\n",
    "\n",
    "power = building.elec[5].power_series_all_data(sample_period=set_sampling_rate)\n",
    "appliance = pd.DataFrame(data = {\"Power\": power.values }, index=power.index)\n",
    "print('Appliance shape: ', appliance.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.049934Z",
     "start_time": "2019-09-10T20:15:29.426Z"
    }
   },
   "outputs": [],
   "source": [
    "tStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.050932Z",
     "start_time": "2019-09-10T20:15:29.430Z"
    }
   },
   "outputs": [],
   "source": [
    "len(appliance.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.051929Z",
     "start_time": "2019-09-10T20:15:29.671Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Conjunto de dataframes (chunks de 5 minutos)\n",
    "dataframes = []\n",
    "\n",
    "# Iterando sobre blocos de tempos (5 minutos)\n",
    "for idx in tqdm_notebook(range(1, len(dts))):\n",
    "    \n",
    "    tStart = dts[idx-1]\n",
    "    tEnd = dts[idx]\n",
    "\n",
    "    # Intervalo de treino do modelo\n",
    "    redd.set_window(start=tStart, end=tEnd)\n",
    "    try:\n",
    "        print('- Chunk #',idx,': from ', tStart, 'to', tEnd)\n",
    "        \n",
    "        dfs = {}\n",
    "        _index = []\n",
    "        for m in building.elec.all_meters():\n",
    "\n",
    "            label = str(m.label()).lower().replace(' ','_') + '_' + str(m.instance())\n",
    "            power = m.power_series_all_data(sample_period=set_sampling_rate)\n",
    "            \n",
    "            dfs[label] = pd.DataFrame(data = {\"Power\": power.values }, index=power.index)\n",
    "            \n",
    "            if len(_index) < len(power.index) and ('site_meter' not in label):\n",
    "                _index = power.index\n",
    "\n",
    "        for meter_label in dfs:\n",
    "            #if 'site_meter' in meter_label:\n",
    "            #    dfs[meter_label] = dfs[meter_label].reindex(index=_index)\n",
    "            dfs[meter_label] = dfs[meter_label].reindex(index=_index)\n",
    "            dfs[meter_label] = dfs[meter_label]['Power'].values\n",
    "\n",
    "        df = pd.DataFrame(dfs, index = _index)\n",
    "        \n",
    "#         power = building.elec[1].power_series_all_data(sample_period=set_sampling_rate)\n",
    "#         mains1 = pd.DataFrame(data = {\"Power\": power.values }, index=power.index)\n",
    "#         print('Mains 1 orginal shape: ', mains1.shape)\n",
    "#         power = building.elec[2].power_series_all_data(sample_period=set_sampling_rate)\n",
    "#         mains2 = pd.DataFrame(data = {\"Power\": power.values }, index=power.index)\n",
    "#         print('Mains 2 orginal shape: ', mains2.shape)\n",
    "\n",
    "#         power = building.elec[5].power_series_all_data(sample_period=set_sampling_rate)\n",
    "#         appliance = pd.DataFrame(data = {\"Power\": power.values }, index=power.index)\n",
    "#         print('Appliance shape: ', appliance.shape)\n",
    "\n",
    "#         # Ajustar timeframes (eletronicos medidos em 3s, contra 1s da rede)\n",
    "#         mains1 = mains1.reindex(index=appliance.index)\n",
    "#         print('---\\nMains 1 new shape: ', mains1.shape)\n",
    "#         mains2 = mains2.reindex(index=appliance.index)\n",
    "#         print('Mains 2 new shape: ', mains2.shape)\n",
    "\n",
    "#         # Dataframe da modelagem\n",
    "#         df = pd.DataFrame({\n",
    "#             'Mains1': mains1[\"Power\"].values,\n",
    "#             'Mains2': mains2[\"Power\"].values,\n",
    "#             'Appliance': appliance[\"Power\"].values\n",
    "#         }, index = appliance.index)\n",
    "        print('\\n---\\nDataframe shape: ', df.shape,'\\n')\n",
    "\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(' ----- Error: ', str(e))#str(traceback.format_exc()))\n",
    "        #print(' ----- Não foi possível extrair dados do intervalo!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.052926Z",
     "start_time": "2019-09-10T20:15:29.674Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Total Chunks:', len(dataframes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.054921Z",
     "start_time": "2019-09-10T20:15:29.677Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if the chunk has the valid length\n",
    "valid_chunk_length = (time_interval_minutes*60)/set_sampling_rate\n",
    "valid_chunks = [d for d in dataframes if d.shape[0] == valid_chunk_length]\n",
    "\n",
    "print('Valid Chunks:', len(valid_chunks) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.056917Z",
     "start_time": "2019-09-10T20:15:29.680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting 5 chunks\n",
    "for df in tqdm(dataframes):\n",
    "    #df = valid_chunks[i]\n",
    "#     if sum(df['Appliance'].values) == 0:\n",
    "#         fig = plt.figure(figsize=(10,8))\n",
    "#         plt.plot(df['Mains1'].values)\n",
    "#         plt.plot(df['Mains2'].values)\n",
    "#         plt.plot(df['Appliance'].values)\n",
    "#         plt.gca().legend(('Mains1','Mains2', 'Appliance'))\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    for column in df.columns:\n",
    "        plt.plot(df[column].values)\n",
    "    plt.gca().legend(df.columns)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction / Label Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.057912Z",
     "start_time": "2019-09-10T20:15:29.684Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "rows = []\n",
    "classes = [c for c in dataframes[0].columns if 'site_meter' not in c]\n",
    "for df in dataframes:\n",
    "    attributes = {\n",
    "        'mean_1': df['site_meter_1'].mean(),\n",
    "        'std_1': df['site_meter_1'].std(),\n",
    "        'max_1': df['site_meter_1'].max(),\n",
    "        'min_1': df['site_meter_1'].min(),\n",
    "        'sum_1': df['site_meter_1'].sum(),\n",
    "        'mean_2': df['site_meter_2'].mean(),\n",
    "        'std_2': df['site_meter_2'].std(),\n",
    "        'max_2': df['site_meter_2'].max(),\n",
    "        'min_2': df['site_meter_2'].min(),\n",
    "        'sum_2': df['site_meter_2'].sum()\n",
    "    }\n",
    "    labels = {}\n",
    "    for c in classes:\n",
    "        labels[c] = 1 if df[c].sum() > 0 else 0\n",
    "    \n",
    "    final_df = final_df.append({**attributes, **labels}, ignore_index=True)\n",
    "\n",
    "final_df = final_df[ list(attributes.keys()) + list(labels.keys()) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.060905Z",
     "start_time": "2019-09-10T20:15:29.692Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.061903Z",
     "start_time": "2019-09-10T20:15:29.696Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.063898Z",
     "start_time": "2019-09-10T20:15:29.699Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df.to_csv('df_building_1_statistics_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.064894Z",
     "start_time": "2019-09-10T20:15:29.704Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#- Validar metodologia\n",
    "#- Validar chunks gerados (noralizar erros)\n",
    "#- Rotular base (labels binários por dispositivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.066890Z",
     "start_time": "2019-09-10T20:15:29.708Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df[final_df.columns[:10]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T00:56:54.124358Z",
     "start_time": "2019-09-07T00:56:54.064357Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.068885Z",
     "start_time": "2019-09-10T20:15:29.712Z"
    }
   },
   "outputs": [],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "scores = cross_val_score(\n",
    "    MLkNN(k=3),\n",
    "    final_df[final_df.columns[:5]].values,\n",
    "    final_df[final_df.columns[10:]].values,\n",
    "    scoring = 'f1_micro',\n",
    "    cv=5,\n",
    "    n_jobs = 8\n",
    ")\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.069890Z",
     "start_time": "2019-09-10T20:15:29.716Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, hamming_loss\n",
    "hamming_score = make_scorer(hamming_loss)\n",
    "\n",
    "scores = cross_val_score(\n",
    "    MLkNN(k=3),\n",
    "    final_df[final_df.columns[:5]].values,\n",
    "    final_df[final_df.columns[10:]].values,\n",
    "    scoring = hamming_score,\n",
    "    cv=5,\n",
    "    n_jobs = 8\n",
    ")\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.071876Z",
     "start_time": "2019-09-10T20:15:29.720Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "scores = cross_val_score(\n",
    "    RandomForestClassifier(n_estimators=1000),\n",
    "    final_df[final_df.columns[:5]].values,\n",
    "    final_df[final_df.columns[10:]].values,\n",
    "    scoring = hamming_score,\n",
    "    cv=5,\n",
    "    n_jobs = 8\n",
    ")\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Outras análises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.073871Z",
     "start_time": "2019-09-10T20:15:29.725Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#dates = [str(dt).split(' ')[0] for dt in df.index]\n",
    "dates = [str(time)[:10] for time in df.index.values]\n",
    "dates = sorted(list(set(dates)))\n",
    "print('Os dados da Residência modelada contém medições de {1} dia(s) (de {2} a {3}).'.format(i,len(dates),dates[0], dates[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.075866Z",
     "start_time": "2019-09-10T20:15:29.727Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Split de treino, teste e validação\n",
    "df1_train = df.loc[:dates[10]]\n",
    "df1_val = df.loc[dates[11]:dates[16]]\n",
    "df1_test = df.loc[dates[17]:]\n",
    "print('df_train.shape: ', df1_train.shape)\n",
    "print('df_val.shape: ', df1_val.shape)\n",
    "print('df_test.shape: ', df1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.079868Z",
     "start_time": "2019-09-10T20:15:29.730Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Usando a corrente 1 e 2 (variaveis independetes) para a previsão do refrigerador (variavel dependente)\n",
    "X_train = df1_train[['Mains1','Mains2']].values\n",
    "y_train = df1_train['Appliance'].values\n",
    "\n",
    "X_test = df1_test[['Mains1','Mains2']].values\n",
    "y_test = df1_test['Appliance'].values\n",
    "\n",
    "X_val = df1_val[['Mains1','Mains2']].values\n",
    "y_val = df1_val['Appliance'].values\n",
    "\n",
    "print(\n",
    "    'Train: ', X_train.shape, y_train.shape, '\\n',\n",
    "    'Test: ', X_val.shape, y_val.shape, '\\n',\n",
    "    'Validation: ', X_test.shape, y_test.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.082847Z",
     "start_time": "2019-09-10T20:15:29.733Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Metrcas de avaliação da regressão\n",
    "def mse_loss(y_predict, y):\n",
    "    return np.mean(np.square(y_predict - y)) \n",
    "def mae_loss(y_predict, y):\n",
    "    return np.mean(np.abs(y_predict - y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.085838Z",
     "start_time": "2019-09-10T20:15:29.736Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Dropout, LSTM, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "def build_fc_model(layers):\n",
    "    fc_model = Sequential()\n",
    "    for i in range(len(layers)-1):\n",
    "        #fc_model.add( Dense(input_dim=layers[i], output_dim= layers[i+1]) )#, W_regularizer=l2(0.1)) )\n",
    "        fc_model.add( Dense(input_shape=(layers[i],), units = layers[i+1]) )#, W_regularizer=l2(0.1)) )\n",
    "        fc_model.add( Dropout(0.5) )\n",
    "        if i < (len(layers) - 2):\n",
    "            fc_model.add( Activation('relu') )\n",
    "    fc_model.build()\n",
    "    fc_model.summary()\n",
    "    plot_model(fc_model)\n",
    "    return fc_model\n",
    "\n",
    "fc_model_1 = build_fc_model([2, 256, 512, 1024, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.087833Z",
     "start_time": "2019-09-10T20:15:29.740Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr = 1e-5)\n",
    "fc_model_1.compile(loss='mean_squared_error', optimizer=adam)\n",
    "start = time.time()\n",
    "model_path = \"./resources/mlp_fridge_h1.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=model_path, verbose=0, save_best_only=True)\n",
    "hist_fc_1 = fc_model_1.fit( X_train, y_train,\n",
    "                    batch_size=512, verbose=1, nb_epoch=200,\n",
    "                    validation_split=0.33, callbacks=[checkpointer])\n",
    "print('Tempo total de treinamento do modelo (s):', round(time.time() - start, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.088837Z",
     "start_time": "2019-09-10T20:15:29.745Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fc_model = load_model(model_path)\n",
    "pred_fc = fc_model.predict(X_test).reshape(-1)\n",
    "mse_loss_fc = mse_loss(pred_fc, y_test)\n",
    "mae_loss_fc = mae_loss(pred_fc, y_test)\n",
    "print('MSE no conjunto de teste: ', mse_loss_fc)\n",
    "print('MAE no conjunto de teste:', mae_loss_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.090826Z",
     "start_time": "2019-09-10T20:15:29.752Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_loss = hist_fc_1.history['loss']\n",
    "val_loss = hist_fc_1.history['val_loss']\n",
    "def plot_losses(train_loss, val_loss):\n",
    "    plt.rcParams[\"figure.figsize\"] = [24,10]\n",
    "    plt.title('MSE dos conjuntos de treino e teste - Resid. 1')\n",
    "    plt.plot( range(len(train_loss)), train_loss, color = 'b', alpha = 0.6, label='loss (treino)' )\n",
    "    plt.plot( range(len( val_loss )), val_loss, color = 'r', alpha = 0.6, label='loss (validação)' )\n",
    "    plt.xlabel( 'época' )\n",
    "    plt.ylabel( 'loss' )\n",
    "    plt.legend()\n",
    "\n",
    "plot_losses(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.093819Z",
     "start_time": "2019-09-10T20:15:29.761Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plotando os cnsumos REAL e o PREVISTO do refrigerador nos 6 dias dos dados de teste\n",
    "def plot_each_app(df, dates, predict, y_test, title, look_back = 0):\n",
    "    num_date = len(dates)\n",
    "    fig, axes = plt.subplots(num_date,1,figsize=(24, num_date*5) )\n",
    "    plt.suptitle(title, fontsize = '25')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.95)\n",
    "    for i in range(num_date):\n",
    "        if i == 0: l = 0\n",
    "        ind = df.ix[dates[i]].index[look_back:]\n",
    "        axes.flat[i].plot(ind, y_test[l:l+len(ind)], color = 'blue', alpha = 0.6, label = 'REAL')\n",
    "        axes.flat[i].plot(ind, predict[l:l+len(ind)], color = 'red', alpha = 0.6, label = 'PREVISTO')\n",
    "        axes.flat[i].legend()\n",
    "        l = len(ind)\n",
    "plot_each_app(df1_test, dates[17:], pred_fc, y_test, \n",
    "              'Rede Neural FC: Real e Previsão nos 6 dias do Conjunto de Teste da Resid. 1', look_back = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.095812Z",
     "start_time": "2019-09-10T20:15:29.773Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Testar FC mais complexa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.097807Z",
     "start_time": "2019-09-10T20:15:29.779Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_lstm_model(layers):\n",
    "#         #fc_model.add( Dense(input_dim=layers[i], output_dim= layers[i+1]) )#, W_regularizer=l2(0.1)) )\n",
    "#         fc_model.add( Dense(input_shape=(layers[i],), units = layers[i+1]) )#, W_regularizer=l2(0.1)) )\n",
    "#         fc_model.add( Dropout(0.5) )\n",
    "#         if i < (len(layers) - 2):\n",
    "#             fc_model.add( Activation('relu') )\n",
    "    model = Sequential()\n",
    "    for i in range(len(layers) - 2):\n",
    "        if i == 0:\n",
    "            model.add(Embedding(input_dim=layers[i], output_dim=layers[i+1]))\n",
    "        else:\n",
    "            model.add(LSTM(\n",
    "                input_shape=(layers[i],),\n",
    "                units=layers[i+1],\n",
    "                return_sequences = True if i < len(layers) - 3 else False ))\n",
    "            model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(layers[-1]))\n",
    "    \n",
    "    model.build()\n",
    "    model.summary()\n",
    "    plot_model(model)\n",
    "    return model\n",
    "\n",
    "model = build_lstm_model([2,64,128,256, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.099801Z",
     "start_time": "2019-09-10T20:15:29.784Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Utilizando 50 registros de consumos para retreinar o modelo, e prever o consumo de energia de cada aparelho\n",
    "def process_data(df, dates, x_features, y_features, look_back = 50):\n",
    "    i = 0\n",
    "    for date in dates:\n",
    "        data = df.loc[date]\n",
    "        len_data = data.shape[0]\n",
    "        x = np.array([data[x_features].values[i:i+look_back] \n",
    "                      for i in range(len_data - look_back) ]).reshape(-1,look_back, 2)\n",
    "        y = data[y_features].values[look_back:,:]\n",
    "        if i == 0:\n",
    "            X = x\n",
    "            Y = y\n",
    "        else:\n",
    "            X = np.append(X, x, axis=0)\n",
    "            Y = np.append(Y, y, axis=0)\n",
    "        i += 1\n",
    "    return X,Y\n",
    "\n",
    "start = time.time()\n",
    "X_train, y_train = process_data(df, dates[:17], ['Mains1','Mains2'], df.columns.values[2:])\n",
    "X_test, y_test = process_data(df, dates[17:], ['Mains1','Mains2'], df.columns.values[2:])\n",
    "print('Tempo de execução total (s): ', time.time() - start)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.100799Z",
     "start_time": "2019-09-10T20:15:29.790Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "adam = Adam(lr = 5e-5)\n",
    "lstm_model_path = \"./resources/lstm_model.hdf5\"\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "checkpointer = ModelCheckpoint(filepath=lstm_model_path, verbose=0, save_best_only=True)\n",
    "hist_lstm = model.fit(\n",
    "            X_train,\n",
    "            y_train[:,2],\n",
    "            batch_size=512,\n",
    "            verbose=1,\n",
    "            nb_epoch=200,\n",
    "            validation_split=0.3,\n",
    "            callbacks=[checkpointer])\n",
    "print('Tempo de treino (s): ', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T20:15:35.102794Z",
     "start_time": "2019-09-10T20:15:29.794Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. http://nilmtk.github.io/nilmtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
