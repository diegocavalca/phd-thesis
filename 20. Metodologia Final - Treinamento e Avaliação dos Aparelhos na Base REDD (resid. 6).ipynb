{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodologia Final - Avaliação na Base REDD / Residência 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os resultados obtidos até aqui foi possível consolidar a metodologia final que será adotada na pesquisa, a qual é estruturada em:\n",
    "\n",
    "1. Taxa Amostral e Janela flexível por Aparelho;\n",
    "2. Gráfico de Recorrência como ferramenta de pré-processamento da série temporal;\n",
    "3. Rede Neural Convolucional como modelo classificador;\n",
    "4. Estratégia de treinamento do modelo:\n",
    "    * Transfer Learning do Aparelho em Outras residências;\n",
    "    * Treinamento utilizando Punição para o Desbalanceamento de classes;\n",
    "    * Adoção de Parada Antecipada (em função de AUC, a fim de otimizar sensibilidade);\n",
    "    * Uso de Threshold-adaptativo;\n",
    "    * Avaliação de Carga Descartada (threshold).\n",
    "    \n",
    "Sendo assim, neste estudo iremos aplicar a metodologia nos dados da base REDD, especificamente na `residência 6`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T13:32:41.379195Z",
     "start_time": "2020-08-08T13:32:38.152040Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "from pretty_confusion_matrix import *\n",
    "\n",
    "# TODO: implementar rotina na classe PyNILM.utils\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "def listar_variaveis_memoria(ambiente):\n",
    "    print(\"* Variáveis instanciadas em memória:\")\n",
    "    print(\"---\")\n",
    "    total = 0\n",
    "    for name, size in sorted(((name, sys.getsizeof(value)) for name, value in ambiente.items()),\n",
    "                             key= lambda x: -x[1])[:10]:\n",
    "        total += size\n",
    "        print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n",
    "    print(\"---\")\n",
    "    print(\"Total:\", sizeof_fmt(total))\n",
    "    \n",
    "# TODO: implementar na classe utils\n",
    "def highlight_col(x):\n",
    "    r = 'background-color: #D9D9D9'\n",
    "    df1 = pd.DataFrame('', index=x.index, columns=x.columns)\n",
    "    df1.iloc[:, -2] = r\n",
    "    return df1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T13:32:41.441436Z",
     "start_time": "2020-08-08T13:32:41.411447Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONSTANTES FUNDAMENTAIS DE ORGANIZACAO DE PASTAS/ARQUIVOS\n",
    "RESIDENCIA = 6\n",
    "\n",
    "# Path do arquivo H5 (base REDD ja preparada p/ NILMTK) e outros insumos fundamentais\n",
    "caminho_dados = \"D:/Projetos/phd-thesis/datasets/\"\n",
    "\n",
    "# Definir diretorios onde iremos salvar os insumos gerados do notebook (dados, imagens, etc.)\n",
    "caminho_dados_notebook = os.path.join(caminho_dados, \"20\", f\"residencia_{RESIDENCIA}\") # Num. notebook\n",
    "if not os.path.isdir(caminho_dados_notebook):\n",
    "    os.makedirs(caminho_dados_notebook)\n",
    "caminho_imagens_notebook = os.path.join(caminho_dados_notebook, \"imagens\") # Num. notebook\n",
    "if not os.path.isdir(caminho_imagens_notebook):\n",
    "    os.makedirs(caminho_imagens_notebook)\n",
    "\n",
    "# Path do arquivo H5 (base REDD ja preparada p/ NILMTK)\n",
    "caminho_redd = os.path.join(caminho_dados, \"REDD/low_freq\")\n",
    "\n",
    "# Path completo do arquivo REDD\n",
    "arquivo_dataset = os.path.join(caminho_redd, \"redd.h5\")\n",
    "\n",
    "# VARIAVEL AUXILIAR\n",
    "# Path dos arquivos relacionados as janelas\n",
    "caminho_janelas = os.path.join(caminho_redd, \"../../phd\")\n",
    "if not os.path.isdir(caminho_janelas):\n",
    "    os.makedirs(caminho_janelas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base REDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T13:32:44.710102Z",
     "start_time": "2020-08-08T13:32:42.468094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NILMTK -> Detalhes sobre o dataset REDD:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ul><li><strong>name</strong>: REDD</li><li><strong>long_name</strong>: The Reference Energy Disaggregation Data set</li><li><strong>creators</strong>: <ul><li>Kolter, Zico</li><li>Johnson, Matthew</li></ul></li><li><strong>publication_date</strong>: 2011</li><li><strong>institution</strong>: Massachusetts Institute of Technology (MIT)</li><li><strong>contact</strong>: zkolter@cs.cmu.edu</li><li><strong>description</strong>: Several weeks of power data for 6 different homes.</li><li><strong>subject</strong>: Disaggregated power demand from domestic buildings.</li><li><strong>number_of_buildings</strong>: 6</li><li><strong>timezone</strong>: US/Eastern</li><li><strong>geo_location</strong>: <ul><li><strong>locality</strong>: Massachusetts</li><li><strong>country</strong>: US</li><li><strong>latitude</strong>: 42.360091</li><li><strong>longitude</strong>: -71.09416</li></ul></li><li><strong>related_documents</strong>: <ul><li><a href=\"http://redd.csail.mit.edu\">http://redd.csail.mit.edu</a></li><li>J. Zico Kolter and Matthew J. Johnson. REDD: A public data set for energy disaggregation research. In proceedings of the SustKDD workshop on Data Mining Applications in Sustainability, 2011. <a href=\"http://redd.csail.mit.edu/kolter-kddsust11.pdf\">http://redd.csail.mit.edu/kolter-kddsust11.pdf</a>\n",
       "</li></ul></li><li><strong>schema</strong>: <a href=\"https://github.com/nilmtk/nilm_metadata/tree/v0.2\">https://github.com/nilmtk/nilm_metadata/tree/v0.2</a></li><li><strong>meter_devices</strong>: <ul><li><strong>eMonitor</strong>: <ul><li><strong>model</strong>: eMonitor</li><li><strong>manufacturer</strong>: Powerhouse Dynamics</li><li><strong>manufacturer_url</strong>: <a href=\"http://powerhousedynamics.com\">http://powerhousedynamics.com</a></li><li><strong>description</strong>: Measures circuit-level power demand.  Comes with 24 CTs. This FAQ page suggests the eMonitor measures real (active) power: <a href=\"http://www.energycircle.com/node/14103\">http://www.energycircle.com/node/14103</a>  although the REDD readme.txt says all channels record apparent power.\n",
       "</li><li><strong>sample_period</strong>: 3</li><li><strong>max_sample_period</strong>: 50</li><li><strong>measurements</strong>: <ul><li>{'physical_quantity': 'power', 'type': 'active', 'upper_limit': 5000, 'lower_limit': 0}</li></ul></li><li><strong>wireless</strong>: False</li></ul></li><li><strong>REDD_whole_house</strong>: <ul><li><strong>description</strong>: REDD's DIY power meter used to measure whole-home AC waveforms at high frequency.  To quote from their paper: \"CTs from TED (<a href=\"http://www.theenergydetective.com\">http://www.theenergydetective.com</a>) to measure current in the power mains, a Pico TA041 oscilloscope probe (<a href=\"http://www.picotechnologies.com\">http://www.picotechnologies.com</a>) to measure voltage for one of the two phases in the home, and a National Instruments NI-9239 analog to digital converter to transform both these analog signals to digital readings. This A/D converter has 24 bit resolution with noise of approximately 70 ÂµV, which determines the noise level of our current and voltage readings: the TED CTs are rated for 200 amp circuits and a maximum of 3 volts, so we are able to differentiate between currents of approximately ((200))(70 Ã— 10âˆ’6)/(3) = 4.66mA, corresponding to power changes of about 0.5 watts. Similarly, since we use a 1:100 voltage stepdown in the oscilloscope probe, we can detect voltage differences of about 7mV.\"\n",
       "</li><li><strong>sample_period</strong>: 1</li><li><strong>max_sample_period</strong>: 30</li><li><strong>measurements</strong>: <ul><li>{'physical_quantity': 'power', 'type': 'apparent', 'upper_limit': 50000, 'lower_limit': 0}</li></ul></li><li><strong>wireless</strong>: False</li></ul></li></ul></li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARÂMETROS DO ESTUDO:\n",
      "{'base': <nilmtk.dataset.DataSet object at 0x000001E949C9BA20>,\n",
      " 'debug': False,\n",
      " 'fim_intervalo': '2011-04-23 08:43:26',\n",
      " 'id_residencia': 6,\n",
      " 'inicio_intervalo': '2011-04-16 05:11:30'}\n"
     ]
    }
   ],
   "source": [
    "# Gerar arquivo H5 (Nilmtk) do dataset REDD, caso n exista\n",
    "if not os.path.isfile(arquivo_dataset):\n",
    "    from nilmtk.dataset_converters import convert_redd\n",
    "    \n",
    "    print(\"Gerando arquivo H5 (NILMTK) da base REDD, aguarde...\")\n",
    "    print(\"-----\")\n",
    "    convert_redd(caminho_redd, arquivo_dataset)\n",
    "\n",
    "# Carregando dataset REDD no objeto NILMTK\n",
    "# Exemplo de carregamento da base REDD no NILMTK\n",
    "import h5py # * Evitar erro de incompatibilidade entre h5py e nilmtk\n",
    "from nilmtk import DataSet\n",
    "from nilmtk.utils import print_dict\n",
    "redd = DataSet(arquivo_dataset)\n",
    "print(\"NILMTK -> Detalhes sobre o dataset REDD:\")\n",
    "print_dict(redd.metadata)\n",
    "print()\n",
    "\n",
    "# Parametros dos dados\n",
    "PARAMETROS_DATASET = {\n",
    "    \"base\":redd,\n",
    "    \"id_residencia\": RESIDENCIA,\n",
    "    \"inicio_intervalo\":'2011-04-16 05:11:30',\n",
    "    \"fim_intervalo\":'2011-04-23 08:43:26',\n",
    "    \"debug\": False    \n",
    "}\n",
    "print(\"PARÂMETROS DO ESTUDO:\")\n",
    "pprint(PARAMETROS_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T13:32:44.758102Z",
     "start_time": "2020-08-08T13:32:44.744102Z"
    }
   },
   "outputs": [],
   "source": [
    "def carregar_dados_aparelho(janelas, instancia, aparelho, taxa, tamanho_janela, split_teste=None, eliminar_janelas_vazias=False, debug=False):\n",
    "    # Extrair series divididas em janelas para cada medidor\n",
    "    dados_cargas = janelas.preparar(\n",
    "        taxa_amostral=taxa, \n",
    "        intervalo_medicao=tamanho_janela\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    # Pprearando dados (Serie / Estado)\n",
    "    # X\n",
    "    dados_medidores = janelas.filtrar_cargas(\n",
    "        dados_cargas,\n",
    "        filtros=[\n",
    "            (1, 'site_meter'),\n",
    "            (2, 'site_meter'),    \n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    dados_aparelho = janelas.filtrar_cargas(dados_cargas, filtros=[(instancia, aparelho)])[0]\n",
    "    \n",
    "    # Validar tamanho dos dados de medidores (podem ter mais registros que os aparelhos)\n",
    "    janela_media_medidores = int(np.sum([len(d[\"janelas\"])for d in dados_medidores])/len(dados_medidores))\n",
    "    janela_media_aparelho = len(dados_aparelho[\"janelas\"])#int(np.sum([len(d[\"janelas\"])for d in dados_aparelho])/len(dados_aparelho))\n",
    "\n",
    "    # Ajustando para medidores terem o mesmo shape de janelas dos aparelhos \n",
    "    if janela_media_medidores > janela_media_aparelho:\n",
    "        diferenca = janela_media_medidores-janela_media_aparelho\n",
    "        #if debug: print(\"  -> Diferenca encontrada entre medidores/aparelhos:\", diferenca, \", ajustando..\")\n",
    "        for i in range(len(dados_medidores)):\n",
    "            removidos = 0\n",
    "            while removidos < diferenca:\n",
    "                # Remover ultima janela\n",
    "                dados_medidores[i][\"janelas\"] = dados_medidores[i][\"janelas\"][:-1,:]\n",
    "                removidos += 1\n",
    "    \n",
    "    # Estruturando dados modelagem (X e y)\n",
    "    X = dados_medidores[0][\"janelas\"] + dados_medidores[1][\"janelas\"]\n",
    "\n",
    "    # Selecionando apenas janelas VALIDAS (ocorrencia de ao menos 1 carga)\n",
    "    # TODO: Implementar na biblioteca esta rotina de validacao\n",
    "    if eliminar_janelas_vazias:\n",
    "        idx_janelas_validas = np.where(np.sum(X, axis=1)>0)[0]\n",
    "        X = X[idx_janelas_validas]\n",
    "        #for i in range(len(dados_aparelhos)):\n",
    "        dados_aparelho[\"janelas\"] = dados_aparelho[\"janelas\"][idx_janelas_validas]\n",
    "        rotulos = copy.deepcopy(dados_aparelho[\"rotulos\"])\n",
    "        dados_aparelho[\"rotulos\"][\"estado\"] = rotulos[\"estado\"][idx_janelas_validas]\n",
    "        dados_aparelho[\"rotulos\"][\"media\"]  = rotulos[\"media\"][idx_janelas_validas]\n",
    "        dados_aparelho[\"rotulos\"][\"total\"]  = rotulos[\"total\"][idx_janelas_validas]\n",
    "        if debug:\n",
    "            print(\"   - `{}-{}`: {} => {}\".format(\n",
    "                dados_aparelho[\"carga\"].upper(), \n",
    "                dados_aparelho[\"instancia\"],\n",
    "                Counter(rotulos[\"estado\"]),\n",
    "                Counter(dados_aparelho[\"rotulos\"][\"estado\"])\n",
    "            ))\n",
    "\n",
    "    # y\n",
    "    y = dados_aparelho[\"rotulos\"][\"estado\"]\n",
    "\n",
    "    # <<< Limpando memoria >>>\n",
    "    dados_cargas = None\n",
    "    del dados_cargas\n",
    "    dados_medidores = None\n",
    "    del dados_medidores\n",
    "    dados_aparelho = None\n",
    "    del dados_aparelho\n",
    "    gc.collect()\n",
    "    # <<< Limpando memoria >>>\n",
    "\n",
    "    # Fazendo split dos dados (treino/teste)\n",
    "    if split_teste is None:\n",
    "        return X, y\n",
    "    else:\n",
    "        X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
    "            X, y, \n",
    "            test_size=split_teste,\n",
    "            stratify=y,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        return X_treino, X_teste, y_treino, y_teste        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melhores Combinações de Taxas e Janelas para cada Aparelho (estudo 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T13:32:45.264152Z",
     "start_time": "2020-08-08T13:32:45.194100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carga</th>\n",
       "      <th>taxa_amostragem</th>\n",
       "      <th>janela</th>\n",
       "      <th>loss</th>\n",
       "      <th>acuracia</th>\n",
       "      <th>precisao</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sockets - 3</td>\n",
       "      <td>3</td>\n",
       "      <td>90</td>\n",
       "      <td>2.314749e-01</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>40.00</td>\n",
       "      <td>66.67</td>\n",
       "      <td>50.00</td>\n",
       "      <td>74.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sockets - 4</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>1.049300e-09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>light - 5</td>\n",
       "      <td>4</td>\n",
       "      <td>360</td>\n",
       "      <td>7.989106e-03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ce_appliance - 6</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>1.013742e-09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fridge - 7</td>\n",
       "      <td>8</td>\n",
       "      <td>1080</td>\n",
       "      <td>4.944706e-04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>waste_disposal_unit - 8</td>\n",
       "      <td>4</td>\n",
       "      <td>900</td>\n",
       "      <td>2.370523e+00</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>11.11</td>\n",
       "      <td>50.00</td>\n",
       "      <td>18.18</td>\n",
       "      <td>53.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dish_washer - 9</td>\n",
       "      <td>5</td>\n",
       "      <td>360</td>\n",
       "      <td>9.249995e-02</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>74.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>electric_furnace - 10</td>\n",
       "      <td>5</td>\n",
       "      <td>720</td>\n",
       "      <td>1.154453e+00</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>83.33</td>\n",
       "      <td>62.50</td>\n",
       "      <td>71.43</td>\n",
       "      <td>82.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>light - 11</td>\n",
       "      <td>8</td>\n",
       "      <td>900</td>\n",
       "      <td>2.041479e-01</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>85.71</td>\n",
       "      <td>85.71</td>\n",
       "      <td>85.71</td>\n",
       "      <td>89.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sockets - 12</td>\n",
       "      <td>5</td>\n",
       "      <td>540</td>\n",
       "      <td>2.085883e+00</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>75.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>42.86</td>\n",
       "      <td>67.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>washer_dryer - 13</td>\n",
       "      <td>3</td>\n",
       "      <td>900</td>\n",
       "      <td>2.585185e-04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>washer_dryer - 14</td>\n",
       "      <td>3</td>\n",
       "      <td>900</td>\n",
       "      <td>1.416176e-01</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>66.67</td>\n",
       "      <td>100.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>89.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>light - 15</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>4.583221e-04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>microwave - 16</td>\n",
       "      <td>8</td>\n",
       "      <td>180</td>\n",
       "      <td>7.108389e-01</td>\n",
       "      <td>0.971963</td>\n",
       "      <td>100.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>57.14</td>\n",
       "      <td>77.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>light - 17</td>\n",
       "      <td>5</td>\n",
       "      <td>900</td>\n",
       "      <td>1.932175e+00</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>92.86</td>\n",
       "      <td>81.25</td>\n",
       "      <td>86.67</td>\n",
       "      <td>88.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>smoke_alarm - 18</td>\n",
       "      <td>8</td>\n",
       "      <td>1080</td>\n",
       "      <td>9.479684e-05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>light - 19</td>\n",
       "      <td>5</td>\n",
       "      <td>1080</td>\n",
       "      <td>2.895203e+00</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>91.67</td>\n",
       "      <td>84.62</td>\n",
       "      <td>88.00</td>\n",
       "      <td>89.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>unknown - 20</td>\n",
       "      <td>8</td>\n",
       "      <td>1080</td>\n",
       "      <td>1.307548e+00</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>100.00</td>\n",
       "      <td>66.67</td>\n",
       "      <td>80.00</td>\n",
       "      <td>88.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      carga  taxa_amostragem  janela          loss  acuracia  \\\n",
       "0               sockets - 3                3      90  2.314749e-01  0.986014   \n",
       "1               sockets - 4                3      30  1.049300e-09  1.000000   \n",
       "2                 light - 5                4     360  7.989106e-03  1.000000   \n",
       "3          ce_appliance - 6                3      30  1.013742e-09  1.000000   \n",
       "4                fridge - 7                8    1080  4.944706e-04  1.000000   \n",
       "5   waste_disposal_unit - 8                4     900  2.370523e+00  0.790698   \n",
       "6           dish_washer - 9                5     360  9.249995e-02  0.976744   \n",
       "7     electric_furnace - 10                5     720  1.154453e+00  0.906977   \n",
       "8                light - 11                8     900  2.041479e-01  0.909091   \n",
       "9              sockets - 12                5     540  2.085883e+00  0.859649   \n",
       "10        washer_dryer - 13                3     900  2.585185e-04  1.000000   \n",
       "11        washer_dryer - 14                3     900  1.416176e-01  0.982456   \n",
       "12               light - 15                5      30  4.583221e-04  1.000000   \n",
       "13           microwave - 16                8     180  7.108389e-01  0.971963   \n",
       "14               light - 17                5     900  1.932175e+00  0.885714   \n",
       "15         smoke_alarm - 18                8    1080  9.479684e-05  1.000000   \n",
       "16               light - 19                5    1080  2.895203e+00  0.896552   \n",
       "17             unknown - 20                8    1080  1.307548e+00  0.944444   \n",
       "\n",
       "    precisao  recall      f1  f1_macro  \n",
       "0      40.00   66.67   50.00     74.65  \n",
       "1       0.00    0.00    0.00    100.00  \n",
       "2       0.00    0.00    0.00    100.00  \n",
       "3     100.00  100.00  100.00    100.00  \n",
       "4     100.00  100.00  100.00    100.00  \n",
       "5      11.11   50.00   18.18     53.09  \n",
       "6      50.00   50.00   50.00     74.40  \n",
       "7      83.33   62.50   71.43     82.94  \n",
       "8      85.71   85.71   85.71     89.52  \n",
       "9      75.00   30.00   42.86     67.43  \n",
       "10    100.00  100.00  100.00    100.00  \n",
       "11     66.67  100.00   80.00     89.54  \n",
       "12      0.00    0.00    0.00    100.00  \n",
       "13    100.00   40.00   57.14     77.85  \n",
       "14     92.86   81.25   86.67     88.33  \n",
       "15      0.00    0.00    0.00    100.00  \n",
       "16     91.67   84.62   88.00     89.45  \n",
       "17    100.00   66.67   80.00     88.39  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_melhores_taxas_janelas = pd.read_csv(os.path.join(caminho_dados, \"19\", \"melhores_taxa_janela_aparelhos.csv\"), index_col=0)\n",
    "df_melhores_taxas_janelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T13:32:45.866160Z",
     "start_time": "2020-08-08T13:32:45.859189Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# - Desenvolver módulo da metodologia na lib PyNILM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros de RP dos Aparelhos (estudo 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T13:32:47.598612Z",
     "start_time": "2020-08-08T13:32:47.553613Z"
    }
   },
   "outputs": [],
   "source": [
    "# Carregando arquivos de parametros, caso n estejam (kernel reiniciado)\n",
    "if not 'parametros_rp_aparelho' in locals():\n",
    "    with open(os.path.join(caminho_dados, \"18\", \"parametros_rp_aparelho.json\"),'r') as arquivo:\n",
    "        parametros_rp_aparelho = json.load(arquivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodologia\n",
    "---\n",
    "\n",
    "Aplicação da metodologia nos aparelhos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiente e Funções Auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando ambiente de desenvolvimento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T13:33:04.465869Z",
     "start_time": "2020-08-08T13:32:52.442172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# GPUs habilitadas: 1 física(s) / 1 lógica(s)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from PyNILM.dados.janelas import Janelas\n",
    "from PyNILM.dados.janelas import Janelas\n",
    "from PyNILM.avaliacao.metricas import *\n",
    "from PyNILM.avaliacao.graficos import *\n",
    "from PyNILM.avaliacao.analises import *\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from pyts.image import RecurrencePlot, GramianAngularField\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow.keras import backend as K\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Garantindo reprodutibilidade\n",
    "import random as rn\n",
    "# Constantes dos experimentos\n",
    "SEED = 33\n",
    "FRACAO_TESTE = 0.25\n",
    "EPOCAS = 100\n",
    "TAMANHO_LOTE = 32\n",
    "VERBOSIDADE = 2\n",
    "# Parametros RP (verificado empiricamente)\n",
    "PARAMETROS_RP = {\n",
    "    \"dimension\": 1,\n",
    "    \"time_delay\": 1,\n",
    "    \"threshold\": None,\n",
    "    \"percentage\": 10\n",
    "}\n",
    "TAMANHO_IMAGEM = (32,32)\n",
    "# PARAMETROS_RP={\n",
    "#     \"dimension\": 3,\n",
    "#     \"time_delay\": 8,\n",
    "#     \"threshold\": 0.1\n",
    "# }\n",
    "# Travar Seed's\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Habilitando/limitando utilização de GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 4GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*3)]\n",
    "        )\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(\"# GPUs habilitadas:\", \"{} física(s)\".format(len(gpus)), \"/\", \"{} lógica(s)\".format(len(gpus)))\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -K.mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotinas da Etapa de Modelagem de DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T13:33:05.064610Z",
     "start_time": "2020-08-08T13:33:04.880539Z"
    }
   },
   "outputs": [],
   "source": [
    "def convnet_metodologia(\n",
    "    input_shape_ = (32, 32, 1), \n",
    "    output_dim=1, \n",
    "    optimizer='adam',\n",
    "    loss_function = 'binary_crossentropy', \n",
    "    metrics=['accuracy'],\n",
    "    output_activation = 'sigmoid',\n",
    "    bias_output = None,\n",
    "):\n",
    "    \n",
    "    if bias_output is not None:\n",
    "        bias_output = tf.keras.initializers.Constant(bias_output)\n",
    "        \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", input_shape=input_shape_))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(output_dim, bias_initializer=bias_output, activation=output_activation))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=[loss_function], metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T13:33:05.631688Z",
     "start_time": "2020-08-08T13:33:05.446539Z"
    }
   },
   "outputs": [],
   "source": [
    "def centralizar_dados(X):\n",
    "    return np.array([x - x.mean() for x in X], dtype=TIPO_DADOS)\n",
    "\n",
    "def normalizar_dados(X):\n",
    "    X_ = np.empty(np.asarray(X).shape)\n",
    "    for i, x in enumerate(X):\n",
    "        if len(np.unique(x))>1:\n",
    "            X_[i] = (x - x.min()) / (x.max() - x.min())\n",
    "        elif x.max()>0:\n",
    "            X_[i] = x / x.max()\n",
    "        else:\n",
    "            X_[i] = x\n",
    "    return X_.astype(TIPO_DADOS)\n",
    "\n",
    "def padronizar_dados(X):\n",
    "    \"\"\"\n",
    "    Calcular z-score por amostra.\n",
    "    Ref.: https://datascience.stackexchange.com/questions/16034/dtw-dynamic-time-warping-requires-prior-normalization    \n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    \n",
    "    return np.array([stats.zscore(x) for x in X], dtype=TIPO_DADOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotinas do Gráfico de Recorrências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T13:33:06.373691Z",
     "start_time": "2020-08-08T13:33:06.045530Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construindo o pipeline de dados\n",
    "# ----------\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Constante fundamentais\n",
    "TAMANHO_IMAGEM = (32,32,1) # Apenas 1 canal\n",
    "TIPO_DADOS = np.float32\n",
    "def serie_para_imagem(serie, params_rp = PARAMETROS_RP, tam_imagem=TAMANHO_IMAGEM, \n",
    "                      normalizar=False, padronizar=False):\n",
    "    \"\"\"\n",
    "    Funcao responsavel por gerar e tratar a imagem RP (baseado estudo #17).\n",
    "    \"\"\"\n",
    "    # Gerando imagem RP/redimensiona_prndo\n",
    "    imagem = RecurrencePlot(**params_rp).fit_transform([serie])[0]\n",
    "    imagem = cv2.resize(\n",
    "            imagem, \n",
    "            dsize=tam_imagem[:2], \n",
    "            interpolation=cv2.INTER_CUBIC\n",
    "        ).astype(TIPO_DADOS)\n",
    "    \n",
    "    if np.sum(imagem) > 0:\n",
    "        # Normalizar\n",
    "        if normalizar:\n",
    "                imagem = (imagem - imagem.min()) / (imagem.max() - imagem.min()) # MinMax (0,1)\n",
    "            #imagem = (imagem - imagem.mean()) / np.max([imagem.std(), 1e-4])\n",
    "\n",
    "    #     # centralizar\n",
    "    #     if centralizar:\n",
    "    #         imagem -= imagem.mean()\n",
    "\n",
    "        # Padronizar\n",
    "        elif padronizar:\n",
    "            imagem = (imagem - imagem.mean())/imagem.std()#tf.image.per_image_standardization(imagem).numpy()\n",
    "\n",
    "    # N canais\n",
    "    imagem = np.stack([imagem for i in range(tam_imagem[-1])],axis=-1).astype(TIPO_DADOS)     \n",
    "    \n",
    "    return imagem\n",
    "\n",
    "def preparar_amostras(X, y, params_rp=PARAMETROS_RP, tam_imagem=TAMANHO_IMAGEM, normalizar=False, padronizar=False):\n",
    "    X_imagem = np.empty((len(X), *TAMANHO_IMAGEM))\n",
    "    for i, x in tqdm_notebook(enumerate(X), total=len(X)):\n",
    "        X_imagem[i,] = serie_para_imagem(\n",
    "            x, \n",
    "            params_rp=PARAMETROS_RP, \n",
    "            tam_imagem=TAMANHO_IMAGEM,\n",
    "            normalizar=normalizar,\n",
    "            padronizar=padronizar,\n",
    "        )\n",
    "    return X_imagem, y\n",
    "\n",
    "def preparar_amostra_tfdata(amostra, rotulo):\n",
    "    \"\"\"\n",
    "    Preparação da amostra/rótulo para o modelo.\n",
    "    \"\"\"\n",
    "    # Convertendo serie para imagem\n",
    "    amostra = tf.numpy_function(serie_para_imagem, [amostra], TIPO_DADOS)\n",
    "    amostra = tf.reshape(amostra, TAMANHO_IMAGEM)\n",
    "    return amostra, rotulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T13:33:06.995811Z",
     "start_time": "2020-08-08T13:33:06.773810Z"
    }
   },
   "outputs": [],
   "source": [
    "from nolitsa import delay, dimension, utils    \n",
    "\n",
    "def obter_delay_autocorrelacao(x):\n",
    "    # Compute autocorrelation and delayed mutual information.\n",
    "    try:\n",
    "        if np.unique(x).shape[0] > 1:\n",
    "            r = delay.acorr(x, maxtau=x.shape[0])\n",
    "            r_delay = np.argmax(r < 1.0 / np.e)\n",
    "        else:\n",
    "            r_delay = 1\n",
    "    except:\n",
    "        r_delay = 1\n",
    "    finally:\n",
    "        return r_delay\n",
    "\n",
    "def dimensoes_validas(serie, dimensoes, delay, maxnum=None):\n",
    "    \"\"\"\n",
    "    Baseado em:\n",
    "        (linha 97 - https://github.com/manu-mannattil/nolitsa/blob/master/nolitsa/dimension.py); e\n",
    "        (linha 165 - https://github.com/manu-mannattil/nolitsa/blob/master/nolitsa/utils.py)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if not maxnum:\n",
    "        maxnum = (delay + 1) + 1 + (delay + 1)\n",
    "    else:\n",
    "        maxnum = max(1, maxnum)\n",
    "        \n",
    "    d = []\n",
    "    comprimento_serie = len(serie)\n",
    "    for dimensao in (dimensoes):\n",
    "        # Reconstrucao do espaço de fases, se viavel\n",
    "        try:\n",
    "            y = utils.reconstruct(serie[:-delay], dimensao, delay)\n",
    "            if maxnum < len(y):\n",
    "                d.append(dimensao)\n",
    "        except:\n",
    "            pass\n",
    "    return d\n",
    "\n",
    "def obter_dimensao_falsosvizinhos(serie, dimensoes_avaliacao, delay, metrica=\"euclidean\"):\n",
    "    \n",
    "    try: \n",
    "        f1, f2, f3 = dimension.fnn(\n",
    "            serie, \n",
    "            tau=delay, \n",
    "            dim=dimensoes_avaliacao, \n",
    "            window=delay, \n",
    "            metric=metrica\n",
    "        ) \n",
    "        embedding_dimension = f3.argmin()+1\n",
    "    except:\n",
    "        embedding_dimension = 1\n",
    "    finally:\n",
    "        return embedding_dimension\n",
    "    \n",
    "def k_valores_mais_frequentes(valores, k=10, ascendente=True):\n",
    "    ordernado = sorted(Counter(valores).most_common(k), key=lambda tup: tup[0])\n",
    "    counter_ordenado = sorted(ordernado, key=lambda tup: tup[1], reverse=True)\n",
    "    return counter_ordenado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicação da Metodologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T13:33:08.012580Z",
     "start_time": "2020-08-08T13:33:07.700402Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "\n",
    "janelas = Janelas(**PARAMETROS_DATASET)\n",
    "\n",
    "def classificacao_threshold(modelo, X, y, threshold={0:0.5, 1:0.5}, grafico=True, debug=False):\n",
    "    y_hat = modelo.predict(X_teste_imagem).round().astype(np.int16)\n",
    "    y_proba = np.array([y[0] for y in modelo.predict(X_teste_imagem).round(3)])\n",
    "    y_proba_ = np.array([1-proba if proba <= .5 else proba for proba in y_proba])\n",
    "\n",
    "    # Graficos de analise\n",
    "    if grafico:\n",
    "        fig,axes = plt.subplots(2,2, figsize=(20,10));\n",
    "\n",
    "        sns.distplot(y_proba, ax=axes[0,0], color='black')\n",
    "        axes[0,0].set_title(\"Saída Sigmoid Modelo\");\n",
    "\n",
    "        sns.distplot(y_proba_, ax=axes[0,1], color=\"orange\")\n",
    "        axes[0,1].set_title(\"Probabilidades Escalanodas [0,1]\");\n",
    "\n",
    "        plt.suptitle(\"Distribuição de Probabilidades Inferidas\", size=18, y=1.05);\n",
    "\n",
    "        sns.distplot(y_proba_[y_teste==0], ax=axes[1,0], color='red')\n",
    "        axes[1,0].set_title(\"Probabilidades p/ CLASSE `0`\");\n",
    "\n",
    "        sns.distplot(y_proba_[y_teste==1], ax=axes[1,1], color='g')\n",
    "        axes[1,1].set_title(\"Probabilidades p/ CLASSE `1`\");\n",
    "\n",
    "        plt.tight_layout();\n",
    "        plt.show()\n",
    "\n",
    "    # Registros avaliados threshold p/ classe\n",
    "    y_teste_ = []\n",
    "    y_pred_ =  []\n",
    "    idx_descartados = []\n",
    "    \n",
    "    for i, c in enumerate(threshold.keys()):\n",
    "        \n",
    "        # Selecionando registros por classe\n",
    "        idx_classe = [y_[0] for y_ in np.argwhere(y_teste == c)]\n",
    "        y_teste_classe = np.array(y_teste[idx_classe])\n",
    "        y_pred_classe = np.array(y_hat[idx_classe])\n",
    "        y_proba_classe = np.array(y_proba_[idx_classe])\n",
    "\n",
    "        # Filtrando pela probabilidade da classe\n",
    "        idx_threshold = [y_[0] for y_ in np.argwhere(y_proba_classe >= threshold[c])]\n",
    "        idx_descartados_classe = list(\n",
    "            set(range(len(y_proba_classe))) - set(idx_threshold)\n",
    "        ) \n",
    "        \n",
    "        y_teste_.extend(y_teste_classe[idx_threshold])\n",
    "        y_pred_.extend(y_pred_classe[idx_threshold])\n",
    "        if i == 0:\n",
    "            idx_descartados.extend(idx_descartados_classe)\n",
    "        else:\n",
    "            idx_descartados.extend(np.array(idx_descartados_classe) + len(idx_classe))\n",
    "    \n",
    "    # Verificar o impacto do threshold\n",
    "    suporte_relativo = (len(y_teste_)/len(y_teste))*100\n",
    "    if debug:\n",
    "        print(\"   - Análise de Suporte: {:.1f}% dos registros de teste considerados\".format(\n",
    "            suporte_relativo\n",
    "        ) )\n",
    "        print()\n",
    "\n",
    "        print(\"   - Métricas:\")\n",
    "        print()\n",
    "        \n",
    "    acc = accuracy_score(y_teste_, y_pred_)\n",
    "    f1_ = f1_score(y_teste_, y_pred_, average=\"macro\")\n",
    "    try:\n",
    "        auc_ = roc_auc_score(y_teste_, y_pred_)\n",
    "    except:\n",
    "        if f1_ == 1:\n",
    "            auc_ = 1\n",
    "        else:\n",
    "            auc_ = 0.5\n",
    "    if debug:\n",
    "        print(\"     _ Accuracy: {:.2f}%\".format(acc))\n",
    "        print(\"     _ F1-macro: {:.2f}%\".format(f1_))\n",
    "        print(\"     _ AUC     : {:.3f}\".format(auc_))\n",
    "        print()\n",
    "        print(\"   - Relatório de classificação:\")\n",
    "        print()\n",
    "        print(classification_report(y_teste_, y_pred_))\n",
    "        print(\"   - Matrix de confusao:\")\n",
    "        print()\n",
    "        print(confusion_matrix(y_teste_, y_pred_))\n",
    "\n",
    "    return y_teste_, y_pred_, acc, f1_, auc_, suporte_relativo, idx_descartados\n",
    "\n",
    "def calcular_carga_desconsiderada(y_pred, y_teste, X_teste, modo_calculo = \"positivo\"):\n",
    "    if len(y_hat.shape)>1:\n",
    "        y_pred = np.squeeze(y_pred)\n",
    "\n",
    "    # Erros de classificacao\n",
    "    idx_erros = [y_[0] for y_ in np.argwhere(np.equal(y_pred, y_teste) == False)]\n",
    "\n",
    "    # Totalizar por erro cometido pelo modelo\n",
    "    if modo_calculo == \"positivo\":\n",
    "        total_por_erro = []\n",
    "        for ie in idx_erros:\n",
    "            if y_teste[ie] == 1:\n",
    "                total_por_erro.append(np.sum(X_teste[ie]))\n",
    "    elif modo_calculo == \"negativo\":\n",
    "        total_por_erro = []\n",
    "        for ie in idx_erros:\n",
    "            if y_teste[ie] == 0:\n",
    "                total_por_erro.append(np.sum(X_teste[ie]))\n",
    "    else:\n",
    "        total_por_erro = np.sum(X_teste[idx_erros], axis=1)\n",
    "    return total_por_erro\n",
    "    #np.sum(total_por_erro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T13:33:09.030807Z",
     "start_time": "2020-08-08T13:33:08.843163Z"
    }
   },
   "outputs": [],
   "source": [
    "def instancia_aparelho_residencia(aparelho, residencia, base = redd):\n",
    "    \"\"\"Função para coletar o id/instancia do aparelho na residencia,\n",
    "    permitindo executar os testes independente da residencia\"\"\"\n",
    "    instancia = []\n",
    "    #for e in base.buildings[residencia].elec.all_meters():\n",
    "    for e_i in range(1, len(janelas.base.buildings[residencia].elec.all_meters())):\n",
    "\n",
    "        # Selecionando canal/aparelho\n",
    "        e = janelas.base.buildings[residencia].elec[e_i]\n",
    "        \n",
    "        if not hasattr(e,'meters'):\n",
    "            if e.label().lower().replace(\" \",\"_\") == aparelho:\n",
    "                instancia.append( e.instance() )\n",
    "        else:\n",
    "            for e_ in e.meters:\n",
    "                if e_.label().lower().replace(\" \",\"_\") == aparelho:\n",
    "                    instancia.append( e_.instance() )\n",
    "    return instancia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodologia Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Treino/Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T13:52:32.962141Z",
     "start_time": "2020-08-08T13:52:32.692128Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Aparelho FRIDGE:\n",
      "\n",
      "   - Carregando dados (taxa=8, janela=1080 - instancia 8)...\n",
      "# ERRO: não foi possível carregar os dados do aparelho `fridge - 8` ('NoneType' object has no attribute 'values')\n",
      "\n",
      "* Aparelho DISH_WASHER:\n",
      "\n",
      "   - Carregando dados (taxa=5, janela=360 - instancia 9)...\n",
      "# ERRO: não foi possível carregar os dados do aparelho `dish_washer - 9` ('NoneType' object has no attribute 'values')\n",
      "\n",
      "* Aparelho WASHER_DRYER:\n",
      "\n",
      "   - Carregando dados (taxa=3, janela=900 - instancia 4)...\n",
      "# ERRO: não foi possível carregar os dados do aparelho `washer_dryer - 4` ('NoneType' object has no attribute 'values')\n",
      "\n",
      "* Aparelho WASHER_DRYER:\n",
      "\n",
      "   - Carregando dados (taxa=3, janela=900 - instancia 4)...\n",
      "# ERRO: não foi possível carregar os dados do aparelho `washer_dryer - 4` ('NoneType' object has no attribute 'values')\n",
      "\n",
      "* Aparelho MICROWAVE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultados = {\n",
    "    \"aparelho\": [],\n",
    "    \"teste\": [],\n",
    "    \"acuracia\": [],\n",
    "    \"f1\": [],\n",
    "    \"auc\":[],\n",
    "}\n",
    "for rotulo_aparelho in df_melhores_taxas_janelas.loc[\n",
    "    df_melhores_taxas_janelas[\"carga\"].isin(\n",
    "        ['dish_washer - 9','fridge - 7','microwave - 16','washer_dryer - 13',\n",
    "         'washer_dryer - 14']),\n",
    "    : ][\"carga\"].values:\n",
    "    \n",
    "    \n",
    "    # Informacoes da carga selecionada\n",
    "    CARGA = rotulo_aparelho.split(\" - \")[0]\n",
    "    #INSTANCIA = int(rotulo_aparelho.split(\" - \")[1])\n",
    "    \n",
    "\n",
    "    print(f\"* Aparelho {CARGA.upper()}:\")\n",
    "    print()\n",
    "\n",
    "    config_aparelho = df_melhores_taxas_janelas[\n",
    "        df_melhores_taxas_janelas[\"carga\"]==rotulo_aparelho\n",
    "    ].to_dict(\"records\")[0]\n",
    "    TAXA = config_aparelho[\"taxa_amostragem\"]\n",
    "    TAMANHO_JANELA = config_aparelho[\"janela\"]\n",
    "    CONFIG_RP_APARELHO = PARAMETROS_RP\n",
    "\n",
    "    # Percorrer instancias do aparelho na residencia\n",
    "    for INSTANCIA in instancia_aparelho_residencia(CARGA, RESIDENCIA, base = redd):\n",
    "        \n",
    "        # Extrair series divididas em janelas para cada medidor\n",
    "        print(\"   - Carregando dados (taxa={:.0f}, janela={:.0f} - instancia {})...\".format(\n",
    "            TAXA, TAMANHO_JANELA, INSTANCIA\n",
    "        ))\n",
    "        try:\n",
    "            X, y = carregar_dados_aparelho(\n",
    "                janelas=janelas,\n",
    "                instancia=INSTANCIA,\n",
    "                aparelho=CARGA,\n",
    "                tamanho_janela=TAMANHO_JANELA,\n",
    "                taxa=TAXA,\n",
    "                eliminar_janelas_vazias=True\n",
    "            )\n",
    "            print()\n",
    "            #break\n",
    "\n",
    "            print(\"   - Detalhes da amostragem (lotes):\")\n",
    "            print(\"   ---\")\n",
    "            for item in Counter(y).items():\n",
    "                print(f\"      - Classe `{item[0]}`: {item[1]} amostras ({round(item[1]/len(y)*100,1)}%)\" )\n",
    "            print()\n",
    "\n",
    "            # Checando series estaveis\n",
    "            estavel = []\n",
    "            for i, x in enumerate(X):\n",
    "                if len(np.unique(x)) == 1:\n",
    "                    estavel.append(i)\n",
    "            print(\"      - Séries estáveis (1 amplitude)         : {} ({:.2f}%)\".format(len(estavel), len(estavel)/len(X)*100) )\n",
    "            print(\"      - Distribuicao de classes nestas séries :\", Counter(y[estavel]))\n",
    "            print()\n",
    "\n",
    "            print(\"      - Estatísticas das séries: = Min. / Max. / Média / STD:\", X.min(), X.max(), X.mean(), X.std())\n",
    "            print()\n",
    "\n",
    "            # Convertendo series para imagem\n",
    "            print(\"   - Preparando dados para modelagem (treino/teste)...\")\n",
    "            print(\"   ---\")\n",
    "            # Fazendo split dos dados (treino/teste)\n",
    "            X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
    "                X, y, \n",
    "                test_size=FRACAO_TESTE,\n",
    "                stratify=y if Counter(y)[0] > 1 and Counter(y)[1] > 1 else None,\n",
    "                random_state=SEED\n",
    "            )\n",
    "            print()\n",
    "\n",
    "            print(\"      - Distribuições dos lotes:\")\n",
    "            print(\"        -> Treino:\", Counter(y_treino))\n",
    "            print(\"        -> Teste :\", Counter(y_teste))\n",
    "\n",
    "            X_treino_imagem, _ = preparar_amostras(\n",
    "                X_treino, y_treino, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "            X_teste_imagem, _ = preparar_amostras(\n",
    "                X_teste, y_teste, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "\n",
    "            # Normalizar dados p/ modelo\n",
    "            X_treino_imagem = normalizar_dados(X_treino_imagem)\n",
    "            X_teste_imagem = normalizar_dados(X_teste_imagem)\n",
    "\n",
    "            print()\n",
    "\n",
    "            print(\"   - Avaliando modelo, aguarde...\")\n",
    "            print(\"   ---\")\n",
    "            y_true, y_pred = [], []\n",
    "            accs = []\n",
    "            scores = []\n",
    "            aucs = []\n",
    "\n",
    "            # Avaliando N vezes o conjunto treino/teste (consitencia)\n",
    "            N = 10\n",
    "            for i in tqdm_notebook(range(N)):\n",
    "\n",
    "                ##################### METODOLOGIA ANTIGA #####################\n",
    "                # Modelo baseline\n",
    "                modelo = convnet_metodologia(\n",
    "                    input_shape_= TAMANHO_IMAGEM,\n",
    "                    output_dim = 1,\n",
    "                    loss_function='binary_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    output_activation='sigmoid'\n",
    "                )\n",
    "                # Treinando\n",
    "                historico = modelo.fit(\n",
    "                    X_treino_imagem, y_treino,\n",
    "                    validation_data=(X_teste_imagem, y_teste),\n",
    "                    epochs=EPOCAS,\n",
    "                    batch_size=TAMANHO_LOTE,\n",
    "                    #verbose=VERBOSIDADE\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Avaliando\n",
    "                y_hat = modelo.predict(X_teste_imagem).round().astype(np.int16)\n",
    "                ##################### METODOLOGIA ANTIGA #####################\n",
    "\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "                # Incrementando resultados\n",
    "                acc = accuracy_score(y_teste, y_hat)\n",
    "                score = f1_score(y_teste, y_hat, average=\"macro\")\n",
    "                try:\n",
    "                    auc_score = roc_auc_score(y_teste, y_hat)\n",
    "                except:\n",
    "                    if score == 1:\n",
    "                        auc_score = 1\n",
    "                    else:\n",
    "                        auc_score = 0.5\n",
    "                print(\"      -> # TENTATIVA #{}: Acurácia = {:.2f}% / F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                    i+1, \n",
    "                    acc*100,\n",
    "                    score*100,\n",
    "                    auc_score,\n",
    "                ))\n",
    "                accs.append(acc)\n",
    "                scores.append(score)\n",
    "                aucs.append(auc_score)\n",
    "                y_true.extend(y_teste)\n",
    "                y_pred.extend(y_hat)\n",
    "\n",
    "                # Guardando resultados do modelo\n",
    "                resultados[\"aparelho\"].append(rotulo_aparelho)\n",
    "                resultados[\"teste\"].append(i+1)\n",
    "                resultados[\"acuracia\"].append(acc)\n",
    "                resultados[\"f1\"].append(score)\n",
    "                resultados[\"auc\"].append(auc_score)\n",
    "        \n",
    "\n",
    "            print()\n",
    "            print(\"   - Resultados finais:\")\n",
    "            print(\"   ---\")\n",
    "            print()\n",
    "\n",
    "            print(\"      -> Acurácia:\")\n",
    "            print()\n",
    "            print(\"         . Média geral   : {:.2f}%\".format(np.mean(accs)*100 ) )\n",
    "            print(\"         . Desvio padrão : {:.2f}%\".format(np.std(accs)*100) )\n",
    "            print(\"         . Mínimo        : {:.2f}%\".format(np.min(accs)*100) )\n",
    "            print(\"         . Máximo        : {:.2f}%\".format(np.max(accs)*100) )\n",
    "            print()\n",
    "\n",
    "            print(\"      -> F1-score (macro):\")\n",
    "            print()\n",
    "            print(\"         . Média geral   : {:.2f}%\".format(np.mean(scores)*100 ) )\n",
    "            print(\"         . Desvio padrão : {:.2f}%\".format(np.std(scores)*100) )\n",
    "            print(\"         . Mínimo        : {:.2f}%\".format(np.min(scores)*100) )\n",
    "            print(\"         . Máximo        : {:.2f}%\".format(np.max(scores)*100) )\n",
    "            print()\n",
    "            print(\"      -> AUC:\")\n",
    "            print()\n",
    "            print(\"         . Média geral   : {:.3f}\".format(np.mean(aucs) ) )\n",
    "            print(\"         . Desvio padrão : {:.3f}\".format(np.std(aucs)) )\n",
    "            print(\"         . Mínimo        : {:.3f}\".format(np.min(aucs)) )\n",
    "            print(\"         . Máximo        : {:.3f}\".format(np.max(aucs)) )\n",
    "            print()\n",
    "\n",
    "            print(\"      -> Relatório de classificação:\")\n",
    "            print()\n",
    "            print(classification_report(y_true, y_pred))\n",
    "            print(\"      -> Matrix de confusao:\")\n",
    "            print()\n",
    "            print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "            print()\n",
    "            print(\"-\"*80)\n",
    "            print()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"# ERRO: não foi possível carregar os dados do aparelho `{CARGA} - {INSTANCIA}` ({str(e)})\")\n",
    "            print()\n",
    "    \n",
    "# Exportando resultados\n",
    "df_baseline = pd.DataFrame(resultados)\n",
    "df_baseline[\"metodologia\"] = \"Baseline\"\n",
    "df_baseline.to_excel(\n",
    "    os.path.join(caminho_dados_notebook, \"resultados_baseline.xlsx\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.942625Z",
     "start_time": "2020-08-05T11:27:47.368Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultados = {\n",
    "    \"aparelho\": [],\n",
    "    \"iteracao\": [],\n",
    "    \"teste\": [],\n",
    "    \"acuracia\": [],\n",
    "    \"f1\": [],\n",
    "    \"auc\":[],\n",
    "}\n",
    "for rotulo_aparelho in df_melhores_taxas_janelas.loc[\n",
    "    df_melhores_taxas_janelas[\"carga\"].isin(\n",
    "        ['dish_washer - 9','fridge - 7','microwave - 16','washer_dryer - 13',\n",
    "         'washer_dryer - 14']),\n",
    "    : ][\"carga\"].values:\n",
    "    \n",
    "    print(f\"* Aparelho {rotulo_aparelho.upper()}:\")\n",
    "    print()\n",
    "    \n",
    "    # Informacoes da carga selecionada\n",
    "    CARGA = rotulo_aparelho.split(\" - \")[0]\n",
    "    #INSTANCIA = int(rotulo_aparelho.split(\" - \")[1])\n",
    "\n",
    "    config_aparelho = df_melhores_taxas_janelas[\n",
    "        df_melhores_taxas_janelas[\"carga\"]==rotulo_aparelho\n",
    "    ].to_dict(\"records\")[0]\n",
    "    TAXA = config_aparelho[\"taxa_amostragem\"]\n",
    "    TAMANHO_JANELA = config_aparelho[\"janela\"]\n",
    "    CONFIG_RP_APARELHO = PARAMETROS_RP\n",
    "    \n",
    "    # Percorrer instancias do aparelho na residencia\n",
    "    for INSTANCIA in instancia_aparelho_residencia(CARGA, RESIDENCIA, base = redd):\n",
    "        \n",
    "        # Extrair series divididas em janelas para cada medidor\n",
    "        print(\"   - Carregando dados (taxa={:.0f}, janela={:.0f})...\".format(\n",
    "            TAXA, TAMANHO_JANELA\n",
    "        ))\n",
    "        X, y = carregar_dados_aparelho(\n",
    "            janelas=janelas,\n",
    "            instancia=INSTANCIA,\n",
    "            aparelho=CARGA,\n",
    "            tamanho_janela=TAMANHO_JANELA,\n",
    "            taxa=TAXA,\n",
    "            eliminar_janelas_vazias=True\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        print(\"   - Detalhes da amostragem (lotes):\")\n",
    "        print(\"   ---\")\n",
    "        for item in Counter(y).items():\n",
    "            print(f\"      - Classe `{item[0]}`: {item[1]} amostras ({round(item[1]/len(y)*100,1)}%)\" )\n",
    "        print()\n",
    "\n",
    "        # Checando series estaveis\n",
    "        estavel = []\n",
    "        for i, x in enumerate(X):\n",
    "            if len(np.unique(x)) == 1:\n",
    "                estavel.append(i)\n",
    "        print(\"      - Séries estáveis (1 amplitude)         : {} ({:.2f}%)\".format(len(estavel), len(estavel)/len(X)*100) )\n",
    "        print(\"      - Distribuicao de classes nestas séries :\", Counter(y[estavel]))\n",
    "        print()\n",
    "\n",
    "        print(\"      - Estatísticas das séries: = Min. / Max. / Média / STD:\", X.min(), X.max(), X.mean(), X.std())\n",
    "        print()\n",
    "\n",
    "        # Convertendo series para imagem (CV)\n",
    "        for it, (idx_treino, idx_teste) in enumerate(kfold.split(X, y)):\n",
    "\n",
    "            iteracao = it + 1\n",
    "\n",
    "            print(f\"   - Preparando dados para modelagem (cv-{iteracao})...\")\n",
    "            print(\"   ---\")\n",
    "            X_treino, X_teste = X[idx_treino], X[idx_teste]\n",
    "            y_treino, y_teste = y[idx_treino], y[idx_teste]\n",
    "            print()\n",
    "\n",
    "            print(\"      - Distribuições dos lotes:\")\n",
    "            print(\"        -> Treino:\", Counter(y_treino))\n",
    "            print(\"        -> Teste :\", Counter(y_teste))\n",
    "\n",
    "            X_treino_imagem, _ = preparar_amostras(\n",
    "                X_treino, y_treino, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "            X_teste_imagem, _ = preparar_amostras(\n",
    "                X_teste, y_teste, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "\n",
    "            # Normalizar dados p/ modelo\n",
    "            X_treino_imagem = normalizar_dados(X_treino_imagem)\n",
    "            X_teste_imagem = normalizar_dados(X_teste_imagem)\n",
    "\n",
    "            print()\n",
    "\n",
    "            print(\"   - Avaliando modelo, aguarde...\")\n",
    "            print(\"   ---\")\n",
    "            y_true, y_pred = [], []\n",
    "            accs = []\n",
    "            scores = []\n",
    "            aucs = []\n",
    "\n",
    "            # Avaliando N vezes o conjunto treino/teste (consitencia)\n",
    "            N = 10\n",
    "            for i in tqdm_notebook(range(N)):\n",
    "\n",
    "                ##################### METODOLOGIA ANTIGA #####################\n",
    "                # Modelo baseline\n",
    "                modelo = convnet_metodologia(\n",
    "                    input_shape_= TAMANHO_IMAGEM,\n",
    "                    output_dim = 1,\n",
    "                    loss_function='binary_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    output_activation='sigmoid'\n",
    "                )\n",
    "                # Treinando\n",
    "                historico = modelo.fit(\n",
    "                    X_treino_imagem, y_treino,\n",
    "                    validation_data=(X_teste_imagem, y_teste),\n",
    "                    epochs=EPOCAS,\n",
    "                    batch_size=TAMANHO_LOTE,\n",
    "                    #verbose=VERBOSIDADE\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Avaliando\n",
    "                y_hat = modelo.predict(X_teste_imagem).round().astype(np.int16)\n",
    "                ##################### METODOLOGIA ANTIGA #####################\n",
    "\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "                # Incrementando resultados\n",
    "                acc = accuracy_score(y_teste, y_hat)\n",
    "                score = f1_score(y_teste, y_hat, average=\"macro\")\n",
    "                try:\n",
    "                    auc_score = roc_auc_score(y_teste, y_hat)\n",
    "                except:\n",
    "                    if score == 1:\n",
    "                        auc_score = 1\n",
    "                    else:\n",
    "                        auc_score = 0.5\n",
    "                print(\"      -> # TENTATIVA #{}: Acurácia = {:.2f}% / F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                    i+1, \n",
    "                    acc*100,\n",
    "                    score*100,\n",
    "                    auc_score,\n",
    "                ))\n",
    "                accs.append(acc)\n",
    "                scores.append(score)\n",
    "                aucs.append(auc_score)\n",
    "                y_true.extend(y_teste)\n",
    "                y_pred.extend(y_hat)\n",
    "\n",
    "                # Guardando resultados do modelo\n",
    "                resultados[\"aparelho\"].append(rotulo_aparelho)\n",
    "                resultados[\"iteracao\"].append(iteracao)\n",
    "                resultados[\"teste\"].append(i+1)\n",
    "                resultados[\"acuracia\"].append(acc)\n",
    "                resultados[\"f1\"].append(score)\n",
    "                resultados[\"auc\"].append(auc_score)\n",
    "\n",
    "    print()\n",
    "    print(\"   - Resultados finais:\")\n",
    "    print(\"   ---\")\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Acurácia:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(accs)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(accs)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(accs)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(accs)*100) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> F1-score (macro):\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(scores)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(scores)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(scores)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(scores)*100) )\n",
    "    print()\n",
    "    print(\"      -> AUC:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.3f}\".format(np.mean(aucs) ) )\n",
    "    print(\"         . Desvio padrão : {:.3f}\".format(np.std(aucs)) )\n",
    "    print(\"         . Mínimo        : {:.3f}\".format(np.min(aucs)) )\n",
    "    print(\"         . Máximo        : {:.3f}\".format(np.max(aucs)) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Relatório de classificação:\")\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"      -> Matrix de confusao:\")\n",
    "    print()\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    print()\n",
    "    \n",
    "# Exportando resultados\n",
    "df_baseline_cv = pd.DataFrame(resultados)\n",
    "df_baseline_cv[\"metodologia\"] = \"Baseline\"\n",
    "df_baseline_cv.to_excel(\n",
    "    os.path.join(caminho_dados_notebook, \"resultados_baseline_cv.xlsx\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodologia Final/TCE (Transfer Learning, Class Weight e Early Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.943629Z",
     "start_time": "2020-08-05T11:27:47.622Z"
    }
   },
   "outputs": [],
   "source": [
    "metricas = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    tf.keras.metrics.AUC(name='auc'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "]\n",
    "early_stopping_auc = tf.keras.callbacks.EarlyStopping(\n",
    "    min_delta=1e-5,\n",
    "    monitor='val_auc', \n",
    "    verbose=0,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Treino/Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.945628Z",
     "start_time": "2020-08-05T11:27:47.907Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultados = {\n",
    "    \"aparelho\": [],\n",
    "    \"teste\": [],\n",
    "    \"acuracia\": [],\n",
    "    \"f1\": [],\n",
    "    #\"suporte_relativo\": [],\n",
    "    \"auc\":[],\n",
    "}\n",
    "for rotulo_aparelho in df_melhores_taxas_janelas.loc[\n",
    "    df_melhores_taxas_janelas[\"carga\"].isin(\n",
    "        ['dish_washer - 9','fridge - 7','microwave - 16','washer_dryer - 13',\n",
    "         'washer_dryer - 14']),\n",
    "    : ][\"carga\"].values:\n",
    "    \n",
    "    print(f\"* Aparelho {rotulo_aparelho.upper()}:\")\n",
    "    print()\n",
    "    \n",
    "    # Informacoes da carga selecionada\n",
    "    CARGA = rotulo_aparelho.split(\" - \")[0]\n",
    "    #INSTANCIA = int(rotulo_aparelho.split(\" - \")[1])\n",
    "\n",
    "    config_aparelho = df_melhores_taxas_janelas[\n",
    "        df_melhores_taxas_janelas[\"carga\"]==rotulo_aparelho\n",
    "    ].to_dict(\"records\")[0]\n",
    "    TAXA = config_aparelho[\"taxa_amostragem\"]\n",
    "    TAMANHO_JANELA = config_aparelho[\"janela\"]\n",
    "    CONFIG_RP_APARELHO = PARAMETROS_RP\n",
    "\n",
    "    # Percorrer instancias do aparelho na residencia\n",
    "    for INSTANCIA in instancia_aparelho_residencia(CARGA, RESIDENCIA, base = redd):\n",
    "\n",
    "        # Extrair series divididas em janelas para cada medidor\n",
    "        print(\"   - Carregando dados (taxa={:.0f}, janela={:.0f})...\".format(\n",
    "            TAXA, TAMANHO_JANELA\n",
    "        ))\n",
    "        X, y = carregar_dados_aparelho(\n",
    "            janelas=janelas,\n",
    "            instancia=INSTANCIA,\n",
    "            aparelho=CARGA,\n",
    "            tamanho_janela=TAMANHO_JANELA,\n",
    "            taxa=TAXA,\n",
    "            eliminar_janelas_vazias=True\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        print(\"   - Detalhes da amostragem (lotes):\")\n",
    "        print(\"   ---\")\n",
    "        for item in Counter(y).items():\n",
    "            print(f\"      - Classe `{item[0]}`: {item[1]} amostras ({round(item[1]/len(y)*100,1)}%)\" )\n",
    "        print()\n",
    "\n",
    "        # Checando series estaveis\n",
    "        estavel = []\n",
    "        for i, x in enumerate(X):\n",
    "            if len(np.unique(x)) == 1:\n",
    "                estavel.append(i)\n",
    "        print(\"      - Séries estáveis (1 amplitude)         : {} ({:.2f}%)\".format(len(estavel), len(estavel)/len(X)*100) )\n",
    "        print(\"      - Distribuicao de classes nestas séries :\", Counter(y[estavel]))\n",
    "        print()\n",
    "\n",
    "        print(\"      - Estatísticas das séries: = Min. / Max. / Média / STD:\", X.min(), X.max(), X.mean(), X.std())\n",
    "        print()\n",
    "\n",
    "        # Convertendo series para imagem\n",
    "        print(\"   - Preparando dados para modelagem (treino/teste)...\")\n",
    "        print(\"   ---\")\n",
    "        # Fazendo split dos dados (treino/teste)\n",
    "        X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
    "            X, y, \n",
    "            test_size=FRACAO_TESTE,\n",
    "            stratify=y if Counter(y)[0] > 1 and Counter(y)[1] > 1 else None,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        print(\"      - Distribuições dos lotes:\")\n",
    "        print(\"        -> Treino:\", Counter(y_treino))\n",
    "        print(\"        -> Teste :\", Counter(y_teste))\n",
    "\n",
    "        X_treino_imagem, _ = preparar_amostras(\n",
    "            X_treino, y_treino, \n",
    "            params_rp=CONFIG_RP_APARELHO,\n",
    "            tam_imagem=TAMANHO_IMAGEM,\n",
    "            normalizar=False # config. estudo 17 = False\n",
    "        )\n",
    "        X_teste_imagem, _ = preparar_amostras(\n",
    "            X_teste, y_teste, \n",
    "            params_rp=CONFIG_RP_APARELHO,\n",
    "            tam_imagem=TAMANHO_IMAGEM,\n",
    "            normalizar=False # config. estudo 17 = False\n",
    "        )\n",
    "\n",
    "        # Normalizar dados p/ modelo\n",
    "        X_treino_imagem = normalizar_dados(X_treino_imagem)\n",
    "        X_teste_imagem = normalizar_dados(X_teste_imagem)\n",
    "\n",
    "        print()\n",
    "\n",
    "        ########################### METODOLOGIA FINAL ###########################\n",
    "        # Selecionar informacoes sobre aparelhos em outras residencias\n",
    "        dados_pretreino = {}\n",
    "        for r in list(set([1,2,3,4,5,6]) - set([RESIDENCIA])):\n",
    "            for e in redd.buildings[r].elec.meters:\n",
    "                if e.label().lower().replace(\" \",\"_\") == CARGA:\n",
    "                    dados_pretreino[r] = e.instance() if not hasattr(e,'meters') else e.instance()[0]\n",
    "        modelo_pretreino = convnet_metodologia(\n",
    "            metrics=metricas,\n",
    "            optimizer=tf.keras.optimizers.Adam(1e-3), \n",
    "            #bias_output=bias_inicial_output\n",
    "        )    \n",
    "        # Realizando pre-treinamento\n",
    "        print(f\"   - Realizando pré-treinamento do modelo ({dados_pretreino})...\")\n",
    "        for res, inst in dados_pretreino.items():\n",
    "            try:\n",
    "                X_pretreino, y_pretreino = carregar_dados_aparelho(\n",
    "                    janelas=Janelas(\n",
    "                        base=redd,\n",
    "                        id_residencia=res,\n",
    "                        inicio_intervalo=PARAMETROS_DATASET[\"inicio_intervalo\"],\n",
    "                        fim_intervalo=PARAMETROS_DATASET[\"fim_intervalo\"],\n",
    "                        debug=False\n",
    "                    ),\n",
    "                    instancia=inst, \n",
    "                    aparelho=CARGA, \n",
    "                    taxa=TAXA,\n",
    "                    tamanho_janela=TAMANHO_JANELA,\n",
    "                )\n",
    "                X_pretreino, y_pretreino = preparar_amostras(\n",
    "                    X_pretreino, y_pretreino, \n",
    "                    params_rp=CONFIG_RP_APARELHO,\n",
    "                    tam_imagem=TAMANHO_IMAGEM,\n",
    "                    normalizar=False \n",
    "                ) \n",
    "\n",
    "                # Normalizar dados p/ modelo\n",
    "                X_pretreino = normalizar_dados(X_pretreino)\n",
    "\n",
    "                # calculando punicao para classes (desbalanceamento)\n",
    "                try:\n",
    "                    neg, pos = np.bincount(y_pretreino)\n",
    "                except:\n",
    "                    dist = Counter(y_pretreino)\n",
    "                    neg, pos = dist[0], dist[1]\n",
    "                    neg += 1e-5\n",
    "                    pos += 1e-5\n",
    "                    del dist\n",
    "                total = neg + pos\n",
    "                p0 = (1 / neg)*(total)/2.0 \n",
    "                p1 = (1 / pos)*(total)/2.0\n",
    "                pesos_classes_tl = {\n",
    "                    0: p0 if not np.isinf(p0) else 1e-3, \n",
    "                    1: p1 if not np.isinf(p1) else 1e-3\n",
    "                }\n",
    "\n",
    "                # Treinando\n",
    "                historico = modelo_pretreino.fit(\n",
    "                    X_pretreino, y_pretreino,\n",
    "                    validation_data=(X_pretreino, y_pretreino),\n",
    "                    epochs=EPOCAS,\n",
    "                    batch_size=int(len(y_pretreino)/3),#TAMANHO_LOTE_AUMENTADO,\n",
    "                    class_weight=pesos_classes_tl,\n",
    "                    callbacks=[early_stopping_auc],\n",
    "                    #verbose=VERBOSIDADE\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Avaliando\n",
    "                y_hat = modelo_pretreino.predict(X_pretreino).round().astype(np.int16)\n",
    "\n",
    "                print(classification_report(y_pretreino, y_hat))\n",
    "                print()\n",
    "                print(confusion_matrix(y_pretreino, y_hat))\n",
    "                print()\n",
    "\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "                # Incrementando resultados\n",
    "                score = f1_score(y_pretreino, y_hat, average=\"macro\")\n",
    "                auc_score = roc_auc_score(y_pretreino, y_hat) if np.unique(y_pretreino).shape[0]>1 else 1\n",
    "                print(\"      -> Resultados residencia #{}: F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                    res,\n",
    "                    score*100,\n",
    "                    auc_score,\n",
    "                ))\n",
    "                print()\n",
    "            except Exception as e:\n",
    "                print(f\"      -> Resultados residencia #{res}: Não foi possível \"+\\\n",
    "                      \"pré-treinar o modelo com os dados desta residência.\")\n",
    "                print(f\"         # MOTIVO: {str(e)}\")\n",
    "                print()\n",
    "\n",
    "\n",
    "        # calculando punicao para classes (desbalanceamento)\n",
    "        try:\n",
    "            neg, pos = np.bincount(y_treino)\n",
    "        except:\n",
    "            dist = Counter(y_treino)\n",
    "            neg, pos = dist[0], dist[1]\n",
    "            neg += 1e-5\n",
    "            pos += 1e-5\n",
    "            del dist\n",
    "        total = neg + pos\n",
    "        p0 = (1 / neg)*(total)/2.0 \n",
    "        p1 = (1 / pos)*(total)/2.0\n",
    "        pesos_classes = {\n",
    "            0: p0 if not np.isinf(p0) else 1e-3, \n",
    "            1: p1 if not np.isinf(p1) else 1e-3\n",
    "        }\n",
    "        print(\"   - Punição de classes:\", pesos_classes)\n",
    "        print()\n",
    "\n",
    "        print(\"   - Avaliando modelo, aguarde...\")\n",
    "        print(\"   ---\")\n",
    "        y_true, y_pred = [], []\n",
    "        accs = []\n",
    "        scores = []\n",
    "        aucs = []\n",
    "        suportes_relativos = []\n",
    "\n",
    "        # Avaliando N vezes o conjunto treino/teste (consitencia)\n",
    "        N = 10\n",
    "        for i in tqdm_notebook(range(N)):\n",
    "\n",
    "            # Instanciando modelo pre-treinado\n",
    "            modelo = modelo_pretreino\n",
    "            modelo.set_weights(modelo_pretreino.get_weights())\n",
    "\n",
    "            # Treinando\n",
    "            historico = modelo.fit(\n",
    "                X_treino_imagem, y_treino,\n",
    "                validation_data=(X_teste_imagem, y_teste),\n",
    "                epochs=EPOCAS,\n",
    "                batch_size=int(len(y_treino)/4),#TAMANHO_LOTE**2, # TODO: Validar heuristica do tamanho do lote\n",
    "                class_weight=pesos_classes,\n",
    "                callbacks=[early_stopping_auc],\n",
    "                #verbose=VERBOSIDADE\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            y_teste_, y_pred_, acc, score, auc_score, suporte_relativo, idx_descartados = \\\n",
    "                classificacao_threshold(\n",
    "                    modelo, X_teste_imagem, y_teste, \n",
    "                    threshold={0:0, 1:0}, # TODO: validar melhor corte por aparelho\n",
    "                    grafico=False\n",
    "                )\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "            # Incrementando resultados\n",
    "            print(\"      -> # TENTATIVA #{}: Acurácia = {:.2f}% / F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                i+1, \n",
    "                acc*100,\n",
    "                score*100,\n",
    "                auc_score,\n",
    "            ))\n",
    "            accs.append(acc)\n",
    "            scores.append(score)\n",
    "            aucs.append(auc_score)\n",
    "            y_true.extend(y_teste_)\n",
    "            y_pred.extend(y_pred_)\n",
    "\n",
    "            # Guardando resultados do modelo\n",
    "            resultados[\"aparelho\"].append(rotulo_aparelho)\n",
    "            #resultados[\"iteracao\"].append(iteracao) # APENAS PARA VALIDACAO CRUZADA\n",
    "            resultados[\"teste\"].append(i+1)\n",
    "            resultados[\"acuracia\"].append(acc)\n",
    "            resultados[\"f1\"].append(score)\n",
    "            resultados[\"auc\"].append(auc_score)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(\"   - Resultados finais:\")\n",
    "    print(\"   ---\")\n",
    "    print()\n",
    "\n",
    "\n",
    "    print(\"      -> Acurácia:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(accs)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(accs)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(accs)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(accs)*100) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> F1-score (macro):\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(scores)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(scores)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(scores)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(scores)*100) )\n",
    "    print()\n",
    "    print(\"      -> AUC:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.3f}\".format(np.mean(aucs) ) )\n",
    "    print(\"         . Desvio padrão : {:.3f}\".format(np.std(aucs)) )\n",
    "    print(\"         . Mínimo        : {:.3f}\".format(np.min(aucs)) )\n",
    "    print(\"         . Máximo        : {:.3f}\".format(np.max(aucs)) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Relatório de classificação:\")\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"      -> Matrix de confusao:\")\n",
    "    print()\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.946625Z",
     "start_time": "2020-08-05T11:27:47.918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exportando resultados\n",
    "df_tce = pd.DataFrame(resultados)\n",
    "df_tce[\"metodologia\"] = \"Fnal/TCE\"\n",
    "df_tce.to_excel(\n",
    "    os.path.join(caminho_dados_notebook, \"resultados_tce.xlsx\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.948629Z",
     "start_time": "2020-08-05T11:27:48.292Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados = {\n",
    "    \"aparelho\": [],\n",
    "    \"iteracao\": [],\n",
    "    \"teste\": [],\n",
    "    \"acuracia\": [],\n",
    "    \"f1\": [],\n",
    "    #\"suporte_relativo\": [],\n",
    "    \"auc\":[],\n",
    "}\n",
    "for rotulo_aparelho in df_melhores_taxas_janelas.loc[\n",
    "    df_melhores_taxas_janelas[\"carga\"].isin(\n",
    "        ['dish_washer - 9','fridge - 7','microwave - 16','washer_dryer - 13',\n",
    "         'washer_dryer - 14']),\n",
    "    : ][\"carga\"].values:\n",
    "    \n",
    "    print(f\"* Aparelho {rotulo_aparelho.upper()}:\")\n",
    "    print()\n",
    "    \n",
    "    # Informacoes da carga selecionada\n",
    "    CARGA = rotulo_aparelho.split(\" - \")[0]\n",
    "    #INSTANCIA = int(rotulo_aparelho.split(\" - \")[1])\n",
    "\n",
    "    config_aparelho = df_melhores_taxas_janelas[\n",
    "        df_melhores_taxas_janelas[\"carga\"]==rotulo_aparelho\n",
    "    ].to_dict(\"records\")[0]\n",
    "    TAXA = config_aparelho[\"taxa_amostragem\"]\n",
    "    TAMANHO_JANELA = config_aparelho[\"janela\"]\n",
    "    CONFIG_RP_APARELHO = PARAMETROS_RP\n",
    "\n",
    "    # Percorrer instancias do aparelho na residencia\n",
    "    for INSTANCIA in instancia_aparelho_residencia(CARGA, RESIDENCIA, base = redd):\n",
    "\n",
    "        # Extrair series divididas em janelas para cada medidor\n",
    "        print(\"   - Carregando dados (taxa={:.0f}, janela={:.0f})...\".format(\n",
    "            TAXA, TAMANHO_JANELA\n",
    "        ))\n",
    "        X, y = carregar_dados_aparelho(\n",
    "            janelas=janelas,\n",
    "            instancia=INSTANCIA,\n",
    "            aparelho=CARGA,\n",
    "            tamanho_janela=TAMANHO_JANELA,\n",
    "            taxa=TAXA,\n",
    "            eliminar_janelas_vazias=True\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        print(\"   - Detalhes da amostragem (lotes):\")\n",
    "        print(\"   ---\")\n",
    "        for item in Counter(y).items():\n",
    "            print(f\"      - Classe `{item[0]}`: {item[1]} amostras ({round(item[1]/len(y)*100,1)}%)\" )\n",
    "        print()\n",
    "\n",
    "        # Checando series estaveis\n",
    "        estavel = []\n",
    "        for i, x in enumerate(X):\n",
    "            if len(np.unique(x)) == 1:\n",
    "                estavel.append(i)\n",
    "        print(\"      - Séries estáveis (1 amplitude)         : {} ({:.2f}%)\".format(len(estavel), len(estavel)/len(X)*100) )\n",
    "        print(\"      - Distribuicao de classes nestas séries :\", Counter(y[estavel]))\n",
    "        print()\n",
    "\n",
    "        print(\"      - Estatísticas das séries: = Min. / Max. / Média / STD:\", X.min(), X.max(), X.mean(), X.std())\n",
    "        print()\n",
    "\n",
    "        # Convertendo series para imagem (CV)\n",
    "        for it, (idx_treino, idx_teste) in enumerate(kfold.split(X, y)):\n",
    "\n",
    "            iteracao = it + 1    \n",
    "            print(f\"   - Preparando dados para modelagem (cv-{iteracao})...\")\n",
    "            print(\"   ---\")\n",
    "            X_treino, X_teste = X[idx_treino], X[idx_teste]\n",
    "            y_treino, y_teste = y[idx_treino], y[idx_teste]\n",
    "            print()\n",
    "\n",
    "            print(\"      - Distribuições dos lotes:\")\n",
    "            print(\"        -> Treino:\", Counter(y_treino))\n",
    "            print(\"        -> Teste :\", Counter(y_teste))\n",
    "\n",
    "            X_treino_imagem, _ = preparar_amostras(\n",
    "                X_treino, y_treino, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "            X_teste_imagem, _ = preparar_amostras(\n",
    "                X_teste, y_teste, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "\n",
    "            # Normalizar dados p/ modelo\n",
    "            X_treino_imagem = normalizar_dados(X_treino_imagem)\n",
    "            X_teste_imagem = normalizar_dados(X_teste_imagem)\n",
    "\n",
    "            print()\n",
    "\n",
    "            ########################### METODOLOGIA FINAL ###########################\n",
    "            # Selecionar informacoes sobre aparelhos em outras residencias\n",
    "            dados_pretreino = {}\n",
    "            for r in list(set([1,2,3,4,5,6]) - set([RESIDENCIA])):\n",
    "                for e in redd.buildings[r].elec.meters:\n",
    "                    if e.label().lower().replace(\" \",\"_\") == CARGA:\n",
    "                        dados_pretreino[r] = e.instance() if not hasattr(e,'meters') else e.instance()[0]\n",
    "            modelo_pretreino = convnet_metodologia(\n",
    "                metrics=metricas,\n",
    "                optimizer=tf.keras.optimizers.Adam(1e-3), \n",
    "                #bias_output=bias_inicial_output\n",
    "            )    \n",
    "            # Realizando pre-treinamento\n",
    "            print(f\"   - Realizando pré-treinamento do modelo ({dados_pretreino})...\")\n",
    "            for res, inst in dados_pretreino.items():\n",
    "                try:\n",
    "                    X_pretreino, y_pretreino = carregar_dados_aparelho(\n",
    "                        janelas=Janelas(\n",
    "                            base=redd,\n",
    "                            id_residencia=res,\n",
    "                            inicio_intervalo=PARAMETROS_DATASET[\"inicio_intervalo\"],\n",
    "                            fim_intervalo=PARAMETROS_DATASET[\"fim_intervalo\"],\n",
    "                            debug=False\n",
    "                        ),\n",
    "                        instancia=inst, \n",
    "                        aparelho=CARGA, \n",
    "                        taxa=TAXA,\n",
    "                        tamanho_janela=TAMANHO_JANELA,\n",
    "                    )\n",
    "                    X_pretreino, y_pretreino = preparar_amostras(\n",
    "                        X_pretreino, y_pretreino, \n",
    "                        params_rp=CONFIG_RP_APARELHO,\n",
    "                        tam_imagem=TAMANHO_IMAGEM,\n",
    "                        normalizar=False \n",
    "                    ) \n",
    "\n",
    "                    # Normalizar dados p/ modelo\n",
    "                    X_pretreino = normalizar_dados(X_pretreino)\n",
    "\n",
    "                    # calculando punicao para classes (desbalanceamento)\n",
    "                    try:\n",
    "                        neg, pos = np.bincount(y_pretreino)\n",
    "                    except:\n",
    "                        dist = Counter(y_pretreino)\n",
    "                        neg, pos = dist[0], dist[1]\n",
    "                        neg += 1e-5\n",
    "                        pos += 1e-5\n",
    "                        del dist\n",
    "                    total = neg + pos\n",
    "                    p0 = (1 / neg)*(total)/2.0 \n",
    "                    p1 = (1 / pos)*(total)/2.0\n",
    "                    pesos_classes_tl = {\n",
    "                        0: p0 if not np.isinf(p0) else 1e-3, \n",
    "                        1: p1 if not np.isinf(p1) else 1e-3\n",
    "                    }\n",
    "\n",
    "                    # Treinando\n",
    "                    historico = modelo_pretreino.fit(\n",
    "                        X_pretreino, y_pretreino,\n",
    "                        validation_data=(X_pretreino, y_pretreino),\n",
    "                        epochs=EPOCAS,\n",
    "                        batch_size=int(len(y_pretreino)/3),#TAMANHO_LOTE_AUMENTADO,\n",
    "                        class_weight=pesos_classes_tl,\n",
    "                        callbacks=[early_stopping_auc],\n",
    "                        #verbose=VERBOSIDADE\n",
    "                        verbose=0\n",
    "                    )\n",
    "\n",
    "                    # Avaliando\n",
    "                    y_hat = modelo_pretreino.predict(X_pretreino).round().astype(np.int16)\n",
    "\n",
    "                    print(classification_report(y_pretreino, y_hat))\n",
    "                    print()\n",
    "                    print(confusion_matrix(y_pretreino, y_hat))\n",
    "                    print()\n",
    "\n",
    "                    tf.keras.backend.clear_session()\n",
    "\n",
    "                    # Incrementando resultados\n",
    "                    score = f1_score(y_pretreino, y_hat, average=\"macro\")\n",
    "                    auc_score = roc_auc_score(y_pretreino, y_hat) if np.unique(y_pretreino).shape[0]>1 else 1\n",
    "                    print(\"      -> Resultados residencia #{}: F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                        res,\n",
    "                        score*100,\n",
    "                        auc_score,\n",
    "                    ))\n",
    "                    print()\n",
    "                except Exception as e:\n",
    "                    print(f\"      -> Resultados residencia #{res}: Não foi possível \"+\\\n",
    "                          \"pré-treinar o modelo com os dados desta residência.\")\n",
    "                    print(f\"         # MOTIVO: {str(e)}\")\n",
    "                    print()\n",
    "\n",
    "\n",
    "            # calculando punicao para classes (desbalanceamento)\n",
    "            try:\n",
    "                neg, pos = np.bincount(y_treino)\n",
    "            except:\n",
    "                dist = Counter(y_treino)\n",
    "                neg, pos = dist[0], dist[1]\n",
    "                neg += 1e-5\n",
    "                pos += 1e-5\n",
    "                del dist\n",
    "            total = neg + pos\n",
    "            p0 = (1 / neg)*(total)/2.0 \n",
    "            p1 = (1 / pos)*(total)/2.0\n",
    "            pesos_classes = {\n",
    "                0: p0 if not np.isinf(p0) else 1e-3, \n",
    "                1: p1 if not np.isinf(p1) else 1e-3\n",
    "            }\n",
    "            print(\"   - Punição de classes:\", pesos_classes)\n",
    "            print()\n",
    "\n",
    "            print(\"   - Avaliando modelo, aguarde...\")\n",
    "            print(\"   ---\")\n",
    "            y_true, y_pred = [], []\n",
    "            accs = []\n",
    "            scores = []\n",
    "            aucs = []\n",
    "            suportes_relativos = []\n",
    "\n",
    "            # Avaliando N vezes o conjunto treino/teste (consitencia)\n",
    "            N = 10\n",
    "            for i in tqdm_notebook(range(N)):\n",
    "\n",
    "                # Instanciando modelo pre-treinado\n",
    "                modelo = modelo_pretreino\n",
    "                modelo.set_weights(modelo_pretreino.get_weights())\n",
    "\n",
    "                # Treinando\n",
    "                historico = modelo.fit(\n",
    "                    X_treino_imagem, y_treino,\n",
    "                    validation_data=(X_teste_imagem, y_teste),\n",
    "                    epochs=EPOCAS,\n",
    "                    batch_size=int(len(y_treino)/4),#TAMANHO_LOTE**2, # TODO: Validar heuristica do tamanho do lote\n",
    "                    class_weight=pesos_classes,\n",
    "                    callbacks=[early_stopping_auc],\n",
    "                    #verbose=VERBOSIDADE\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                y_teste_, y_pred_, acc, score, auc_score, suporte_relativo, idx_descartados = \\\n",
    "                    classificacao_threshold(\n",
    "                        modelo, X_teste_imagem, y_teste, \n",
    "                        threshold={0:0, 1:0}, # TODO: validar melhor corte por aparelho\n",
    "                        grafico=False\n",
    "                    )\n",
    "\n",
    "\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "                # Incrementando resultados\n",
    "                print(\"      -> # TENTATIVA #{}: Acurácia = {:.2f}% / F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                    i+1, \n",
    "                    acc*100,\n",
    "                    score*100,\n",
    "                    auc_score,\n",
    "                ))\n",
    "                accs.append(acc)\n",
    "                scores.append(score)\n",
    "                aucs.append(auc_score)\n",
    "                y_true.extend(y_teste_)\n",
    "                y_pred.extend(y_pred_)\n",
    "\n",
    "\n",
    "                # Guardando resultados do modelo\n",
    "                resultados[\"aparelho\"].append(rotulo_aparelho)\n",
    "                resultados[\"iteracao\"].append(iteracao) # APENAS PARA VALIDACAO CRUZADA\n",
    "                resultados[\"teste\"].append(i+1)\n",
    "                resultados[\"acuracia\"].append(acc)\n",
    "                resultados[\"f1\"].append(score)\n",
    "                resultados[\"auc\"].append(auc_score)\n",
    "                \n",
    "    print(\"      -> Acurácia:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(accs)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(accs)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(accs)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(accs)*100) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> F1-score (macro):\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(scores)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(scores)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(scores)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(scores)*100) )\n",
    "    print()\n",
    "    print(\"      -> AUC:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.3f}\".format(np.mean(aucs) ) )\n",
    "    print(\"         . Desvio padrão : {:.3f}\".format(np.std(aucs)) )\n",
    "    print(\"         . Mínimo        : {:.3f}\".format(np.min(aucs)) )\n",
    "    print(\"         . Máximo        : {:.3f}\".format(np.max(aucs)) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Relatório de classificação:\")\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"      -> Matrix de confusao:\")\n",
    "    print()\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    print()\n",
    "    \n",
    "# Exportando resultados\n",
    "df_tce_cv = pd.DataFrame(resultados)\n",
    "df_tce_cv[\"metodologia\"] = \"Fnal/TCE\"\n",
    "df_tce_cv.to_excel(\n",
    "    os.path.join(caminho_dados_notebook, \"resultados_tce_cv.xlsx\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T14:33:04.749280Z",
     "start_time": "2020-06-24T14:33:04.741301Z"
    }
   },
   "source": [
    "**NOTA:** O desbalanceamento pode estar impactar MUITO a convergência do modelo. Todavia, é esperado esta característica nos dados de NILM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos Resultados (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Treino/Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.949626Z",
     "start_time": "2020-08-05T11:27:49.253Z"
    }
   },
   "outputs": [],
   "source": [
    "df_baseline = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_baseline.xlsx\"))\n",
    "df_tce = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_tce.xlsx\"))\n",
    "\n",
    "df_analise = pd.concat([\n",
    "    # Resultados das metodologias Baseline e Final\n",
    "    df_baseline, \n",
    "    df_tce,\n",
    "  \n",
    "])\n",
    "print(\"* Análise por metodologia:\")\n",
    "df_analise_metodologia = df_analise.groupby([\"metodologia\"]).agg({\n",
    "    \"f1\": [\"mean\",\"std\",\"max\",\"min\"],\n",
    "    \"auc\": [\"mean\",\"std\",\"max\",\"min\"]\n",
    "}).reset_index().sort_values(('f1','mean'), ascending=False).set_index(\"metodologia\")\n",
    "display(df_analise_metodologia)\n",
    "df_analise_metodologia.to_excel(os.path.join(caminho_dados_notebook, \"df_analise1_metodologia.xlsx\"))\n",
    "\n",
    "print()\n",
    "print(\"* Análise por aparelho/metodologia:\")\n",
    "df_analise_aparelho = df_analise.groupby([\"aparelho\",\"metodologia\"]).agg({\n",
    "    \"f1\": [\"mean\",\"std\",\"max\",\"min\"],\n",
    "    \"auc\": [\"mean\",\"std\",\"max\",\"min\"]\n",
    "})#.reset_index().sort_values(('f1','mean'), ascending=False).set_index([\"aparelho\",\"metodologia\"])\n",
    "display(df_analise_aparelho)\n",
    "df_analise_aparelho.to_excel(os.path.join(caminho_dados_notebook, \"df_analise1_aparelho.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.951629Z",
     "start_time": "2020-08-05T11:27:49.586Z"
    }
   },
   "outputs": [],
   "source": [
    "df_baseline_cv = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_baseline_cv.xlsx\"))\n",
    "df_tce_cv = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_tce_cv.xlsx\"))\n",
    "\n",
    "df_analise_cv = pd.concat([\n",
    "    # Resultados das metodologias Baseline e Final\n",
    "    df_baseline_cv, \n",
    "    df_tce_cv,  \n",
    "])\n",
    "\n",
    "print(\"* Análise por metodologia:\")\n",
    "df_analise_metodologia = df_analise_cv.groupby([\"metodologia\"]).agg({\n",
    "    \"f1\": [\"mean\",\"std\",\"max\",\"min\"],\n",
    "    \"auc\": [\"mean\",\"std\",\"max\",\"min\"]\n",
    "}).reset_index().sort_values(('f1','mean'), ascending=False).set_index(\"metodologia\")\n",
    "display(df_analise_metodologia)\n",
    "df_analise_metodologia.to_excel(os.path.join(caminho_dados_notebook, \"df_analise1_metodologia_cv.xlsx\"))\n",
    "\n",
    "print()\n",
    "print(\"* Análise por aparelho/metodologia:\")\n",
    "df_analise_aparelho = df_analise_cv.groupby([\"aparelho\",\"metodologia\"]).agg({\n",
    "    \"f1\": [\"mean\",\"std\",\"max\",\"min\"],\n",
    "    \"auc\": [\"mean\",\"std\",\"max\",\"min\"]\n",
    "})#.reset_index().sort_values(('f1','mean'), ascending=False).set_index([\"aparelho\",\"metodologia\"])\n",
    "display(df_analise_aparelho)\n",
    "df_analise_aparelho.to_excel(os.path.join(caminho_dados_notebook, \"df_analise1_aparelho_cv.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T18:59:31.520252Z",
     "start_time": "2020-07-25T18:59:31.291887Z"
    }
   },
   "source": [
    "**CONCLUSÕES:**\n",
    "\n",
    "1. A Metodologia Final/TCE melhorou apenas o aparelho Microwave (+9%), tendo desempenho PIOR em todos os outros\n",
    "2. Analisando os dados do aparelho Fridge, é possível notar um fenômeno interessante: as amostras negativas refletem a momentos que TODOS os aparelhos estão desligados (fica a questão: é útil considerar este aparelho nas análises?);\n",
    "\n",
    "Em resumo, percebe-se que a metodologia final/TCE é instável. Este comportamento pode estar AINDA diretamente envolvido com o FORTE desbalanceamento das classes, de modo que o Transfer Learning em outras residências (que também possuem poucas amostras) não fornece conhecimento necessário para o modelo. \n",
    "\n",
    "Vale ressaltar que as classes raras (< 5% das amostras) apresentam um grande desafio para modelos baseados em retropropagação, pois o cálculo do gradientes para os erros nessas amostras serão irrelevantes no processo de treinamento.\n",
    "\n",
    "Sendo assim, é válido avaliar estratégias que de alguma forna EQUILIBRE a disponibilização de amostras raras no cálculo do gradiente. Logo, vamos desenvolver uma nova metodologia, baseada na geração equilibrada de classes nos minilotes. Dado que o aparelho que possui MAIS AMOSTRAS da classe rara no conjunto de treino (Microwave) contempla apenas 16 amostras desta, assumiremos que preservar estas durante toda a época, selecionando apenas a mesma quantidade de amostras MAJORITÁRIAS em cada minilote.\n",
    "\n",
    "Chamaremos essa metodologia de Treinamento com Minilotes Equilibrados, baseado nos trabalhos de [Shimizu et. al (2020)](https://ieeexplore.ieee.org/document/8665709) e [Cappelletti et. al](https://www.sensorsportal.com/HTML/DIGEST/P_3087.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hipóteses para Lidar com Desbalanceamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodologia: Transfer Learning (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Treino/Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.954640Z",
     "start_time": "2020-08-05T11:27:50.918Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados = {\n",
    "    \"aparelho\": [],\n",
    "    \"teste\": [],\n",
    "    \"acuracia\": [],\n",
    "    \"f1\": [],\n",
    "    #\"suporte_relativo\": [],\n",
    "    \"auc\":[],\n",
    "}\n",
    "for rotulo_aparelho in df_melhores_taxas_janelas.loc[\n",
    "    df_melhores_taxas_janelas[\"carga\"].isin(\n",
    "        ['dish_washer - 9','fridge - 7','microwave - 16','washer_dryer - 13',\n",
    "         'washer_dryer - 14']),\n",
    "    : ][\"carga\"].values:\n",
    "    \n",
    "    print(f\"* Aparelho {rotulo_aparelho.upper()}:\")\n",
    "    print()\n",
    "    \n",
    "    # Informacoes da carga selecionada\n",
    "    CARGA = rotulo_aparelho.split(\" - \")[0]\n",
    "    #INSTANCIA = int(rotulo_aparelho.split(\" - \")[1])\n",
    "\n",
    "    config_aparelho = df_melhores_taxas_janelas[\n",
    "        df_melhores_taxas_janelas[\"carga\"]==rotulo_aparelho\n",
    "    ].to_dict(\"records\")[0]\n",
    "    TAXA = config_aparelho[\"taxa_amostragem\"]\n",
    "    TAMANHO_JANELA = config_aparelho[\"janela\"]\n",
    "    CONFIG_RP_APARELHO = PARAMETROS_RP\n",
    "\n",
    "    # Percorrer instancias do aparelho na residencia\n",
    "    for INSTANCIA in instancia_aparelho_residencia(CARGA, RESIDENCIA, base = redd):\n",
    "\n",
    "        # Extrair series divididas em janelas para cada medidor\n",
    "        print(\"   - Carregando dados (taxa={:.0f}, janela={:.0f})...\".format(\n",
    "            TAXA, TAMANHO_JANELA\n",
    "        ))\n",
    "        X, y = carregar_dados_aparelho(\n",
    "            janelas=janelas,\n",
    "            instancia=INSTANCIA,\n",
    "            aparelho=CARGA,\n",
    "            tamanho_janela=TAMANHO_JANELA,\n",
    "            taxa=TAXA,\n",
    "            eliminar_janelas_vazias=True\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        print(\"   - Detalhes da amostragem (lotes):\")\n",
    "        print(\"   ---\")\n",
    "        for item in Counter(y).items():\n",
    "            print(f\"      - Classe `{item[0]}`: {item[1]} amostras ({round(item[1]/len(y)*100,1)}%)\" )\n",
    "        print()\n",
    "\n",
    "        # Checando series estaveis\n",
    "        estavel = []\n",
    "        for i, x in enumerate(X):\n",
    "            if len(np.unique(x)) == 1:\n",
    "                estavel.append(i)\n",
    "        print(\"      - Séries estáveis (1 amplitude)         : {} ({:.2f}%)\".format(len(estavel), len(estavel)/len(X)*100) )\n",
    "        print(\"      - Distribuicao de classes nestas séries :\", Counter(y[estavel]))\n",
    "        print()\n",
    "\n",
    "        print(\"      - Estatísticas das séries: = Min. / Max. / Média / STD:\", X.min(), X.max(), X.mean(), X.std())\n",
    "        print()\n",
    "\n",
    "        # Convertendo series para imagem\n",
    "        print(\"   - Preparando dados para modelagem (treino/teste)...\")\n",
    "        print(\"   ---\")\n",
    "        # Fazendo split dos dados (treino/teste)\n",
    "        X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
    "            X, y, \n",
    "            test_size=FRACAO_TESTE,\n",
    "            stratify=y if Counter(y)[0] > 1 and Counter(y)[1] > 1 else None,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        print(\"      - Distribuições dos lotes:\")\n",
    "        print(\"        -> Treino:\", Counter(y_treino))\n",
    "        print(\"        -> Teste :\", Counter(y_teste))\n",
    "\n",
    "        X_treino_imagem, _ = preparar_amostras(\n",
    "            X_treino, y_treino, \n",
    "            params_rp=CONFIG_RP_APARELHO,\n",
    "            tam_imagem=TAMANHO_IMAGEM,\n",
    "            normalizar=False # config. estudo 17 = False\n",
    "        )\n",
    "        X_teste_imagem, _ = preparar_amostras(\n",
    "            X_teste, y_teste, \n",
    "            params_rp=CONFIG_RP_APARELHO,\n",
    "            tam_imagem=TAMANHO_IMAGEM,\n",
    "            normalizar=False # config. estudo 17 = False\n",
    "        )\n",
    "\n",
    "        # Normalizar dados p/ modelo\n",
    "        X_treino_imagem = normalizar_dados(X_treino_imagem)\n",
    "        X_teste_imagem = normalizar_dados(X_teste_imagem)\n",
    "\n",
    "        print()\n",
    "\n",
    "        ########################### METODOLOGIA FINAL ###########################\n",
    "        # Selecionar informacoes sobre aparelhos em outras residencias\n",
    "        dados_pretreino = {}\n",
    "        for r in list(set([1,2,3,4,5,6]) - set([RESIDENCIA])):\n",
    "            for e in redd.buildings[r].elec.meters:\n",
    "                if e.label().lower().replace(\" \",\"_\") == CARGA:\n",
    "                    dados_pretreino[r] = e.instance() if not hasattr(e,'meters') else e.instance()[0]\n",
    "        modelo_pretreino = convnet_metodologia(\n",
    "            metrics=metricas,\n",
    "            optimizer=tf.keras.optimizers.Adam(1e-3), \n",
    "            #bias_output=bias_inicial_output\n",
    "        )    \n",
    "        # Realizando pre-treinamento\n",
    "        print(f\"   - Realizando pré-treinamento do modelo ({dados_pretreino})...\")\n",
    "        for res, inst in dados_pretreino.items():\n",
    "            try:\n",
    "                X_pretreino, y_pretreino = carregar_dados_aparelho(\n",
    "                    janelas=Janelas(\n",
    "                        base=redd,\n",
    "                        id_residencia=res,\n",
    "                        inicio_intervalo=PARAMETROS_DATASET[\"inicio_intervalo\"],\n",
    "                        fim_intervalo=PARAMETROS_DATASET[\"fim_intervalo\"],\n",
    "                        debug=False\n",
    "                    ),\n",
    "                    instancia=inst, \n",
    "                    aparelho=CARGA, \n",
    "                    taxa=TAXA,\n",
    "                    tamanho_janela=TAMANHO_JANELA,\n",
    "                )\n",
    "                X_pretreino, y_pretreino = preparar_amostras(\n",
    "                    X_pretreino, y_pretreino, \n",
    "                    params_rp=CONFIG_RP_APARELHO,\n",
    "                    tam_imagem=TAMANHO_IMAGEM,\n",
    "                    normalizar=False \n",
    "                ) \n",
    "\n",
    "                # Normalizar dados p/ modelo\n",
    "                X_pretreino = normalizar_dados(X_pretreino)\n",
    "\n",
    "                # calculando punicao para classes (desbalanceamento)\n",
    "                try:\n",
    "                    neg, pos = np.bincount(y_pretreino)\n",
    "                except:\n",
    "                    dist = Counter(y_pretreino)\n",
    "                    neg, pos = dist[0], dist[1]\n",
    "                    neg += 1e-5\n",
    "                    pos += 1e-5\n",
    "                    del dist\n",
    "                p0 = (1 / neg)*(total)/2.0 \n",
    "                p1 = (1 / pos)*(total)/2.0\n",
    "                pesos_classes_tl = {\n",
    "                    0: p0 if not np.isinf(p0) else 1e-3, \n",
    "                    1: p1 if not np.isinf(p1) else 1e-3\n",
    "                }\n",
    "\n",
    "                # Treinando\n",
    "                historico = modelo_pretreino.fit(\n",
    "                    X_pretreino, y_pretreino,\n",
    "                    validation_data=(X_pretreino, y_pretreino),\n",
    "                    epochs=EPOCAS,\n",
    "                    batch_size=int(len(y_pretreino)/3),#TAMANHO_LOTE_AUMENTADO,\n",
    "                    class_weight=pesos_classes_tl,\n",
    "                    callbacks=[early_stopping_auc],\n",
    "                    #verbose=VERBOSIDADE\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Avaliando\n",
    "                y_hat = modelo_pretreino.predict(X_pretreino).round().astype(np.int16)\n",
    "\n",
    "                print(classification_report(y_pretreino, y_hat))\n",
    "                print()\n",
    "                print(confusion_matrix(y_pretreino, y_hat))\n",
    "                print()\n",
    "\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "                # Incrementando resultados\n",
    "                score = f1_score(y_pretreino, y_hat, average=\"macro\")\n",
    "                auc_score = roc_auc_score(y_pretreino, y_hat) if np.unique(y_pretreino).shape[0]>1 else 1\n",
    "                print(\"      -> Resultados residencia #{}: F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                    res,\n",
    "                    score*100,\n",
    "                    auc_score,\n",
    "                ))\n",
    "                print()\n",
    "            except Exception as e:\n",
    "                print(f\"      -> Resultados residencia #{res}: Não foi possível \"+\\\n",
    "                      \"pré-treinar o modelo com os dados desta residência.\")\n",
    "                print(f\"         # MOTIVO: {str(e)}\")\n",
    "                print()\n",
    "\n",
    "\n",
    "        # calculando punicao para classes (desbalanceamento)\n",
    "        try:\n",
    "            neg, pos = np.bincount(y_treino)\n",
    "        except:\n",
    "            dist = Counter(y_treino)\n",
    "            neg, pos = dist[0], dist[1]\n",
    "            neg += 1e-5\n",
    "            pos += 1e-5\n",
    "            del dist\n",
    "        total = neg + pos\n",
    "        p0 = (1 / neg)*(total)/2.0 \n",
    "        p1 = (1 / pos)*(total)/2.0\n",
    "        pesos_classes = {\n",
    "            0: p0 if not np.isinf(p0) else 1e-3, \n",
    "            1: p1 if not np.isinf(p1) else 1e-3\n",
    "        }\n",
    "        print(\"   - Punição de classes:\", pesos_classes)\n",
    "        print()\n",
    "\n",
    "        print(\"   - Avaliando modelo, aguarde...\")\n",
    "        print(\"   ---\")\n",
    "        y_true, y_pred = [], []\n",
    "        accs = []\n",
    "        scores = []\n",
    "        aucs = []\n",
    "        suportes_relativos = []\n",
    "\n",
    "        # Avaliando N vezes o conjunto treino/teste (consitencia)\n",
    "        N = 10\n",
    "        for i in tqdm_notebook(range(N)):\n",
    "\n",
    "            # Instanciando modelo pre-treinado\n",
    "            modelo = modelo_pretreino\n",
    "            modelo.set_weights(modelo_pretreino.get_weights())\n",
    "\n",
    "            # Treinando\n",
    "            historico = modelo.fit(\n",
    "                X_treino_imagem, y_treino,\n",
    "                validation_data=(X_teste_imagem, y_teste),\n",
    "                epochs=EPOCAS,\n",
    "                batch_size=TAMANHO_LOTE,\n",
    "                #verbose=VERBOSIDADE\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            y_teste_, y_pred_, acc, score, auc_score, suporte_relativo, idx_descartados = \\\n",
    "                classificacao_threshold(\n",
    "                    modelo, X_teste_imagem, y_teste, \n",
    "                    threshold={0:0, 1:0}, # TODO: validar melhor corte por aparelho\n",
    "                    grafico=False\n",
    "                )\n",
    "\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "            # Incrementando resultados\n",
    "            print(\"      -> # TENTATIVA #{}: Acurácia = {:.2f}% / F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                i+1, \n",
    "                acc*100,\n",
    "                score*100,\n",
    "                auc_score,\n",
    "            ))\n",
    "            accs.append(acc)\n",
    "            scores.append(score)\n",
    "            aucs.append(auc_score)\n",
    "            y_true.extend(y_teste_)\n",
    "            y_pred.extend(y_pred_)\n",
    "\n",
    "\n",
    "            # Guardando resultados do modelo\n",
    "            resultados[\"aparelho\"].append(rotulo_aparelho)\n",
    "            #resultados[\"iteracao\"].append(iteracao) # APENAS PARA VALIDACAO CRUZADA\n",
    "            resultados[\"teste\"].append(i+1)\n",
    "            resultados[\"acuracia\"].append(acc)\n",
    "            resultados[\"f1\"].append(score)\n",
    "            resultados[\"auc\"].append(auc_score)\n",
    "\n",
    "    print(\"      -> Acurácia:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(accs)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(accs)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(accs)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(accs)*100) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> F1-score (macro):\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(scores)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(scores)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(scores)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(scores)*100) )\n",
    "    print()\n",
    "    print(\"      -> AUC:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.3f}\".format(np.mean(aucs) ) )\n",
    "    print(\"         . Desvio padrão : {:.3f}\".format(np.std(aucs)) )\n",
    "    print(\"         . Mínimo        : {:.3f}\".format(np.min(aucs)) )\n",
    "    print(\"         . Máximo        : {:.3f}\".format(np.max(aucs)) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Relatório de classificação:\")\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"      -> Matrix de confusao:\")\n",
    "    print()\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    print()\n",
    "    \n",
    "# Exportando resultados\n",
    "df_transfer_learning = pd.DataFrame(resultados)\n",
    "df_transfer_learning[\"metodologia\"] = \"Transfer Learning (Baseline)\"\n",
    "df_transfer_learning.to_excel(\n",
    "    os.path.join(caminho_dados_notebook, \"resultados_transfer_learning.xlsx\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.956631Z",
     "start_time": "2020-08-05T11:27:51.326Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados = {\n",
    "    \"aparelho\": [],\n",
    "    \"iteracao\": [],\n",
    "    \"teste\": [],\n",
    "    \"acuracia\": [],\n",
    "    \"f1\": [],\n",
    "   # \"suporte_relativo\": [],\n",
    "    \"auc\":[],\n",
    "}\n",
    "for rotulo_aparelho in df_melhores_taxas_janelas.loc[\n",
    "    df_melhores_taxas_janelas[\"carga\"].isin(\n",
    "        ['dish_washer - 9','fridge - 7','microwave - 16','washer_dryer - 13',\n",
    "         'washer_dryer - 14']),\n",
    "    : ][\"carga\"].values:\n",
    "    \n",
    "    print(f\"* Aparelho {rotulo_aparelho.upper()}:\")\n",
    "    print()\n",
    "    \n",
    "    # Informacoes da carga selecionada\n",
    "    CARGA = rotulo_aparelho.split(\" - \")[0]\n",
    "    #INSTANCIA = int(rotulo_aparelho.split(\" - \")[1])\n",
    "\n",
    "    config_aparelho = df_melhores_taxas_janelas[\n",
    "        df_melhores_taxas_janelas[\"carga\"]==rotulo_aparelho\n",
    "    ].to_dict(\"records\")[0]\n",
    "    TAXA = config_aparelho[\"taxa_amostragem\"]\n",
    "    TAMANHO_JANELA = config_aparelho[\"janela\"]\n",
    "    CONFIG_RP_APARELHO = PARAMETROS_RP\n",
    "\n",
    "    # Percorrer instancias do aparelho na residencia\n",
    "    for INSTANCIA in instancia_aparelho_residencia(CARGA, RESIDENCIA, base = redd):\n",
    "\n",
    "        # Extrair series divididas em janelas para cada medidor\n",
    "        print(\"   - Carregando dados (taxa={:.0f}, janela={:.0f})...\".format(\n",
    "            TAXA, TAMANHO_JANELA\n",
    "        ))\n",
    "        X, y = carregar_dados_aparelho(\n",
    "            janelas=janelas,\n",
    "            instancia=INSTANCIA,\n",
    "            aparelho=CARGA,\n",
    "            tamanho_janela=TAMANHO_JANELA,\n",
    "            taxa=TAXA,\n",
    "            eliminar_janelas_vazias=True\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        print(\"   - Detalhes da amostragem (lotes):\")\n",
    "        print(\"   ---\")\n",
    "        for item in Counter(y).items():\n",
    "            print(f\"      - Classe `{item[0]}`: {item[1]} amostras ({round(item[1]/len(y)*100,1)}%)\" )\n",
    "        print()\n",
    "\n",
    "        # Checando series estaveis\n",
    "        estavel = []\n",
    "        for i, x in enumerate(X):\n",
    "            if len(np.unique(x)) == 1:\n",
    "                estavel.append(i)\n",
    "        print(\"      - Séries estáveis (1 amplitude)         : {} ({:.2f}%)\".format(len(estavel), len(estavel)/len(X)*100) )\n",
    "        print(\"      - Distribuicao de classes nestas séries :\", Counter(y[estavel]))\n",
    "        print()\n",
    "\n",
    "        print(\"      - Estatísticas das séries: = Min. / Max. / Média / STD:\", X.min(), X.max(), X.mean(), X.std())\n",
    "        print()\n",
    "\n",
    "        # Convertendo series para imagem (CV)\n",
    "        for it, (idx_treino, idx_teste) in enumerate(kfold.split(X, y)):\n",
    "\n",
    "            iteracao = it + 1    \n",
    "            print(f\"   - Preparando dados para modelagem (cv-{iteracao})...\")\n",
    "            print(\"   ---\")\n",
    "            X_treino, X_teste = X[idx_treino], X[idx_teste]\n",
    "            y_treino, y_teste = y[idx_treino], y[idx_teste]\n",
    "            print()\n",
    "\n",
    "\n",
    "            print(\"      - Distribuições dos lotes:\")\n",
    "            print(\"        -> Treino:\", Counter(y_treino))\n",
    "            print(\"        -> Teste :\", Counter(y_teste))\n",
    "\n",
    "            X_treino_imagem, _ = preparar_amostras(\n",
    "                X_treino, y_treino, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "            X_teste_imagem, _ = preparar_amostras(\n",
    "                X_teste, y_teste, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "\n",
    "            # Normalizar dados p/ modelo\n",
    "            X_treino_imagem = normalizar_dados(X_treino_imagem)\n",
    "            X_teste_imagem = normalizar_dados(X_teste_imagem)\n",
    "\n",
    "            print()\n",
    "\n",
    "            ########################### METODOLOGIA FINAL ###########################\n",
    "            # Selecionar informacoes sobre aparelhos em outras residencias\n",
    "            dados_pretreino = {}\n",
    "            for r in list(set([1,2,3,4,5,6]) - set([RESIDENCIA])):\n",
    "                for e in redd.buildings[r].elec.meters:\n",
    "                    if e.label().lower().replace(\" \",\"_\") == CARGA:\n",
    "                        dados_pretreino[r] = e.instance() if not hasattr(e,'meters') else e.instance()[0]\n",
    "            modelo_pretreino = convnet_metodologia(\n",
    "                metrics=metricas,\n",
    "                optimizer=tf.keras.optimizers.Adam(1e-3), \n",
    "                #bias_output=bias_inicial_output\n",
    "            )    \n",
    "            # Realizando pre-treinamento\n",
    "            print(f\"   - Realizando pré-treinamento do modelo ({dados_pretreino})...\")\n",
    "            for res, inst in dados_pretreino.items():\n",
    "                try:\n",
    "                    X_pretreino, y_pretreino = carregar_dados_aparelho(\n",
    "                        janelas=Janelas(\n",
    "                            base=redd,\n",
    "                            id_residencia=res,\n",
    "                            inicio_intervalo=PARAMETROS_DATASET[\"inicio_intervalo\"],\n",
    "                            fim_intervalo=PARAMETROS_DATASET[\"fim_intervalo\"],\n",
    "                            debug=False\n",
    "                        ),\n",
    "                        instancia=inst, \n",
    "                        aparelho=CARGA, \n",
    "                        taxa=TAXA,\n",
    "                        tamanho_janela=TAMANHO_JANELA,\n",
    "                    )\n",
    "                    X_pretreino, y_pretreino = preparar_amostras(\n",
    "                        X_pretreino, y_pretreino, \n",
    "                        params_rp=CONFIG_RP_APARELHO,\n",
    "                        tam_imagem=TAMANHO_IMAGEM,\n",
    "                        normalizar=False \n",
    "                    ) \n",
    "\n",
    "                    # Normalizar dados p/ modelo\n",
    "                    X_pretreino = normalizar_dados(X_pretreino)\n",
    "\n",
    "                    # calculando punicao para classes (desbalanceamento)\n",
    "                    try:\n",
    "                        neg, pos = np.bincount(y_pretreino)\n",
    "                    except:\n",
    "                        dist = Counter(y_pretreino)\n",
    "                        neg, pos = dist[0], dist[1]\n",
    "                        neg += 1e-5\n",
    "                        pos += 1e-5\n",
    "                        del dist\n",
    "                    total = neg + pos\n",
    "                    p0 = (1 / neg)*(total)/2.0 \n",
    "                    p1 = (1 / pos)*(total)/2.0\n",
    "                    pesos_classes_tl = {\n",
    "                        0: p0 if not np.isinf(p0) else 1e-3, \n",
    "                        1: p1 if not np.isinf(p1) else 1e-3\n",
    "                    }\n",
    "\n",
    "                    # Treinando\n",
    "                    historico = modelo_pretreino.fit(\n",
    "                        X_pretreino, y_pretreino,\n",
    "                        validation_data=(X_pretreino, y_pretreino),\n",
    "                        epochs=EPOCAS,\n",
    "                        batch_size=int(len(y_pretreino)/3),#TAMANHO_LOTE_AUMENTADO,\n",
    "                        class_weight=pesos_classes_tl,\n",
    "                        callbacks=[early_stopping_auc],\n",
    "                        #verbose=VERBOSIDADE\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    \n",
    "                    # Avaliando\n",
    "                    y_hat = modelo_pretreino.predict(X_pretreino).round().astype(np.int16)\n",
    "\n",
    "                    print(classification_report(y_pretreino, y_hat))\n",
    "                    print()\n",
    "                    print(confusion_matrix(y_pretreino, y_hat))\n",
    "                    print()\n",
    "\n",
    "                    tf.keras.backend.clear_session()\n",
    "\n",
    "                    # Incrementando resultados\n",
    "                    score = f1_score(y_pretreino, y_hat, average=\"macro\")\n",
    "                    auc_score = roc_auc_score(y_pretreino, y_hat) if np.unique(y_pretreino).shape[0]>1 else 1\n",
    "                    print(\"      -> Resultados residencia #{}: F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                        res,\n",
    "                        score*100,\n",
    "                        auc_score,\n",
    "                    ))\n",
    "                    print()\n",
    "                except Exception as e:\n",
    "                    print(f\"      -> Resultados residencia #{res}: Não foi possível \"+\\\n",
    "                          \"pré-treinar o modelo com os dados desta residência.\")\n",
    "                    print(f\"         # MOTIVO: {str(e)}\")\n",
    "                    print()\n",
    "\n",
    "\n",
    "            # calculando punicao para classes (desbalanceamento)\n",
    "            try:\n",
    "                neg, pos = np.bincount(y_treino)\n",
    "            except:\n",
    "                dist = Counter(y_treino)\n",
    "                neg, pos = dist[0], dist[1]\n",
    "                neg += 1e-5\n",
    "                pos += 1e-5\n",
    "                del dist\n",
    "            total = neg + pos\n",
    "            p0 = (1 / neg)*(total)/2.0 \n",
    "            p1 = (1 / pos)*(total)/2.0\n",
    "            pesos_classes = {\n",
    "                0: p0 if not np.isinf(p0) else 1e-3, \n",
    "                1: p1 if not np.isinf(p1) else 1e-3\n",
    "            }\n",
    "            print(\"   - Punição de classes:\", pesos_classes)\n",
    "            print()\n",
    "\n",
    "            print(\"   - Avaliando modelo, aguarde...\")\n",
    "            print(\"   ---\")\n",
    "            y_true, y_pred = [], []\n",
    "            accs = []\n",
    "            scores = []\n",
    "            aucs = []\n",
    "            suportes_relativos = []\n",
    "\n",
    "            # Avaliando N vezes o conjunto treino/teste (consitencia)\n",
    "            N = 10\n",
    "            for i in tqdm_notebook(range(N)):\n",
    "\n",
    "                # Instanciando modelo pre-treinado\n",
    "                modelo = modelo_pretreino\n",
    "                modelo.set_weights(modelo_pretreino.get_weights())\n",
    "\n",
    "                # Treinando\n",
    "                historico = modelo.fit(\n",
    "                    X_treino_imagem, y_treino,\n",
    "                    validation_data=(X_teste_imagem, y_teste),\n",
    "                    epochs=EPOCAS,\n",
    "                    batch_size=TAMANHO_LOTE,\n",
    "                    #verbose=VERBOSIDADE\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "\n",
    "                y_teste_, y_pred_, acc, score, auc_score, suporte_relativo, idx_descartados = \\\n",
    "                    classificacao_threshold(\n",
    "                        modelo, X_teste_imagem, y_teste, \n",
    "                        threshold={0:0, 1:0}, # TODO: validar melhor corte por aparelho\n",
    "                        grafico=False\n",
    "                    )\n",
    "\n",
    "\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "                print(\"      -> # TENTATIVA #{}: Acurácia = {:.2f}% / F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                    i+1, \n",
    "                    acc*100,\n",
    "                    score*100,\n",
    "                    auc_score,\n",
    "                ))\n",
    "                accs.append(acc)\n",
    "                scores.append(score)\n",
    "                aucs.append(auc_score)\n",
    "                y_true.extend(y_teste_)\n",
    "                y_pred.extend(y_pred_)\n",
    "\n",
    "\n",
    "                # Guardando resultados do modelo\n",
    "                resultados[\"aparelho\"].append(rotulo_aparelho)\n",
    "                resultados[\"iteracao\"].append(iteracao) # APENAS PARA VALIDACAO CRUZADA\n",
    "                resultados[\"teste\"].append(i+1)\n",
    "                resultados[\"acuracia\"].append(acc)\n",
    "                resultados[\"f1\"].append(score)\n",
    "                resultados[\"auc\"].append(auc_score)\n",
    "\n",
    "    print()\n",
    "    print(\"   - Resultados finais:\")\n",
    "    print(\"   ---\")\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Acurácia:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(accs)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(accs)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(accs)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(accs)*100) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> F1-score (macro):\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(scores)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(scores)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(scores)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(scores)*100) )\n",
    "    print()\n",
    "    print(\"      -> AUC:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.3f}\".format(np.mean(aucs) ) )\n",
    "    print(\"         . Desvio padrão : {:.3f}\".format(np.std(aucs)) )\n",
    "    print(\"         . Mínimo        : {:.3f}\".format(np.min(aucs)) )\n",
    "    print(\"         . Máximo        : {:.3f}\".format(np.max(aucs)) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Relatório de classificação:\")\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"      -> Matrix de confusao:\")\n",
    "    print()\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    print()\n",
    "    \n",
    "# Exportando resultados\n",
    "df_transfer_learning_cv = pd.DataFrame(resultados)\n",
    "df_transfer_learning_cv[\"metodologia\"] = \"Transfer Learning (Baseline)\"\n",
    "df_transfer_learning_cv.to_excel(\n",
    "    os.path.join(caminho_dados_notebook, \"resultados_transfer_learning_cv.xlsx\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodologia: Treinamento Estratégico (Minilotes Equilibrados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.958626Z",
     "start_time": "2020-08-05T11:27:51.691Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rotina de treinamento estratificado (ideia principal: usar o lote de positivos\n",
    "# em todos os minibatchs de treinamento do modelo)\n",
    "def treinamento_estrategico(\n",
    "    modelo,\n",
    "    X, y,\n",
    "    validacao=None,\n",
    "    epocas=5,\n",
    "    majoritaria=0,\n",
    "    minoritaria=1,\n",
    "    peso_classe='auto',\n",
    "    debug = True\n",
    "):\n",
    "    from sklearn.utils import shuffle\n",
    "    \n",
    "    if validacao:\n",
    "        X_teste = validacao[0]\n",
    "        y_teste = validacao[1]\n",
    "    \n",
    "    # Totalizando por classe\n",
    "    total = Counter(y)\n",
    "    total_maj = total[majoritaria]\n",
    "    total_min = total[minoritaria]\n",
    "    \n",
    "    if peso_classe == \"auto\":\n",
    "        peso_classe = {\n",
    "            majoritaria: 1,\n",
    "            minoritaria: total_maj/total_min\n",
    "        }\n",
    "\n",
    "    # Selecionando indice das amostras por classe\n",
    "    idx_min = [y_[0] for y_ in np.argwhere(y == minoritaria)]\n",
    "    idx_maj = [y_[0] for y_ in np.argwhere(y == majoritaria)]\n",
    "\n",
    "    for e in range(epocas):\n",
    "\n",
    "        if debug: print(f\"Época {e+1}\")\n",
    "\n",
    "        np.random.shuffle(idx_maj)\n",
    "        np.random.shuffle(idx_min)\n",
    "        for b,i in enumerate(np.arange(total_maj, step=total_min)):\n",
    "\n",
    "            # Lote da classe maioritaria\n",
    "            idx_lote = idx_maj[i:i+total_min]\n",
    "\n",
    "            # Lote final (shuffle majoritaria + minoritaria)\n",
    "            X_lote, y_lote = shuffle(\n",
    "                np.concatenate([X[idx_lote], X[idx_min]]),\n",
    "                np.concatenate([y[idx_lote], y[idx_min]]),\n",
    "            )\n",
    "            # Treinando no lote equilibrado\n",
    "            m_treino = modelo.train_on_batch(X_lote, y_lote, class_weight=peso_classe)\n",
    "\n",
    "            if debug:\n",
    "                if validacao is None:\n",
    "                    sys.stdout.write('\\r - Batch {}/{}: {}'.format(\n",
    "                        b, int(np.floor(total_maj/total_min)),\n",
    "                        \" \".join([f\"{metrica}: {round(valor,4) if metrica=='loss' else round(valor,2)}\" for metrica, valor in dict(zip(modelo.metrics_names, m_treino)).items()]))\n",
    "                    )\n",
    "                    sys.stdout.flush()\n",
    "                else:\n",
    "                    m_teste = modelo.evaluate(X_teste, verbose=0)\n",
    "                    sys.stdout.write('\\r - Batch {}/{}: {} | {}'.format(\n",
    "                        b, int(np.floor(total_maj/total_min)),\n",
    "                        \" \".join([f\"{metrica}: {round(valor,4) if metrica=='loss' else round(valor,2)}\" for metrica, valor in dict(zip(modelo.metrics_names, m_treino)).items()]),\n",
    "                        \" \".join([f\"val_{metrica}: {round(valor,4) if metrica=='loss' else round(valor,3)}\" for metrica, valor in dict(zip(modelo.metrics_names, m_teste)).items()]))\n",
    "                    )\n",
    "                    sys.stdout.flush()\n",
    "        if debug: print()\n",
    "    \n",
    "    if debug: print()\n",
    "    return modelo\n",
    "\n",
    "# Rotina de construcao DINAMICA do modelo (baseado em hiperparametrizacao)\n",
    "def build(\n",
    "    optimizer='adam',\n",
    "    loss = 'binary_crossentropy', \n",
    "    metrics=['accuracy'],\n",
    "    bias_output = None,\n",
    "    arquitetura={\n",
    "        \"conv\": {\n",
    "            \"modules\":1, \n",
    "            \"layers\": 1,\n",
    "            \"filters\": 8,\n",
    "            \"kernel_size\":3,\n",
    "            \"strides\":1,\n",
    "            \"padding\":\"same\"\n",
    "        },\n",
    "        \"pooling\": {\n",
    "            \"mode\": \"max\",\n",
    "            \"size\":2,\n",
    "            \"strides\": 1,\n",
    "            \"padding\": \"same\"\n",
    "        }\n",
    "    },\n",
    "    initializer=None\n",
    "):\n",
    "        \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Input(shape=(32,32,1)))\n",
    "    \n",
    "    conv = arquitetura[\"conv\"]\n",
    "    pooling = arquitetura[\"pooling\"]\n",
    "    \n",
    "    for _ in range(conv[\"modules\"]):\n",
    "        for _ in range(conv[\"layers\"]):\n",
    "                                  \n",
    "            if initializer is not None:\n",
    "                init = initializer\n",
    "            else:\n",
    "                #https://www.researchgate.net/post/Proper_Weight_Initialization_for_ReLU_and_PReLU\n",
    "                init = tf.keras.initializers.RandomNormal(\n",
    "                    stddev=np.sqrt(1/(conv[\"kernel_size\"]*conv[\"kernel_size\"]*1))\n",
    "                ) \n",
    "                                          \n",
    "            model.add(Conv2D(\n",
    "                filters=conv[\"filters\"], \n",
    "                kernel_size=conv[\"kernel_size\"], \n",
    "                strides=conv[\"strides\"], \n",
    "                padding=conv[\"padding\"], \n",
    "                activation=\"relu\",\n",
    "                #kernel_initializer=initializer\n",
    "                kernel_initializer=init\n",
    "            ))\n",
    "        if pooling[\"mode\"] == \"max\":\n",
    "            model.add(MaxPooling2D(\n",
    "                pool_size=pooling[\"size\"], \n",
    "                strides=pooling[\"strides\"], \n",
    "                padding=pooling[\"padding\"], \n",
    "            ))    \n",
    "        else:\n",
    "            model.add(AveragePooling2D(\n",
    "                pool_size=pooling[\"size\"], \n",
    "                strides=pooling[\"strides\"], \n",
    "                padding=pooling[\"padding\"], \n",
    "            ))   \n",
    "            \n",
    "    model.add(Flatten())\n",
    "                                  \n",
    "    model.add(Dense(128, activation='relu', \n",
    "                    #kernel_initializer=tf.keras.initializers.RandomNormal(stddev=np.sqrt(2/128*128))\n",
    "                   ))\n",
    "#     model.add(Dropout(0.25))\n",
    "    \n",
    "    if bias_output is not None:\n",
    "        bias_output = tf.keras.initializers.Constant(bias_output)\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid', bias_initializer=bias_output))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=[loss], metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "                                  \n",
    "# # Instanciando o modelo\n",
    "# modelo = build(\n",
    "#     optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "#     #loss_function='binary_crossentropy',#focal_loss.BinaryFocalLoss(gamma=0.1),        \n",
    "#     loss='binary_crossentropy',\n",
    "#     metrics=metricas,\n",
    "# )\n",
    "# modelo.summary()\n",
    "def classe_majoritaria(d):\n",
    "    \"\"\" a) criar lista de chaves e valores; \n",
    "     b) return da chave com mais ocorrencias\"\"\"  \n",
    "    v=list(d.values())\n",
    "    k=list(d.keys())\n",
    "    return k[v.index(max(v))]\n",
    "def classe_minoritaria(d):\n",
    "    \"\"\" a) criar lista de chaves e valores; \n",
    "     b) return da chave com menos ocorrencias\"\"\"  \n",
    "    v=list(d.values())\n",
    "    k=list(d.keys())\n",
    "    return k[v.index(min(v))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Treino/Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.959626Z",
     "start_time": "2020-08-05T11:27:52.040Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultados = {\n",
    "    \"aparelho\": [],\n",
    "    \"teste\": [],\n",
    "    \"acuracia\": [],\n",
    "    \"f1\": [],\n",
    "    \"auc\":[],\n",
    "}\n",
    "for rotulo_aparelho in df_melhores_taxas_janelas.loc[\n",
    "    df_melhores_taxas_janelas[\"carga\"].isin(\n",
    "        ['dish_washer - 9','fridge - 7','microwave - 16','washer_dryer - 13',\n",
    "         'washer_dryer - 14']),\n",
    "    : ][\"carga\"].values:\n",
    "    \n",
    "    print(f\"* Aparelho {rotulo_aparelho.upper()}:\")\n",
    "    print()\n",
    "    \n",
    "    # Informacoes da carga selecionada\n",
    "    CARGA = rotulo_aparelho.split(\" - \")[0]\n",
    "    #INSTANCIA = int(rotulo_aparelho.split(\" - \")[1])\n",
    "\n",
    "    config_aparelho = df_melhores_taxas_janelas[\n",
    "        df_melhores_taxas_janelas[\"carga\"]==rotulo_aparelho\n",
    "    ].to_dict(\"records\")[0]\n",
    "    TAXA = config_aparelho[\"taxa_amostragem\"]\n",
    "    TAMANHO_JANELA = config_aparelho[\"janela\"]\n",
    "    CONFIG_RP_APARELHO = PARAMETROS_RP\n",
    "\n",
    "\n",
    "    # Percorrer instancias do aparelho na residencia\n",
    "    for INSTANCIA in instancia_aparelho_residencia(CARGA, RESIDENCIA, base = redd):\n",
    "\n",
    "        # Extrair series divididas em janelas para cada medidor\n",
    "        print(\"   - Carregando dados (taxa={:.0f}, janela={:.0f})...\".format(\n",
    "            TAXA, TAMANHO_JANELA\n",
    "        ))\n",
    "        X, y = carregar_dados_aparelho(\n",
    "            janelas=janelas,\n",
    "            instancia=INSTANCIA,\n",
    "            aparelho=CARGA,\n",
    "            tamanho_janela=TAMANHO_JANELA,\n",
    "            taxa=TAXA,\n",
    "            eliminar_janelas_vazias=True\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        print(\"   - Detalhes da amostragem (lotes):\")\n",
    "        print(\"   ---\")\n",
    "        for item in Counter(y).items():\n",
    "            print(f\"      - Classe `{item[0]}`: {item[1]} amostras ({round(item[1]/len(y)*100,1)}%)\" )\n",
    "        print()\n",
    "\n",
    "        # Checando series estaveis\n",
    "        estavel = []\n",
    "        for i, x in enumerate(X):\n",
    "            if len(np.unique(x)) == 1:\n",
    "                estavel.append(i)\n",
    "        print(\"      - Séries estáveis (1 amplitude)         : {} ({:.2f}%)\".format(len(estavel), len(estavel)/len(X)*100) )\n",
    "        print(\"      - Distribuicao de classes nestas séries :\", Counter(y[estavel]))\n",
    "        print()\n",
    "\n",
    "        print(\"      - Estatísticas das séries: = Min. / Max. / Média / STD:\", X.min(), X.max(), X.mean(), X.std())\n",
    "        print()\n",
    "\n",
    "        # Convertendo series para imagem\n",
    "        print(\"   - Preparando dados para modelagem (treino/teste)...\")\n",
    "        print(\"   ---\")\n",
    "        # Fazendo split dos dados (treino/teste)\n",
    "        X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
    "            X, y, \n",
    "            test_size=FRACAO_TESTE,\n",
    "            stratify=y if Counter(y)[0] > 1 and Counter(y)[1] > 1 else None,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        print(\"      - Distribuições dos lotes:\")\n",
    "        print(\"        -> Treino:\", Counter(y_treino))\n",
    "        print(\"        -> Teste :\", Counter(y_teste))\n",
    "\n",
    "        X_treino_imagem, _ = preparar_amostras(\n",
    "            X_treino, y_treino, \n",
    "            params_rp=CONFIG_RP_APARELHO,\n",
    "            tam_imagem=TAMANHO_IMAGEM,\n",
    "            normalizar=False # config. estudo 17 = False\n",
    "        )\n",
    "        X_teste_imagem, _ = preparar_amostras(\n",
    "            X_teste, y_teste, \n",
    "            params_rp=CONFIG_RP_APARELHO,\n",
    "            tam_imagem=TAMANHO_IMAGEM,\n",
    "            normalizar=False # config. estudo 17 = False\n",
    "        )\n",
    "\n",
    "        # Normalizar dados p/ modelo\n",
    "        X_treino_imagem = normalizar_dados(X_treino_imagem)\n",
    "        X_teste_imagem = normalizar_dados(X_teste_imagem)\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(\"   - Avaliando modelo, aguarde...\")\n",
    "        print(\"   ---\")\n",
    "        y_true, y_pred = [], []\n",
    "        accs = []\n",
    "        scores = []\n",
    "        aucs = []\n",
    "\n",
    "        # Avaliando N vezes o conjunto treino/teste (consitencia)\n",
    "        N = 10\n",
    "        for i in tqdm_notebook(range(N)):\n",
    "\n",
    "            ##################### METODOLOGIA ANTIGA #####################\n",
    "            # Treinamento estratico (lotes equilibrados)\n",
    "            modelo = treinamento_estrategico(\n",
    "                modelo=convnet_metodologia(\n",
    "                    input_shape_= TAMANHO_IMAGEM,\n",
    "                    output_dim = 1,\n",
    "                    loss_function='binary_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    output_activation='sigmoid'\n",
    "                ), \n",
    "                X=X_treino_imagem, y=y_treino, \n",
    "                #validacao=(X_teste_imagem, y_teste), \n",
    "                epocas=EPOCAS, \n",
    "                majoritaria=classe_majoritaria(Counter(y_treino)), \n",
    "                minoritaria=classe_minoritaria(Counter(y_treino)), \n",
    "                peso_classe={classe_majoritaria(Counter(y_treino)):1, \n",
    "                             classe_minoritaria(Counter(y_treino)):1}, \n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            # Avaliando\n",
    "            y_hat = modelo.predict(X_teste_imagem).round().astype(np.int16)\n",
    "            ##################### METODOLOGIA ANTIGA #####################\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "            # Incrementando resultados\n",
    "            acc = accuracy_score(y_teste, y_hat)\n",
    "            score = f1_score(y_teste, y_hat, average=\"macro\")\n",
    "            try:\n",
    "                auc_score = roc_auc_score(y_teste, y_hat)\n",
    "            except:\n",
    "                if score == 1:\n",
    "                    auc_score = 1\n",
    "                else:\n",
    "                    auc_score = 0.5\n",
    "            print(\"      -> # TENTATIVA #{}: Acurácia = {:.2f}% / F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                i+1, \n",
    "                acc*100,\n",
    "                score*100,\n",
    "                auc_score,\n",
    "            ))\n",
    "            accs.append(acc)\n",
    "            scores.append(score)\n",
    "            aucs.append(auc_score)\n",
    "            y_true.extend(y_teste)\n",
    "            y_pred.extend(y_hat)\n",
    "\n",
    "            # Guardando resultados do modelo\n",
    "            resultados[\"aparelho\"].append(rotulo_aparelho)\n",
    "            #resultados[\"iteracao\"].append(iteracao) # APENAS PARA VALIDACAO CRUZADA\n",
    "            resultados[\"teste\"].append(i+1)\n",
    "            resultados[\"acuracia\"].append(acc)\n",
    "            resultados[\"f1\"].append(score)\n",
    "            resultados[\"auc\"].append(auc_score)\n",
    "\n",
    "    print()\n",
    "    print(\"   - Resultados finais:\")\n",
    "    print(\"   ---\")\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Acurácia:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(accs)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(accs)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(accs)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(accs)*100) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> F1-score (macro):\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(scores)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(scores)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(scores)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(scores)*100) )\n",
    "    print()\n",
    "    print(\"      -> AUC:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.3f}\".format(np.mean(aucs) ) )\n",
    "    print(\"         . Desvio padrão : {:.3f}\".format(np.std(aucs)) )\n",
    "    print(\"         . Mínimo        : {:.3f}\".format(np.min(aucs)) )\n",
    "    print(\"         . Máximo        : {:.3f}\".format(np.max(aucs)) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Relatório de classificação:\")\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"      -> Matrix de confusao:\")\n",
    "    print()\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    print()\n",
    "    \n",
    "# Exportando resultados\n",
    "df_treinamento_estrategico = pd.DataFrame(resultados)\n",
    "df_treinamento_estrategico[\"metodologia\"] = \"Treinamento Estratégico\"\n",
    "df_treinamento_estrategico.to_excel(\n",
    "    os.path.join(caminho_dados_notebook, \"resultados_treinamento_estrategico.xlsx\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.961629Z",
     "start_time": "2020-08-05T11:27:52.518Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados = {\n",
    "    \"aparelho\": [],\n",
    "    \"iteracao\": [],\n",
    "    \"teste\": [],\n",
    "    \"acuracia\": [],\n",
    "    \"f1\": [],\n",
    "    \"auc\":[],\n",
    "}\n",
    "for rotulo_aparelho in df_melhores_taxas_janelas.loc[\n",
    "    df_melhores_taxas_janelas[\"carga\"].isin(\n",
    "        ['dish_washer - 9','fridge - 7','microwave - 16','washer_dryer - 13',\n",
    "         'washer_dryer - 14']),\n",
    "    : ][\"carga\"].values:\n",
    "    \n",
    "    print(f\"* Aparelho {rotulo_aparelho.upper()}:\")\n",
    "    print()\n",
    "    \n",
    "    # Informacoes da carga selecionada\n",
    "    CARGA = rotulo_aparelho.split(\" - \")[0]\n",
    "    #INSTANCIA = int(rotulo_aparelho.split(\" - \")[1])\n",
    "\n",
    "    config_aparelho = df_melhores_taxas_janelas[\n",
    "        df_melhores_taxas_janelas[\"carga\"]==rotulo_aparelho\n",
    "    ].to_dict(\"records\")[0]\n",
    "    TAXA = config_aparelho[\"taxa_amostragem\"]\n",
    "    TAMANHO_JANELA = config_aparelho[\"janela\"]\n",
    "    CONFIG_RP_APARELHO = PARAMETROS_RP\n",
    "\n",
    "    # Percorrer instancias do aparelho na residencia\n",
    "    for INSTANCIA in instancia_aparelho_residencia(CARGA, RESIDENCIA, base = redd):\n",
    "\n",
    "        # Extrair series divididas em janelas para cada medidor\n",
    "        print(\"   - Carregando dados (taxa={:.0f}, janela={:.0f})...\".format(\n",
    "            TAXA, TAMANHO_JANELA\n",
    "        ))\n",
    "        X, y = carregar_dados_aparelho(\n",
    "            janelas=janelas,\n",
    "            instancia=INSTANCIA,\n",
    "            aparelho=CARGA,\n",
    "            tamanho_janela=TAMANHO_JANELA,\n",
    "            taxa=TAXA,\n",
    "            eliminar_janelas_vazias=True\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        print(\"   - Detalhes da amostragem (lotes):\")\n",
    "        print(\"   ---\")\n",
    "        for item in Counter(y).items():\n",
    "            print(f\"      - Classe `{item[0]}`: {item[1]} amostras ({round(item[1]/len(y)*100,1)}%)\" )\n",
    "        print()\n",
    "\n",
    "        # Checando series estaveis\n",
    "        estavel = []\n",
    "        for i, x in enumerate(X):\n",
    "            if len(np.unique(x)) == 1:\n",
    "                estavel.append(i)\n",
    "        print(\"      - Séries estáveis (1 amplitude)         : {} ({:.2f}%)\".format(len(estavel), len(estavel)/len(X)*100) )\n",
    "        print(\"      - Distribuicao de classes nestas séries :\", Counter(y[estavel]))\n",
    "        print()\n",
    "\n",
    "        print(\"      - Estatísticas das séries: = Min. / Max. / Média / STD:\", X.min(), X.max(), X.mean(), X.std())\n",
    "        print()\n",
    "\n",
    "        # Convertendo series para imagem (CV)\n",
    "        for it, (idx_treino, idx_teste) in enumerate(kfold.split(X, y)):\n",
    "\n",
    "            iteracao = it + 1    \n",
    "            print(f\"   - Preparando dados para modelagem (cv-{iteracao})...\")\n",
    "            print(\"   ---\")\n",
    "            X_treino, X_teste = X[idx_treino], X[idx_teste]\n",
    "            y_treino, y_teste = y[idx_treino], y[idx_teste]\n",
    "            print()\n",
    "\n",
    "            print(\"      - Distribuições dos lotes:\")\n",
    "            print(\"        -> Treino:\", Counter(y_treino))\n",
    "            print(\"        -> Teste :\", Counter(y_teste))\n",
    "\n",
    "            X_treino_imagem, _ = preparar_amostras(\n",
    "                X_treino, y_treino, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "            X_teste_imagem, _ = preparar_amostras(\n",
    "                X_teste, y_teste, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "\n",
    "            # Normalizar dados p/ modelo\n",
    "            X_treino_imagem = normalizar_dados(X_treino_imagem)\n",
    "            X_teste_imagem = normalizar_dados(X_teste_imagem)\n",
    "\n",
    "            print()\n",
    "\n",
    "            print(\"   - Avaliando modelo, aguarde...\")\n",
    "            print(\"   ---\")\n",
    "            y_true, y_pred = [], []\n",
    "            accs = []\n",
    "            scores = []\n",
    "            aucs = []\n",
    "\n",
    "            # Avaliando N vezes o conjunto treino/teste (consitencia)\n",
    "            N = 10\n",
    "            for i in tqdm_notebook(range(N)):\n",
    "\n",
    "                ##################### METODOLOGIA ANTIGA #####################\n",
    "                # Treinamento estratico (lotes equilibrados)\n",
    "                modelo = treinamento_estrategico(\n",
    "                    modelo=convnet_metodologia(\n",
    "                        input_shape_= TAMANHO_IMAGEM,\n",
    "                        output_dim = 1,\n",
    "                        loss_function='binary_crossentropy',\n",
    "                        metrics=['accuracy'],\n",
    "                        output_activation='sigmoid'\n",
    "                    ), \n",
    "                    X=X_treino_imagem, y=y_treino, \n",
    "                    #validacao=(X_teste_imagem, y_teste), \n",
    "                    epocas=EPOCAS, \n",
    "                    majoritaria=classe_majoritaria(Counter(y_treino)), \n",
    "                    minoritaria=classe_minoritaria(Counter(y_treino)), \n",
    "                    peso_classe={classe_majoritaria(Counter(y_treino)):1, \n",
    "                                 classe_minoritaria(Counter(y_treino)):1}, \n",
    "                    debug=False\n",
    "                )\n",
    "\n",
    "                # Avaliando\n",
    "                y_hat = modelo.predict(X_teste_imagem).round().astype(np.int16)\n",
    "                ##################### METODOLOGIA ANTIGA #####################\n",
    "\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "                # Incrementando resultados\n",
    "                acc = accuracy_score(y_teste, y_hat)\n",
    "                score = f1_score(y_teste, y_hat, average=\"macro\")\n",
    "                try:\n",
    "                    auc_score = roc_auc_score(y_teste, y_hat)\n",
    "                except:\n",
    "                    if score == 1:\n",
    "                        auc_score = 1\n",
    "                    else:\n",
    "                        auc_score = 0.5\n",
    "                print(\"      -> # TENTATIVA #{}: Acurácia = {:.2f}% / F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                    i+1, \n",
    "                    acc*100,\n",
    "                    score*100,\n",
    "                    auc_score,\n",
    "                ))\n",
    "                accs.append(acc)\n",
    "                scores.append(score)\n",
    "                aucs.append(auc_score)\n",
    "                y_true.extend(y_teste)\n",
    "                y_pred.extend(y_hat)\n",
    "\n",
    "                # Guardando resultados do modelo\n",
    "                resultados[\"aparelho\"].append(rotulo_aparelho)\n",
    "                resultados[\"iteracao\"].append(iteracao) # APENAS PARA VALIDACAO CRUZADA\n",
    "                resultados[\"teste\"].append(i+1)\n",
    "                resultados[\"acuracia\"].append(acc)\n",
    "                resultados[\"f1\"].append(score)\n",
    "                resultados[\"auc\"].append(auc_score)\n",
    "\n",
    "    print()\n",
    "    print(\"   - Resultados finais:\")\n",
    "    print(\"   ---\")\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Acurácia:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(accs)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(accs)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(accs)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(accs)*100) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> F1-score (macro):\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(scores)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(scores)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(scores)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(scores)*100) )\n",
    "    print()\n",
    "    print(\"      -> AUC:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.3f}\".format(np.mean(aucs) ) )\n",
    "    print(\"         . Desvio padrão : {:.3f}\".format(np.std(aucs)) )\n",
    "    print(\"         . Mínimo        : {:.3f}\".format(np.min(aucs)) )\n",
    "    print(\"         . Máximo        : {:.3f}\".format(np.max(aucs)) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Relatório de classificação:\")\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"      -> Matrix de confusao:\")\n",
    "    print()\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    print()\n",
    "    \n",
    "# Exportando resultados\n",
    "df_treinamento_estrategico_cv = pd.DataFrame(resultados)\n",
    "df_treinamento_estrategico_cv[\"metodologia\"] = \"Treinamento Estratégico\"\n",
    "df_treinamento_estrategico_cv.to_excel(\n",
    "    os.path.join(caminho_dados_notebook, \"resultados_treinamento_estrategico_cv.xlsx\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodologia: Class Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Treino/Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.963626Z",
     "start_time": "2020-08-05T11:27:53.244Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados = {\n",
    "    \"aparelho\": [],\n",
    "    \"teste\": [],\n",
    "    \"acuracia\": [],\n",
    "    \"f1\": [],\n",
    "    #\"suporte_relativo\": [],\n",
    "    \"auc\":[],\n",
    "}\n",
    "for rotulo_aparelho in df_melhores_taxas_janelas.loc[\n",
    "    df_melhores_taxas_janelas[\"carga\"].isin(\n",
    "        ['dish_washer - 9','fridge - 7','microwave - 16','washer_dryer - 13',\n",
    "         'washer_dryer - 14']),\n",
    "    : ][\"carga\"].values:\n",
    "    \n",
    "    print(f\"* Aparelho {rotulo_aparelho.upper()}:\")\n",
    "    print()\n",
    "    \n",
    "    # Informacoes da carga selecionada\n",
    "    CARGA = rotulo_aparelho.split(\" - \")[0]\n",
    "    #INSTANCIA = int(rotulo_aparelho.split(\" - \")[1])\n",
    "\n",
    "    config_aparelho = df_melhores_taxas_janelas[\n",
    "        df_melhores_taxas_janelas[\"carga\"]==rotulo_aparelho\n",
    "    ].to_dict(\"records\")[0]\n",
    "    TAXA = config_aparelho[\"taxa_amostragem\"]\n",
    "    TAMANHO_JANELA = config_aparelho[\"janela\"]\n",
    "    CONFIG_RP_APARELHO = PARAMETROS_RP\n",
    "\n",
    "    # Percorrer instancias do aparelho na residencia\n",
    "    for INSTANCIA in instancia_aparelho_residencia(CARGA, RESIDENCIA, base = redd):\n",
    "\n",
    "        # Extrair series divididas em janelas para cada medidor\n",
    "        print(\"   - Carregando dados (taxa={:.0f}, janela={:.0f})...\".format(\n",
    "            TAXA, TAMANHO_JANELA\n",
    "        ))\n",
    "        X, y = carregar_dados_aparelho(\n",
    "            janelas=janelas,\n",
    "            instancia=INSTANCIA,\n",
    "            aparelho=CARGA,\n",
    "            tamanho_janela=TAMANHO_JANELA,\n",
    "            taxa=TAXA,\n",
    "            eliminar_janelas_vazias=True\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        # Convertendo series para imagem\n",
    "        print(\"   - Preparando dados para modelagem (treino/teste)...\")\n",
    "        print(\"   ---\")\n",
    "        # Fazendo split dos dados (treino/teste)\n",
    "        X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
    "            X, y, \n",
    "            test_size=FRACAO_TESTE,\n",
    "            stratify=y if Counter(y)[0] > 1 and Counter(y)[1] > 1 else None,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        print(\"      - Distribuições dos lotes:\")\n",
    "        print(\"        -> Treino:\", Counter(y_treino))\n",
    "        print(\"        -> Teste :\", Counter(y_teste))\n",
    "\n",
    "        X_treino_imagem, _ = preparar_amostras(\n",
    "            X_treino, y_treino, \n",
    "            params_rp=CONFIG_RP_APARELHO,\n",
    "            tam_imagem=TAMANHO_IMAGEM,\n",
    "            normalizar=False # config. estudo 17 = False\n",
    "        )\n",
    "        X_teste_imagem, _ = preparar_amostras(\n",
    "            X_teste, y_teste, \n",
    "            params_rp=CONFIG_RP_APARELHO,\n",
    "            tam_imagem=TAMANHO_IMAGEM,\n",
    "            normalizar=False # config. estudo 17 = False\n",
    "        )\n",
    "\n",
    "        # Normalizar dados p/ modelo\n",
    "        X_treino_imagem = normalizar_dados(X_treino_imagem)\n",
    "        X_teste_imagem = normalizar_dados(X_teste_imagem)\n",
    "\n",
    "        print()\n",
    "\n",
    "        # calculando punicao para classes (desbalanceamento)\n",
    "        try:\n",
    "            neg, pos = np.bincount(y_treino)\n",
    "        except:\n",
    "            dist = Counter(y_treino)\n",
    "            neg, pos = dist[0], dist[1]\n",
    "            neg += 1e-5\n",
    "            pos += 1e-5\n",
    "            del dist\n",
    "        total = neg + pos\n",
    "        p0 = (1 / neg)*(total)/2.0 \n",
    "        p1 = (1 / pos)*(total)/2.0\n",
    "        pesos_classes = {\n",
    "            0: p0 if not np.isinf(p0) else 1e-3, \n",
    "            1: p1 if not np.isinf(p1) else 1e-3\n",
    "        }\n",
    "        print(\"   - Punição de classes:\", pesos_classes)\n",
    "        print()\n",
    "\n",
    "        print(\"   - Avaliando modelo, aguarde...\")\n",
    "        print(\"   ---\")\n",
    "        y_true, y_pred = [], []\n",
    "        accs = []\n",
    "        scores = []\n",
    "        aucs = []\n",
    "        suportes_relativos = []\n",
    "\n",
    "        # Avaliando N vezes o conjunto treino/teste (consitencia)\n",
    "        N = 10\n",
    "        for i in tqdm_notebook(range(N)):\n",
    "\n",
    "            # Instanciando modelo pre-treinado\n",
    "            modelo = convnet_metodologia(\n",
    "                input_shape_= TAMANHO_IMAGEM,\n",
    "                output_dim = 1,\n",
    "                loss_function='binary_crossentropy',\n",
    "                metrics=['accuracy'],\n",
    "                output_activation='sigmoid'\n",
    "            )\n",
    "\n",
    "            # Treinando\n",
    "            historico = modelo.fit(\n",
    "                X_treino_imagem, y_treino,\n",
    "                validation_data=(X_teste_imagem, y_teste),\n",
    "                epochs=EPOCAS,\n",
    "                batch_size=TAMANHO_LOTE,\n",
    "                class_weight=pesos_classes,\n",
    "                #verbose=VERBOSIDADE\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            y_teste_, y_pred_, acc, score, auc_score, suporte_relativo, idx_descartados = \\\n",
    "                classificacao_threshold(\n",
    "                    modelo, X_teste_imagem, y_teste, \n",
    "                    threshold={0:0, 1:0}, # TODO: validar melhor corte por aparelho\n",
    "                    grafico=False\n",
    "                )\n",
    "\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "            # Incrementando resultados\n",
    "            print(\"      -> # TENTATIVA #{}: Acurácia = {:.2f}% / F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                i+1, \n",
    "                acc*100,\n",
    "                score*100,\n",
    "                auc_score,\n",
    "            ))\n",
    "            accs.append(acc)\n",
    "            scores.append(score)\n",
    "            aucs.append(auc_score)\n",
    "            y_true.extend(y_teste_)\n",
    "            y_pred.extend(y_pred_)\n",
    "\n",
    "\n",
    "            # Guardando resultados do modelo\n",
    "            resultados[\"aparelho\"].append(rotulo_aparelho)\n",
    "            #resultados[\"iteracao\"].append(iteracao) # APENAS PARA VALIDACAO CRUZADA\n",
    "            resultados[\"teste\"].append(i+1)\n",
    "            resultados[\"acuracia\"].append(acc)\n",
    "            resultados[\"f1\"].append(score)\n",
    "            resultados[\"auc\"].append(auc_score)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(\"   - Resultados finais:\")\n",
    "    print(\"   ---\")\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Acurácia:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(accs)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(accs)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(accs)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(accs)*100) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> F1-score (macro):\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(scores)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(scores)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(scores)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(scores)*100) )\n",
    "    print()\n",
    "    print(\"      -> AUC:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.3f}\".format(np.mean(aucs) ) )\n",
    "    print(\"         . Desvio padrão : {:.3f}\".format(np.std(aucs)) )\n",
    "    print(\"         . Mínimo        : {:.3f}\".format(np.min(aucs)) )\n",
    "    print(\"         . Máximo        : {:.3f}\".format(np.max(aucs)) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Relatório de classificação:\")\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"      -> Matrix de confusao:\")\n",
    "    print()\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    print()\n",
    "    \n",
    "df_classweight = pd.DataFrame(resultados)\n",
    "df_classweight[\"metodologia\"] = \"Class Weight\"\n",
    "df_classweight.to_excel(\n",
    "    os.path.join(caminho_dados_notebook, \"resultados_classweight.xlsx\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.964624Z",
     "start_time": "2020-08-05T11:27:53.609Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados = {\n",
    "    \"aparelho\": [],\n",
    "    \"iteracao\": [],\n",
    "    \"teste\": [],\n",
    "    \"acuracia\": [],\n",
    "    \"f1\": [],\n",
    "    #\"suporte_relativo\": [],\n",
    "    \"auc\":[],\n",
    "}\n",
    "for rotulo_aparelho in df_melhores_taxas_janelas.loc[\n",
    "    df_melhores_taxas_janelas[\"carga\"].isin(\n",
    "        ['dish_washer - 9','fridge - 7','microwave - 16','washer_dryer - 13',\n",
    "         'washer_dryer - 14']),\n",
    "    : ][\"carga\"].values:\n",
    "    \n",
    "    print(f\"* Aparelho {rotulo_aparelho.upper()}:\")\n",
    "    print()\n",
    "    \n",
    "    # Informacoes da carga selecionada\n",
    "    CARGA = rotulo_aparelho.split(\" - \")[0]\n",
    "    #INSTANCIA = int(rotulo_aparelho.split(\" - \")[1])\n",
    "\n",
    "    config_aparelho = df_melhores_taxas_janelas[\n",
    "        df_melhores_taxas_janelas[\"carga\"]==rotulo_aparelho\n",
    "    ].to_dict(\"records\")[0]\n",
    "    TAXA = config_aparelho[\"taxa_amostragem\"]\n",
    "    TAMANHO_JANELA = config_aparelho[\"janela\"]\n",
    "    CONFIG_RP_APARELHO = PARAMETROS_RP\n",
    "\n",
    "    # Percorrer instancias do aparelho na residencia\n",
    "    for INSTANCIA in instancia_aparelho_residencia(CARGA, RESIDENCIA, base = redd):\n",
    "\n",
    "        # Extrair series divididas em janelas para cada medidor\n",
    "        print(\"   - Carregando dados (taxa={:.0f}, janela={:.0f})...\".format(\n",
    "            TAXA, TAMANHO_JANELA\n",
    "        ))\n",
    "        X, y = carregar_dados_aparelho(\n",
    "            janelas=janelas,\n",
    "            instancia=INSTANCIA,\n",
    "            aparelho=CARGA,\n",
    "            tamanho_janela=TAMANHO_JANELA,\n",
    "            taxa=TAXA,\n",
    "            eliminar_janelas_vazias=True\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        # Convertendo series para imagem (CV)\n",
    "        for it, (idx_treino, idx_teste) in enumerate(kfold.split(X, y)):\n",
    "\n",
    "            iteracao = it + 1    \n",
    "            print(f\"   - Preparando dados para modelagem (cv-{iteracao})...\")\n",
    "            print(\"   ---\")\n",
    "            X_treino, X_teste = X[idx_treino], X[idx_teste]\n",
    "            y_treino, y_teste = y[idx_treino], y[idx_teste]\n",
    "            print()\n",
    "\n",
    "            print(\"      - Distribuições dos lotes:\")\n",
    "            print(\"        -> Treino:\", Counter(y_treino))\n",
    "            print(\"        -> Teste :\", Counter(y_teste))\n",
    "\n",
    "            X_treino_imagem, _ = preparar_amostras(\n",
    "                X_treino, y_treino, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "            X_teste_imagem, _ = preparar_amostras(\n",
    "                X_teste, y_teste, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "\n",
    "            # Normalizar dados p/ modelo\n",
    "            X_treino_imagem = normalizar_dados(X_treino_imagem)\n",
    "            X_teste_imagem = normalizar_dados(X_teste_imagem)\n",
    "\n",
    "            print()\n",
    "\n",
    "            # calculando punicao para classes (desbalanceamento)\n",
    "            try:\n",
    "                neg, pos = np.bincount(y_treino)\n",
    "            except:\n",
    "                dist = Counter(y_treino)\n",
    "                neg, pos = dist[0], dist[1]\n",
    "                neg += 1e-5\n",
    "                pos += 1e-5\n",
    "                del dist\n",
    "            total = neg + pos\n",
    "            p0 = (1 / neg)*(total)/2.0 \n",
    "            p1 = (1 / pos)*(total)/2.0\n",
    "            pesos_classes = {\n",
    "                0: p0 if not np.isinf(p0) else 1e-3, \n",
    "                1: p1 if not np.isinf(p1) else 1e-3\n",
    "            }\n",
    "            print(\"   - Punição de classes:\", pesos_classes)\n",
    "            print()\n",
    "\n",
    "            print(\"   - Avaliando modelo, aguarde...\")\n",
    "            print(\"   ---\")\n",
    "            y_true, y_pred = [], []\n",
    "            accs = []\n",
    "            scores = []\n",
    "            aucs = []\n",
    "            suportes_relativos = []\n",
    "\n",
    "            # Avaliando N vezes o conjunto treino/teste (consitencia)\n",
    "            N = 10\n",
    "            for i in tqdm_notebook(range(N)):\n",
    "\n",
    "                # Instanciando modelo pre-treinado\n",
    "                modelo = convnet_metodologia(\n",
    "                    input_shape_= TAMANHO_IMAGEM,\n",
    "                    output_dim = 1,\n",
    "                    loss_function='binary_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    output_activation='sigmoid'\n",
    "                )\n",
    "\n",
    "                # Treinando\n",
    "                historico = modelo.fit(\n",
    "                    X_treino_imagem, y_treino,\n",
    "                    validation_data=(X_teste_imagem, y_teste),\n",
    "                    epochs=EPOCAS,\n",
    "                    batch_size=TAMANHO_LOTE,\n",
    "                    class_weight=pesos_classes,\n",
    "                    #verbose=VERBOSIDADE\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                y_teste_, y_pred_, acc, score, auc_score, suporte_relativo, idx_descartados = \\\n",
    "                    classificacao_threshold(\n",
    "                        modelo, X_teste_imagem, y_teste, \n",
    "                        threshold={0:0, 1:0}, # TODO: validar melhor corte por aparelho\n",
    "                        grafico=False\n",
    "                    )\n",
    "\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "                # Incrementando resultados\n",
    "                print(\"      -> # TENTATIVA #{}: Acurácia = {:.2f}% / F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                    i+1, \n",
    "                    acc*100,\n",
    "                    score*100,\n",
    "                    auc_score,\n",
    "                ))\n",
    "                accs.append(acc)\n",
    "                scores.append(score)\n",
    "                aucs.append(auc_score)\n",
    "                y_true.extend(y_teste_)\n",
    "                y_pred.extend(y_pred_)\n",
    "\n",
    "\n",
    "                # Guardando resultados do modelo\n",
    "                resultados[\"aparelho\"].append(rotulo_aparelho)\n",
    "                resultados[\"iteracao\"].append(iteracao) # APENAS PARA VALIDACAO CRUZADA\n",
    "                resultados[\"teste\"].append(i+1)\n",
    "                resultados[\"acuracia\"].append(acc)\n",
    "                resultados[\"f1\"].append(score)\n",
    "                resultados[\"auc\"].append(auc_score)\n",
    "\n",
    "    print()\n",
    "    print(\"   - Resultados finais:\")\n",
    "    print(\"   ---\")\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Acurácia:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(accs)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(accs)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(accs)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(accs)*100) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> F1-score (macro):\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(scores)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(scores)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(scores)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(scores)*100) )\n",
    "    print()\n",
    "    print(\"      -> AUC:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.3f}\".format(np.mean(aucs) ) )\n",
    "    print(\"         . Desvio padrão : {:.3f}\".format(np.std(aucs)) )\n",
    "    print(\"         . Mínimo        : {:.3f}\".format(np.min(aucs)) )\n",
    "    print(\"         . Máximo        : {:.3f}\".format(np.max(aucs)) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Relatório de classificação:\")\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"      -> Matrix de confusao:\")\n",
    "    print()\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    print()\n",
    "    \n",
    "df_classweight_cv = pd.DataFrame(resultados)\n",
    "df_classweight_cv[\"metodologia\"] = \"Class Weight\"\n",
    "df_classweight_cv.to_excel(\n",
    "    os.path.join(caminho_dados_notebook, \"resultados_classweight_cv.xlsx\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T22:42:31.510911Z",
     "start_time": "2020-07-25T22:39:33.060Z"
    }
   },
   "source": [
    "## Metodologia: Random Over Sampling (ROS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.966626Z",
     "start_time": "2020-08-05T11:27:53.924Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Treino/Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.968625Z",
     "start_time": "2020-08-05T11:27:54.221Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados = {\n",
    "    \"aparelho\": [],\n",
    "    \"teste\": [],\n",
    "    \"acuracia\": [],\n",
    "    \"f1\": [],\n",
    "    #\"suporte_relativo\": [],\n",
    "    \"auc\":[],\n",
    "}\n",
    "for rotulo_aparelho in df_melhores_taxas_janelas.loc[\n",
    "    df_melhores_taxas_janelas[\"carga\"].isin(\n",
    "        ['dish_washer - 9','fridge - 7','microwave - 16','washer_dryer - 13',\n",
    "         'washer_dryer - 14']),\n",
    "    : ][\"carga\"].values:\n",
    "    \n",
    "    print(f\"* Aparelho {rotulo_aparelho.upper()}:\")\n",
    "    print()\n",
    "    \n",
    "    # Informacoes da carga selecionada\n",
    "    CARGA = rotulo_aparelho.split(\" - \")[0]\n",
    "    #INSTANCIA = int(rotulo_aparelho.split(\" - \")[1])\n",
    "\n",
    "    config_aparelho = df_melhores_taxas_janelas[\n",
    "        df_melhores_taxas_janelas[\"carga\"]==rotulo_aparelho\n",
    "    ].to_dict(\"records\")[0]\n",
    "    TAXA = config_aparelho[\"taxa_amostragem\"]\n",
    "    TAMANHO_JANELA = config_aparelho[\"janela\"]\n",
    "    CONFIG_RP_APARELHO = PARAMETROS_RP\n",
    "\n",
    "    # Percorrer instancias do aparelho na residencia\n",
    "    for INSTANCIA in instancia_aparelho_residencia(CARGA, RESIDENCIA, base = redd):\n",
    "\n",
    "        # Extrair series divididas em janelas para cada medidor\n",
    "        print(\"   - Carregando dados (taxa={:.0f}, janela={:.0f})...\".format(\n",
    "            TAXA, TAMANHO_JANELA\n",
    "        ))\n",
    "        X, y = carregar_dados_aparelho(\n",
    "            janelas=janelas,\n",
    "            instancia=INSTANCIA,\n",
    "            aparelho=CARGA,\n",
    "            tamanho_janela=TAMANHO_JANELA,\n",
    "            taxa=TAXA,\n",
    "            eliminar_janelas_vazias=True\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        # Convertendo series para imagem\n",
    "        print(\"   - Preparando dados para modelagem (treino/teste)...\")\n",
    "        print(\"   ---\")\n",
    "        # Fazendo split dos dados (treino/teste)\n",
    "        X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
    "            X, y, \n",
    "            test_size=FRACAO_TESTE,\n",
    "            stratify=y if Counter(y)[0] > 1 and Counter(y)[1] > 1 else None,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        print(\"      - Distribuições dos lotes:\")\n",
    "        print(\"        -> Treino:\", Counter(y_treino))\n",
    "        print(\"        -> Teste :\", Counter(y_teste))\n",
    "\n",
    "        X_treino_imagem, _ = preparar_amostras(\n",
    "            X_treino, y_treino, \n",
    "            params_rp=CONFIG_RP_APARELHO,\n",
    "            tam_imagem=TAMANHO_IMAGEM,\n",
    "            normalizar=False # config. estudo 17 = False\n",
    "        )\n",
    "        X_teste_imagem, _ = preparar_amostras(\n",
    "            X_teste, y_teste, \n",
    "            params_rp=CONFIG_RP_APARELHO,\n",
    "            tam_imagem=TAMANHO_IMAGEM,\n",
    "            normalizar=False # config. estudo 17 = False\n",
    "        )\n",
    "\n",
    "        # Normalizar dados p/ modelo\n",
    "        X_treino_imagem = normalizar_dados(X_treino_imagem)\n",
    "        X_teste_imagem = normalizar_dados(X_teste_imagem)\n",
    "\n",
    "        print()\n",
    "\n",
    "        ########################### OVERSAMPLING ###########################\n",
    "        try:\n",
    "            X_treino_imagem, y_treino = RandomOverSampler(random_state=SEED).fit_resample(\n",
    "                X_treino_imagem.reshape(-1, np.product(TAMANHO_IMAGEM)), \n",
    "                y_treino\n",
    "            )\n",
    "            X_treino_imagem = X_treino_imagem.reshape(-1, *TAMANHO_IMAGEM)\n",
    "\n",
    "            print(\"      - Nova Distribuição lote TREINO (Reamostragem):\")\n",
    "            print(\"        -> Treino:\", Counter(y_treino))\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(\"# ERRO:\", str(e))\n",
    "\n",
    "        ########################### OVERSAMPLING ###########################\n",
    "\n",
    "        print(\"   - Avaliando modelo, aguarde...\")\n",
    "        print(\"   ---\")\n",
    "        y_true, y_pred = [], []\n",
    "        accs = []\n",
    "        scores = []\n",
    "        aucs = []\n",
    "        suportes_relativos = []\n",
    "\n",
    "        # Avaliando N vezes o conjunto treino/teste (consitencia)\n",
    "        N = 10\n",
    "        for i in tqdm_notebook(range(N)):\n",
    "\n",
    "            # Instanciando modelo pre-treinado\n",
    "            modelo = convnet_metodologia(\n",
    "                input_shape_= TAMANHO_IMAGEM,\n",
    "                output_dim = 1,\n",
    "                loss_function='binary_crossentropy',\n",
    "                metrics=['accuracy'],\n",
    "                output_activation='sigmoid'\n",
    "            )\n",
    "\n",
    "            # Treinando\n",
    "            historico = modelo.fit(\n",
    "                X_treino_imagem, y_treino,\n",
    "                validation_data=(X_teste_imagem, y_teste),\n",
    "                epochs=EPOCAS,\n",
    "                batch_size=TAMANHO_LOTE,\n",
    "                #verbose=VERBOSIDADE\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            # Avaliando resultados\n",
    "            y_teste_, y_pred_, acc, score, auc_score, suporte_relativo, idx_descartados = \\\n",
    "                classificacao_threshold(\n",
    "                    modelo, X_teste_imagem, y_teste, \n",
    "                    threshold={0:0, 1:0}, # TODO: validar melhor corte por aparelho\n",
    "                    grafico=False\n",
    "                )\n",
    "\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "            # Incrementando resultados\n",
    "            print(\"      -> # TENTATIVA #{}: Acurácia = {:.2f}% / F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                i+1, \n",
    "                acc*100,\n",
    "                score*100,\n",
    "                auc_score,\n",
    "            ))\n",
    "            accs.append(acc)\n",
    "            scores.append(score)\n",
    "            aucs.append(auc_score)\n",
    "            y_true.extend(y_teste_)\n",
    "            y_pred.extend(y_pred_)\n",
    "\n",
    "\n",
    "            # Guardando resultados do modelo\n",
    "            resultados[\"aparelho\"].append(rotulo_aparelho)\n",
    "            #resultados[\"iteracao\"].append(iteracao) # APENAS PARA VALIDACAO CRUZADA\n",
    "            resultados[\"teste\"].append(i+1)\n",
    "            resultados[\"acuracia\"].append(acc)\n",
    "            resultados[\"f1\"].append(score)\n",
    "            resultados[\"auc\"].append(auc_score)\n",
    "\n",
    "    print()\n",
    "    print(\"   - Resultados finais:\")\n",
    "    print(\"   ---\")\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Acurácia:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(accs)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(accs)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(accs)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(accs)*100) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> F1-score (macro):\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(scores)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(scores)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(scores)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(scores)*100) )\n",
    "    print()\n",
    "    print(\"      -> AUC:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.3f}\".format(np.mean(aucs) ) )\n",
    "    print(\"         . Desvio padrão : {:.3f}\".format(np.std(aucs)) )\n",
    "    print(\"         . Mínimo        : {:.3f}\".format(np.min(aucs)) )\n",
    "    print(\"         . Máximo        : {:.3f}\".format(np.max(aucs)) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Relatório de classificação:\")\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"      -> Matrix de confusao:\")\n",
    "    print()\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    print()\n",
    "    \n",
    "df_ros = pd.DataFrame(resultados)\n",
    "df_ros[\"metodologia\"] = \"ROS (Baseline + RandomOverSampling)\"\n",
    "df_ros.to_excel(\n",
    "    os.path.join(caminho_dados_notebook, \"resultados_ros.xlsx\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.971626Z",
     "start_time": "2020-08-05T11:27:54.526Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados = {\n",
    "    \"aparelho\": [],\n",
    "    \"iteracao\": [],\n",
    "    \"teste\": [],\n",
    "    \"acuracia\": [],\n",
    "    \"f1\": [],\n",
    "    #\"suporte_relativo\": [],\n",
    "    \"auc\":[],\n",
    "}\n",
    "for rotulo_aparelho in df_melhores_taxas_janelas.loc[\n",
    "    df_melhores_taxas_janelas[\"carga\"].isin(\n",
    "        ['dish_washer - 9','fridge - 7','microwave - 16','washer_dryer - 13',\n",
    "         'washer_dryer - 14']),\n",
    "    : ][\"carga\"].values:\n",
    "    \n",
    "    print(f\"* Aparelho {rotulo_aparelho.upper()}:\")\n",
    "    print()\n",
    "    \n",
    "    # Informacoes da carga selecionada\n",
    "    CARGA = rotulo_aparelho.split(\" - \")[0]\n",
    "    #INSTANCIA = int(rotulo_aparelho.split(\" - \")[1])\n",
    "\n",
    "    config_aparelho = df_melhores_taxas_janelas[\n",
    "        df_melhores_taxas_janelas[\"carga\"]==rotulo_aparelho\n",
    "    ].to_dict(\"records\")[0]\n",
    "    TAXA = config_aparelho[\"taxa_amostragem\"]\n",
    "    TAMANHO_JANELA = config_aparelho[\"janela\"]\n",
    "    CONFIG_RP_APARELHO = PARAMETROS_RP\n",
    "\n",
    "    # Percorrer instancias do aparelho na residencia\n",
    "    for INSTANCIA in instancia_aparelho_residencia(CARGA, RESIDENCIA, base = redd):\n",
    "\n",
    "        # Extrair series divididas em janelas para cada medidor\n",
    "        print(\"   - Carregando dados (taxa={:.0f}, janela={:.0f})...\".format(\n",
    "            TAXA, TAMANHO_JANELA\n",
    "        ))\n",
    "        X, y = carregar_dados_aparelho(\n",
    "            janelas=janelas,\n",
    "            instancia=INSTANCIA,\n",
    "            aparelho=CARGA,\n",
    "            tamanho_janela=TAMANHO_JANELA,\n",
    "            taxa=TAXA,\n",
    "            eliminar_janelas_vazias=True\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        # Convertendo series para imagem (CV)\n",
    "        for it, (idx_treino, idx_teste) in enumerate(kfold.split(X, y)):\n",
    "\n",
    "            iteracao = it + 1    \n",
    "            print(f\"   - Preparando dados para modelagem (cv-{iteracao})...\")\n",
    "            print(\"   ---\")\n",
    "            X_treino, X_teste = X[idx_treino], X[idx_teste]\n",
    "            y_treino, y_teste = y[idx_treino], y[idx_teste]\n",
    "            print()\n",
    "\n",
    "            print(\"      - Distribuições dos lotes:\")\n",
    "            print(\"        -> Treino:\", Counter(y_treino))\n",
    "            print(\"        -> Teste :\", Counter(y_teste))\n",
    "\n",
    "            X_treino_imagem, _ = preparar_amostras(\n",
    "                X_treino, y_treino, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "            X_teste_imagem, _ = preparar_amostras(\n",
    "                X_teste, y_teste, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "\n",
    "            # Normalizar dados p/ modelo\n",
    "            X_treino_imagem = normalizar_dados(X_treino_imagem)\n",
    "            X_teste_imagem = normalizar_dados(X_teste_imagem)\n",
    "\n",
    "            print()\n",
    "\n",
    "            ########################### OVERSAMPLING ###########################\n",
    "            try:\n",
    "                X_treino_imagem, y_treino = RandomOverSampler(random_state=SEED).fit_resample(\n",
    "                    X_treino_imagem.reshape(-1, np.product(TAMANHO_IMAGEM)), \n",
    "                    y_treino\n",
    "                )\n",
    "                X_treino_imagem = X_treino_imagem.reshape(-1, *TAMANHO_IMAGEM)\n",
    "\n",
    "                print(\"      - Nova Distribuição lote TREINO (Reamostragem):\")\n",
    "                print(\"        -> Treino:\", Counter(y_treino))\n",
    "                print()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"# ERRO:\", str(e))\n",
    "            ########################### OVERSAMPLING ###########################\n",
    "\n",
    "            print(\"   - Avaliando modelo, aguarde...\")\n",
    "            print(\"   ---\")\n",
    "            y_true, y_pred = [], []\n",
    "            accs = []\n",
    "            scores = []\n",
    "            aucs = []\n",
    "            suportes_relativos = []\n",
    "\n",
    "            # Avaliando N vezes o conjunto treino/teste (consitencia)\n",
    "            N = 10\n",
    "            for i in tqdm_notebook(range(N)):\n",
    "\n",
    "                # Instanciando modelo pre-treinado\n",
    "                modelo = convnet_metodologia(\n",
    "                    input_shape_= TAMANHO_IMAGEM,\n",
    "                    output_dim = 1,\n",
    "                    loss_function='binary_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    output_activation='sigmoid'\n",
    "                )\n",
    "\n",
    "                # Treinando\n",
    "                historico = modelo.fit(\n",
    "                    X_treino_imagem, y_treino,\n",
    "                    validation_data=(X_teste_imagem, y_teste),\n",
    "                    epochs=EPOCAS,\n",
    "                    batch_size=TAMANHO_LOTE,\n",
    "                    #verbose=VERBOSIDADE\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Avaliando resultados\n",
    "                y_teste_, y_pred_, acc, score, auc_score, suporte_relativo, idx_descartados = \\\n",
    "                    classificacao_threshold(\n",
    "                        modelo, X_teste_imagem, y_teste, \n",
    "                        threshold={0:0, 1:0}, # TODO: validar melhor corte por aparelho\n",
    "                        grafico=False\n",
    "                    )\n",
    "\n",
    "\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "                # Incrementando resultados\n",
    "                print(\"      -> # TENTATIVA #{}: Acurácia = {:.2f}% / F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                    i+1, \n",
    "                    acc*100,\n",
    "                    score*100,\n",
    "                    auc_score,\n",
    "                ))\n",
    "                accs.append(acc)\n",
    "                scores.append(score)\n",
    "                aucs.append(auc_score)\n",
    "                y_true.extend(y_teste_)\n",
    "                y_pred.extend(y_pred_)\n",
    "\n",
    "\n",
    "                # Guardando resultados do modelo\n",
    "                resultados[\"aparelho\"].append(rotulo_aparelho)\n",
    "                resultados[\"iteracao\"].append(iteracao) # APENAS PARA VALIDACAO CRUZADA\n",
    "                resultados[\"teste\"].append(i+1)\n",
    "                resultados[\"acuracia\"].append(acc)\n",
    "                resultados[\"f1\"].append(score)\n",
    "                resultados[\"auc\"].append(auc_score)\n",
    "\n",
    "    print()\n",
    "    print(\"   - Resultados finais:\")\n",
    "    print(\"   ---\")\n",
    "    print()\n",
    "    \n",
    "    print(\"      -> Acurácia:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(accs)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(accs)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(accs)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(accs)*100) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> F1-score (macro):\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(scores)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(scores)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(scores)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(scores)*100) )\n",
    "    print()\n",
    "    print(\"      -> AUC:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.3f}\".format(np.mean(aucs) ) )\n",
    "    print(\"         . Desvio padrão : {:.3f}\".format(np.std(aucs)) )\n",
    "    print(\"         . Mínimo        : {:.3f}\".format(np.min(aucs)) )\n",
    "    print(\"         . Máximo        : {:.3f}\".format(np.max(aucs)) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Relatório de classificação:\")\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"      -> Matrix de confusao:\")\n",
    "    print()\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    print()\n",
    "    \n",
    "df_ros_cv = pd.DataFrame(resultados)\n",
    "df_ros_cv[\"metodologia\"] = \"ROS (Baseline + RandomOverSampling)\"\n",
    "df_ros_cv.to_excel(\n",
    "    os.path.join(caminho_dados_notebook, \"resultados_ros_cv.xlsx\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodologia: Random Under Sampling (RUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.972626Z",
     "start_time": "2020-08-05T11:27:54.810Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Treino/Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.974631Z",
     "start_time": "2020-08-05T11:27:55.098Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados = {\n",
    "    \"aparelho\": [],\n",
    "    \"teste\": [],\n",
    "    \"acuracia\": [],\n",
    "    \"f1\": [],\n",
    "    #\"suporte_relativo\": [],\n",
    "    \"auc\":[],\n",
    "}\n",
    "for rotulo_aparelho in df_melhores_taxas_janelas.loc[\n",
    "    df_melhores_taxas_janelas[\"carga\"].isin(\n",
    "        ['dish_washer - 9','fridge - 7','microwave - 16','washer_dryer - 13',\n",
    "         'washer_dryer - 14']),\n",
    "    : ][\"carga\"].values:\n",
    "    \n",
    "    print(f\"* Aparelho {rotulo_aparelho.upper()}:\")\n",
    "    print()\n",
    "    \n",
    "    # Informacoes da carga selecionada\n",
    "    CARGA = rotulo_aparelho.split(\" - \")[0]\n",
    "    #INSTANCIA = int(rotulo_aparelho.split(\" - \")[1])\n",
    "\n",
    "    config_aparelho = df_melhores_taxas_janelas[\n",
    "        df_melhores_taxas_janelas[\"carga\"]==rotulo_aparelho\n",
    "    ].to_dict(\"records\")[0]\n",
    "    TAXA = config_aparelho[\"taxa_amostragem\"]\n",
    "    TAMANHO_JANELA = config_aparelho[\"janela\"]\n",
    "    CONFIG_RP_APARELHO = PARAMETROS_RP\n",
    "\n",
    "    # Percorrer instancias do aparelho na residencia\n",
    "    for INSTANCIA in instancia_aparelho_residencia(CARGA, RESIDENCIA, base = redd):\n",
    "\n",
    "        # Extrair series divididas em janelas para cada medidor\n",
    "        print(\"   - Carregando dados (taxa={:.0f}, janela={:.0f})...\".format(\n",
    "            TAXA, TAMANHO_JANELA\n",
    "        ))\n",
    "        X, y = carregar_dados_aparelho(\n",
    "            janelas=janelas,\n",
    "            instancia=INSTANCIA,\n",
    "            aparelho=CARGA,\n",
    "            tamanho_janela=TAMANHO_JANELA,\n",
    "            taxa=TAXA,\n",
    "            eliminar_janelas_vazias=True\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        # Convertendo series para imagem\n",
    "        print(\"   - Preparando dados para modelagem (treino/teste)...\")\n",
    "        print(\"   ---\")\n",
    "        # Fazendo split dos dados (treino/teste)\n",
    "        X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
    "            X, y, \n",
    "            test_size=FRACAO_TESTE,\n",
    "            stratify=y if Counter(y)[0] > 1 and Counter(y)[1] > 1 else None,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        print(\"      - Distribuições dos lotes:\")\n",
    "        print(\"        -> Treino:\", Counter(y_treino))\n",
    "        print(\"        -> Teste :\", Counter(y_teste))\n",
    "\n",
    "        X_treino_imagem, _ = preparar_amostras(\n",
    "            X_treino, y_treino, \n",
    "            params_rp=CONFIG_RP_APARELHO,\n",
    "            tam_imagem=TAMANHO_IMAGEM,\n",
    "            normalizar=False # config. estudo 17 = False\n",
    "        )\n",
    "        X_teste_imagem, _ = preparar_amostras(\n",
    "            X_teste, y_teste, \n",
    "            params_rp=CONFIG_RP_APARELHO,\n",
    "            tam_imagem=TAMANHO_IMAGEM,\n",
    "            normalizar=False # config. estudo 17 = False\n",
    "        )\n",
    "\n",
    "        # Normalizar dados p/ modelo\n",
    "        X_treino_imagem = normalizar_dados(X_treino_imagem)\n",
    "        X_teste_imagem = normalizar_dados(X_teste_imagem)\n",
    "\n",
    "        print()\n",
    "\n",
    "        ########################### UNDERSAMPLING ###########################\n",
    "        try:\n",
    "            X_treino_imagem, y_treino = RandomUnderSampler(random_state=SEED).fit_resample(\n",
    "                X_treino_imagem.reshape(-1, np.product(TAMANHO_IMAGEM)), \n",
    "                y_treino\n",
    "            )\n",
    "            X_treino_imagem = X_treino_imagem.reshape(-1, *TAMANHO_IMAGEM)\n",
    "\n",
    "            print(\"      - Nova Distribuição lote TREINO (Reamostragem):\")\n",
    "            print(\"        -> Treino:\", Counter(y_treino))\n",
    "            print()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"# ERRO:\", str(e))\n",
    "        ########################### UNDERSAMPLING ###########################\n",
    "\n",
    "        print(\"   - Avaliando modelo, aguarde...\")\n",
    "        print(\"   ---\")\n",
    "        y_true, y_pred = [], []\n",
    "        accs = []\n",
    "        scores = []\n",
    "        aucs = []\n",
    "        suportes_relativos = []\n",
    "\n",
    "        # Avaliando N vezes o conjunto treino/teste (consitencia)\n",
    "        N = 10\n",
    "        for i in tqdm_notebook(range(N)):\n",
    "\n",
    "            # Instanciando modelo pre-treinado\n",
    "            modelo = convnet_metodologia(\n",
    "                input_shape_= TAMANHO_IMAGEM,\n",
    "                output_dim = 1,\n",
    "                loss_function='binary_crossentropy',\n",
    "                metrics=['accuracy'],\n",
    "                output_activation='sigmoid'\n",
    "            )\n",
    "\n",
    "            # Treinando\n",
    "            historico = modelo.fit(\n",
    "                X_treino_imagem, y_treino,\n",
    "                validation_data=(X_teste_imagem, y_teste),\n",
    "                epochs=EPOCAS,\n",
    "                batch_size=TAMANHO_LOTE,\n",
    "                #verbose=VERBOSIDADE\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            # Avaliando resultados\n",
    "            y_teste_, y_pred_, acc, score, auc_score, suporte_relativo, idx_descartados = \\\n",
    "                classificacao_threshold(\n",
    "                    modelo, X_teste_imagem, y_teste, \n",
    "                    threshold={0:0, 1:0}, # TODO: validar melhor corte por aparelho\n",
    "                    grafico=False\n",
    "                )\n",
    "\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "            # Incrementando resultados\n",
    "            print(\"      -> # TENTATIVA #{}: Acurácia = {:.2f}% / F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                i+1, \n",
    "                acc*100,\n",
    "                score*100,\n",
    "                auc_score,\n",
    "            ))\n",
    "            accs.append(acc)\n",
    "            scores.append(score)\n",
    "            aucs.append(auc_score)\n",
    "            y_true.extend(y_teste_)\n",
    "            y_pred.extend(y_pred_)\n",
    "\n",
    "            # Guardando resultados do modelo\n",
    "            resultados[\"aparelho\"].append(rotulo_aparelho)\n",
    "            #resultados[\"iteracao\"].append(iteracao) # APENAS PARA VALIDACAO CRUZADA\n",
    "            resultados[\"teste\"].append(i+1)\n",
    "            resultados[\"acuracia\"].append(acc)\n",
    "            resultados[\"f1\"].append(score)\n",
    "            resultados[\"auc\"].append(auc_score)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(\"   - Resultados finais:\")\n",
    "    print(\"   ---\")\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Acurácia:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(accs)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(accs)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(accs)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(accs)*100) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> F1-score (macro):\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(scores)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(scores)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(scores)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(scores)*100) )\n",
    "    print()\n",
    "    print(\"      -> AUC:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.3f}\".format(np.mean(aucs) ) )\n",
    "    print(\"         . Desvio padrão : {:.3f}\".format(np.std(aucs)) )\n",
    "    print(\"         . Mínimo        : {:.3f}\".format(np.min(aucs)) )\n",
    "    print(\"         . Máximo        : {:.3f}\".format(np.max(aucs)) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Relatório de classificação:\")\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"      -> Matrix de confusao:\")\n",
    "    print()\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    print()\n",
    "    \n",
    "df_rus = pd.DataFrame(resultados)\n",
    "df_rus[\"metodologia\"] = \"RUS (Baseline + RandomUnderSampling)\"\n",
    "df_rus.to_excel(\n",
    "    os.path.join(caminho_dados_notebook, \"resultados_rus.xlsx\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.976656Z",
     "start_time": "2020-08-05T11:27:55.437Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados = {\n",
    "    \"aparelho\": [],\n",
    "    \"iteracao\": [],\n",
    "    \"teste\": [],\n",
    "    \"acuracia\": [],\n",
    "    \"f1\": [],\n",
    "    #\"suporte_relativo\": [],\n",
    "    \"auc\":[],\n",
    "}\n",
    "for rotulo_aparelho in df_melhores_taxas_janelas.loc[\n",
    "    df_melhores_taxas_janelas[\"carga\"].isin(\n",
    "        ['dish_washer - 9','fridge - 7','microwave - 16','washer_dryer - 13',\n",
    "         'washer_dryer - 14']),\n",
    "    : ][\"carga\"].values:\n",
    "    \n",
    "    print(f\"* Aparelho {rotulo_aparelho.upper()}:\")\n",
    "    print()\n",
    "    \n",
    "    # Informacoes da carga selecionada\n",
    "    CARGA = rotulo_aparelho.split(\" - \")[0]\n",
    "    #INSTANCIA = int(rotulo_aparelho.split(\" - \")[1])\n",
    "\n",
    "    config_aparelho = df_melhores_taxas_janelas[\n",
    "        df_melhores_taxas_janelas[\"carga\"]==rotulo_aparelho\n",
    "    ].to_dict(\"records\")[0]\n",
    "    TAXA = config_aparelho[\"taxa_amostragem\"]\n",
    "    TAMANHO_JANELA = config_aparelho[\"janela\"]\n",
    "    CONFIG_RP_APARELHO = PARAMETROS_RP\n",
    "\n",
    "    # Percorrer instancias do aparelho na residencia\n",
    "    for INSTANCIA in instancia_aparelho_residencia(CARGA, RESIDENCIA, base = redd):\n",
    "\n",
    "        # Extrair series divididas em janelas para cada medidor\n",
    "        print(\"   - Carregando dados (taxa={:.0f}, janela={:.0f})...\".format(\n",
    "            TAXA, TAMANHO_JANELA\n",
    "        ))\n",
    "        X, y = carregar_dados_aparelho(\n",
    "            janelas=janelas,\n",
    "            instancia=INSTANCIA,\n",
    "            aparelho=CARGA,\n",
    "            tamanho_janela=TAMANHO_JANELA,\n",
    "            taxa=TAXA,\n",
    "            eliminar_janelas_vazias=True\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        # Convertendo series para imagem (CV)\n",
    "        for it, (idx_treino, idx_teste) in enumerate(kfold.split(X, y)):\n",
    "\n",
    "            iteracao = it + 1    \n",
    "            print(f\"   - Preparando dados para modelagem (cv-{iteracao})...\")\n",
    "            print(\"   ---\")\n",
    "            X_treino, X_teste = X[idx_treino], X[idx_teste]\n",
    "            y_treino, y_teste = y[idx_treino], y[idx_teste]\n",
    "            print()\n",
    "\n",
    "            print(\"      - Distribuições dos lotes:\")\n",
    "            print(\"        -> Treino:\", Counter(y_treino))\n",
    "            print(\"        -> Teste :\", Counter(y_teste))\n",
    "\n",
    "            X_treino_imagem, _ = preparar_amostras(\n",
    "                X_treino, y_treino, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "            X_teste_imagem, _ = preparar_amostras(\n",
    "                X_teste, y_teste, \n",
    "                params_rp=CONFIG_RP_APARELHO,\n",
    "                tam_imagem=TAMANHO_IMAGEM,\n",
    "                normalizar=False # config. estudo 17 = False\n",
    "            )\n",
    "\n",
    "            # Normalizar dados p/ modelo\n",
    "            X_treino_imagem = normalizar_dados(X_treino_imagem)\n",
    "            X_teste_imagem = normalizar_dados(X_teste_imagem)\n",
    "\n",
    "            print()\n",
    "\n",
    "            ########################### UNDERSAMPLING ###########################\n",
    "            try:\n",
    "                X_treino_imagem, y_treino = RandomUnderSampler(random_state=SEED).fit_resample(\n",
    "                    X_treino_imagem.reshape(-1, np.product(TAMANHO_IMAGEM)), \n",
    "                    y_treino\n",
    "                )\n",
    "                X_treino_imagem = X_treino_imagem.reshape(-1, *TAMANHO_IMAGEM)\n",
    "\n",
    "                print(\"      - Nova Distribuição lote TREINO (Reamostragem):\")\n",
    "                print(\"        -> Treino:\", Counter(y_treino))\n",
    "                print()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"# ERRO:\", str(e))\n",
    "            ########################### UNDERSAMPLING ###########################\n",
    "\n",
    "            print(\"   - Avaliando modelo, aguarde...\")\n",
    "            print(\"   ---\")\n",
    "            y_true, y_pred = [], []\n",
    "            accs = []\n",
    "            scores = []\n",
    "            aucs = []\n",
    "            suportes_relativos = []\n",
    "\n",
    "            # Avaliando N vezes o conjunto treino/teste (consitencia)\n",
    "            N = 10\n",
    "            for i in tqdm_notebook(range(N)):\n",
    "\n",
    "                # Instanciando modelo pre-treinado\n",
    "                modelo = convnet_metodologia(\n",
    "                    input_shape_= TAMANHO_IMAGEM,\n",
    "                    output_dim = 1,\n",
    "                    loss_function='binary_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    output_activation='sigmoid'\n",
    "                )\n",
    "\n",
    "                # Treinando\n",
    "                historico = modelo.fit(\n",
    "                    X_treino_imagem, y_treino,\n",
    "                    validation_data=(X_teste_imagem, y_teste),\n",
    "                    epochs=EPOCAS,\n",
    "                    batch_size=TAMANHO_LOTE,\n",
    "                    #verbose=VERBOSIDADE\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Avaliando resultados\n",
    "                y_teste_, y_pred_, acc, score, auc_score, suporte_relativo, idx_descartados = \\\n",
    "                    classificacao_threshold(\n",
    "                        modelo, X_teste_imagem, y_teste, \n",
    "                        threshold={0:0, 1:0}, # TODO: validar melhor corte por aparelho\n",
    "                        grafico=False\n",
    "                    )\n",
    "\n",
    "\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "                # Incrementando resultados\n",
    "                print(\"      -> # TENTATIVA #{}: Acurácia = {:.2f}% / F1-score = {:.2f}% / AUC = {:.3f}\".format(\n",
    "                    i+1, \n",
    "                    acc*100,\n",
    "                    score*100,\n",
    "                    auc_score,\n",
    "                ))\n",
    "                accs.append(acc)\n",
    "                scores.append(score)\n",
    "                aucs.append(auc_score)\n",
    "                y_true.extend(y_teste_)\n",
    "                y_pred.extend(y_pred_)\n",
    "\n",
    "                # Guardando resultados do modelo\n",
    "                resultados[\"aparelho\"].append(rotulo_aparelho)\n",
    "                resultados[\"iteracao\"].append(iteracao) # APENAS PARA VALIDACAO CRUZADA\n",
    "                resultados[\"teste\"].append(i+1)\n",
    "                resultados[\"acuracia\"].append(acc)\n",
    "                resultados[\"f1\"].append(score)\n",
    "                resultados[\"auc\"].append(auc_score)\n",
    "\n",
    "    print()\n",
    "    print(\"   - Resultados finais:\")\n",
    "    print(\"   ---\")\n",
    "    print()\n",
    "    \n",
    "    print(\"      -> Acurácia:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(accs)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(accs)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(accs)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(accs)*100) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> F1-score (macro):\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.2f}%\".format(np.mean(scores)*100 ) )\n",
    "    print(\"         . Desvio padrão : {:.2f}%\".format(np.std(scores)*100) )\n",
    "    print(\"         . Mínimo        : {:.2f}%\".format(np.min(scores)*100) )\n",
    "    print(\"         . Máximo        : {:.2f}%\".format(np.max(scores)*100) )\n",
    "    print()\n",
    "    print(\"      -> AUC:\")\n",
    "    print()\n",
    "    print(\"         . Média geral   : {:.3f}\".format(np.mean(aucs) ) )\n",
    "    print(\"         . Desvio padrão : {:.3f}\".format(np.std(aucs)) )\n",
    "    print(\"         . Mínimo        : {:.3f}\".format(np.min(aucs)) )\n",
    "    print(\"         . Máximo        : {:.3f}\".format(np.max(aucs)) )\n",
    "    print()\n",
    "\n",
    "    print(\"      -> Relatório de classificação:\")\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"      -> Matrix de confusao:\")\n",
    "    print()\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    print()\n",
    "    \n",
    "df_rus_cv = pd.DataFrame(resultados)\n",
    "df_rus_cv[\"metodologia\"] = \"RUS (Baseline + RandomUnderSampling)\"\n",
    "df_rus_cv.to_excel(\n",
    "    os.path.join(caminho_dados_notebook, \"resultados_rus_cv.xlsx\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos Resultados (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Treino/Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.977628Z",
     "start_time": "2020-08-05T11:27:56.113Z"
    }
   },
   "outputs": [],
   "source": [
    "df_baseline = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_baseline.xlsx\"))\n",
    "df_tce = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_tce.xlsx\"))\n",
    "df_classweight = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_classweight.xlsx\"))\n",
    "df_ros = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_ros.xlsx\"))\n",
    "df_rus = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_rus.xlsx\"))\n",
    "df_treinamento_estrategico = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_treinamento_estrategico.xlsx\"))\n",
    "df_transfer_learning = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_transfer_learning.xlsx\"))\n",
    "\n",
    "df_analise = pd.concat([\n",
    "    # Resultados das metodologias Baseline e Final\n",
    "    df_baseline, \n",
    "    df_tce,\n",
    "    \n",
    "    # Desbalancmeanto\n",
    "    df_classweight,\n",
    "    df_ros,\n",
    "    df_rus,    \n",
    "    df_treinamento_estrategico,\n",
    "    df_transfer_learning,\n",
    "])\n",
    "\n",
    "print(\"* Análise por metodologia:\")\n",
    "df_analise_metodologia = df_analise.groupby([\"metodologia\"]).agg({\n",
    "    \"f1\": [\"mean\",\"std\",\"max\",\"min\"],\n",
    "    \"auc\": [\"mean\",\"std\",\"max\",\"min\"]\n",
    "}).reset_index().sort_values(('f1','mean'), ascending=False).set_index(\"metodologia\")\n",
    "display(df_analise_metodologia)\n",
    "df_analise_metodologia.to_excel(os.path.join(caminho_dados_notebook, \"df_analise2_metodologia.xlsx\"))\n",
    "\n",
    "print()\n",
    "print(\"* Análise por aparelho/metodologia:\")\n",
    "df_analise_aparelho = df_analise.groupby([\"aparelho\",\"metodologia\"]).agg({\n",
    "    \"f1\": [\"mean\",\"std\",\"max\",\"min\"],\n",
    "    \"auc\": [\"mean\",\"std\",\"max\",\"min\"]\n",
    "})#.reset_index().sort_values(('f1','mean'), ascending=False).set_index([\"aparelho\",\"metodologia\"])\n",
    "display(df_analise_aparelho)\n",
    "df_analise_aparelho.to_excel(os.path.join(caminho_dados_notebook, \"df_analise2_aparelho.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.978629Z",
     "start_time": "2020-08-05T11:27:56.413Z"
    }
   },
   "outputs": [],
   "source": [
    "df_baseline_cv = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_baseline_cv.xlsx\"))\n",
    "df_tce_cv = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_tce_cv.xlsx\"))\n",
    "df_classweight_cv = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_classweight_cv.xlsx\"))\n",
    "df_ros_cv = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_ros_cv.xlsx\"))\n",
    "df_rus_cv = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_rus_cv.xlsx\"))\n",
    "df_treinamento_estrategico_cv = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_treinamento_estrategico_cv.xlsx\"))\n",
    "df_transfer_learning_cv = pd.read_excel(os.path.join(caminho_dados_notebook, \"resultados_transfer_learning_cv.xlsx\"))\n",
    "\n",
    "df_analise_cv = pd.concat([\n",
    "    # Resultados das metodologias Baseline e Final\n",
    "    df_baseline_cv, \n",
    "    df_tce_cv,\n",
    "    \n",
    "    # Desbalancmeanto\n",
    "    df_classweight_cv,\n",
    "    df_ros_cv,\n",
    "    df_rus_cv,    \n",
    "    df_treinamento_estrategico_cv,\n",
    "    df_transfer_learning_cv,\n",
    "])\n",
    "\n",
    "print(\"* Análise por metodologia:\")\n",
    "df_analise_metodologia = df_analise_cv.groupby([\"metodologia\"]).agg({\n",
    "    \"f1\": [\"mean\",\"std\",\"max\",\"min\"],\n",
    "    \"auc\": [\"mean\",\"std\",\"max\",\"min\"]\n",
    "}).reset_index().sort_values(('f1','mean'), ascending=False).set_index(\"metodologia\")\n",
    "display(df_analise_metodologia)\n",
    "df_analise_metodologia.to_excel(os.path.join(caminho_dados_notebook, \"df_analise2_metodologia_cv.xlsx\"))\n",
    "\n",
    "print()\n",
    "print(\"* Análise por aparelho/metodologia:\")\n",
    "df_analise_aparelho = df_analise_cv.groupby([\"aparelho\",\"metodologia\"]).agg({\n",
    "    \"f1\": [\"mean\",\"std\",\"max\",\"min\"],\n",
    "    \"auc\": [\"mean\",\"std\",\"max\",\"min\"]\n",
    "})#.reset_index().sort_values(('f1','mean'), ascending=False).set_index([\"aparelho\",\"metodologia\"])\n",
    "display(df_analise_aparelho)\n",
    "df_analise_aparelho.to_excel(os.path.join(caminho_dados_notebook, \"df_analise2_aparelho_cv.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após várias análises, ficou evidente que:\n",
    "1. A metodologia original sofre com o desbalanceeamento dos dados inerentes ao problema de NILM, principalmente para a carga analisada (Microondas, casa 3);\n",
    "2. O ajuste fino do bias mais a adoção de punição de erros, parada antecipada e lote aumentado melhoram significativamente a capacidade do modelo em aprender padrões da classe minoritária (positiva, no estudo);\n",
    "3. Além disso, o ajuste do limiar de decisão melhora sensivelmente a qualidade das predições, uma vez que as probabilidades do modelo são bem calibradas;\n",
    "4. Por fim, a estratégia de Transfer Learning se mostrou de grande valia, corroborando as expectativas iniciais, uma vez que esta técnica permite aproveitar o máximo de conhecimento dentro de um domínio.\n",
    "\n",
    "Sendo assim, a `metodologia final` (hipótese 6) otimizada consiste em:\n",
    "\n",
    "- **Entrada:** RP\n",
    "- **Modelo:** CNN\n",
    "- **Estratégia de treinamento:**\n",
    "    - Inicialização de Bias da camada de saída em função da distribuição dos dados de treinamento;\n",
    "    - Adoção de fator de penalização da função de erros na classe minoritária;\n",
    "    - Adoção de parada antecipada observando a métrica AUC;\n",
    "    - Lote aumentado, a fim de favorecer a probabilidade de minilotes conterem amostras positivas (TODO: observar se há regra empírica).\n",
    "    - Utilizar transfer learning: contemplando residências que contenham o mesmo aparelho (TODO: definir regras de modo e  sequência de utilização destes dados);\n",
    "- **Pós-processamento:**\n",
    "    - Limiar de Decisão nas probabilidades inferidas pelo modelo (TODO: avaliar melhor limiar);\n",
    "        - Avaliar energia acumulada no descarte.\n",
    "\n",
    "---\n",
    "\n",
    "**PRÓXIMOS PASSOS (QUALIFICAÇÃO):**\n",
    "\n",
    "1. Analisar tabela de resultados do estudo `17` e extrair as melhores configurações de tamanho de janela considerando:\n",
    "    i. Melhor Taxa Amostral Geral;\n",
    "    ii. Melhor combinação de Taxa e Janela por aparelho;\n",
    "2. Encapsular Metodologia Final na classe PyNILM;\n",
    "3. Avaliar a aplicação da Metodologia Final nos cenários definidos no item 1, consolidando as métricas F1, AUC e Descarte de Energia.\n",
    "\n",
    "**IDEIAS DEFESA:**\n",
    "1. Otimização arquitetura, visando Transfer Learning;\n",
    "2. Capsule Networks;\n",
    "3. Processamento dos descartes:\n",
    "    i. One-Shot Learning;\n",
    "    ii. AutoEncoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.979629Z",
     "start_time": "2020-08-05T11:27:57.396Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:28:00.981628Z",
     "start_time": "2020-08-05T11:27:57.407Z"
    }
   },
   "outputs": [],
   "source": [
    "%watermark -a \"Diego Luiz Cavalca\" -u -n -t -z -v -m -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (doutorado)",
   "language": "python",
   "name": "doutorado"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
